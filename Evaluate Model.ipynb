{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "import random\n",
    "import h5py\n",
    "from keras.models import load_model\n",
    "\n",
    "import os\n",
    "import random\n",
    "import time\n",
    "\n",
    "# for reproducibility\n",
    "np.random.seed(1337) \n",
    "random.seed(1337)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# control amount of GPU memory used\n",
    "import tensorflow as tf\n",
    "from keras.backend.tensorflow_backend import set_session\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth=True\n",
    "set_session(tf.Session(config=config))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# external custom code I wrote\n",
    "from load_data import *\n",
    "from windowing import *\n",
    "from nn_util import *\n",
    "from pesq import *\n",
    "from consts import *\n",
    "from nn_blocks import *\n",
    "from perceptual_loss import *\n",
    "from evaluation import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "[train_paths, val_paths, test_paths], \\\n",
    "[train_waveforms, val_waveforms, test_waveforms], \\\n",
    "[train_procwave, val_procwave, test_procwave], \\\n",
    "[train_wparams, val_wparams, test_wparams], \\\n",
    "[train_windows, val_windows, test_windows] = load_data(TRAIN_SIZE, VAL_SIZE, TEST_SIZE)\n",
    "\n",
    "# flatten all of the train windows into vectors\n",
    "train_processed = np.array([i for z in train_windows for i in z])\n",
    "train_processed = np.reshape(train_processed, (train_processed.shape[0], WINDOW_SIZE,))\n",
    "\n",
    "X_train = np.copy(train_processed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/keras/models.py:258: UserWarning: No training configuration found in save file: the model was *not* compiled. Compile it manually.\n",
      "  warnings.warn('No training configuration found in save file: '\n"
     ]
    }
   ],
   "source": [
    "MODEL_FILENAME = './best_coder.h5'\n",
    "\n",
    "KERAS_LOAD_MAP = {'PhaseShiftUp1D' : PhaseShiftUp1D,\n",
    "                  'SoftmaxQuantization' : SoftmaxQuantization,\n",
    "                  'SoftmaxDequantization' : SoftmaxDequantization}\n",
    "\n",
    "autoencoder = load_model(MODEL_FILENAME, KERAS_LOAD_MAP)\n",
    "K.set_value(QUANTIZATION_ON, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "encoder = autoencoder.layers[1]\n",
    "decoder = autoencoder.layers[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test bitrate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19840/20000 [============================>.] - ETA: 0sBitrate: 32.2539333028kbps\n"
     ]
    }
   ],
   "source": [
    "all_embed = encoder.predict(X_train[:20000], batch_size = BATCH_SIZE, verbose = 1)\n",
    "\n",
    "probs = np.reshape(all_embed, (all_embed.shape[0] * all_embed.shape[1], NBINS))\n",
    "hist = np.sum(probs, axis = 0)\n",
    "hist /= np.sum(hist)\n",
    "\n",
    "entropy = 0\n",
    "for i in hist:\n",
    "    if (i < 1e-5): continue\n",
    "    entropy += i * math.log(i, 2)\n",
    "entropy = -entropy\n",
    "\n",
    "bitrate = float(SAMPLE_RATE) / (WINDOW_SIZE - OVERLAP_SIZE) * 256.0 * entropy\n",
    "bitrate /= 1000\n",
    "\n",
    "print \"Bitrate:\", str(bitrate) + \"kbps\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test speed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder: Averaged 2.9253466924 ms per window\n",
      "Decoder: Averaged 2.9927666982 ms per window\n"
     ]
    }
   ],
   "source": [
    "windows = np.random.uniform(-1.0, 1.0, (150, WINDOW_SIZE))\n",
    "\n",
    "# test encoder\n",
    "start = time.time()\n",
    "encoded = encoder.predict(windows, batch_size = 1, verbose = 0)\n",
    "end = time.time()\n",
    "\n",
    "averageMs = (end - start) / encoded.shape[0] * 1000.0\n",
    "print \"Encoder: Averaged\", averageMs, \"ms per window\"\n",
    "\n",
    "# test encoder\n",
    "start = time.time()\n",
    "decoded = decoder.predict(encoded, batch_size = 1, verbose = 0)\n",
    "end = time.time()\n",
    "\n",
    "averageMs = (end - start) / decoded.shape[0] * 1000.0\n",
    "print \"Decoder: Averaged\", averageMs, \"ms per window\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE:         1388.71\n",
      "Avg err:     21.828\n",
      "PESQ:        4.27999210358\n",
      "MSE:         1406.06\n",
      "Avg err:     22.0128\n",
      "PESQ:        4.25643253326\n",
      "MSE:         1346.41\n",
      "Avg err:     16.1279\n",
      "PESQ:        4.29865407944\n",
      "MSE:         1356.9\n",
      "Avg err:     16.2794\n",
      "PESQ:        4.29763698578\n",
      "MSE:         962777.0\n",
      "Avg err:     661.052\n",
      "PESQ:        3.69267249107\n",
      "MSE:         964917.0\n",
      "Avg err:     662.086\n",
      "PESQ:        3.69613170624\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[964917.44, 662.08551, 3.696131706237793]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_model_on_wav(\"./SA1.wav\", \"SA1_final\", autoencoder)\n",
    "test_model_on_wav(\"./SA1.wav\", \"SA1_final\", autoencoder, argmax = True)\n",
    "\n",
    "test_model_on_wav(\"./SX383.wav\", \"SX383_final\", autoencoder)\n",
    "test_model_on_wav(\"./SX383.wav\", \"SX383_final\", autoencoder, argmax = True)\n",
    "\n",
    "test_model_on_wav(\"./fiveYears.wav\", \"fy_final\", autoencoder)\n",
    "test_model_on_wav(\"./fiveYears.wav\", \"fy_final\", autoencoder, argmax = True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model evaluation (training)\n",
      "Mean: 4.40621822651\n",
      "Max:  4.60239028931\n",
      "Min:  2.39944434166\n",
      "\n",
      "Model evaluation (validation)\n",
      "Mean: 4.4601709938\n",
      "Max:  4.58294487\n",
      "Min:  4.09588050842\n",
      "\n",
      "Model evaluation (test)\n",
      "Mean: 4.40570156002\n",
      "Max:  4.58310604095\n",
      "Min:  3.58013510704\n",
      "\n"
     ]
    }
   ],
   "source": [
    "captions = [\"training\", \"validation\", \"test\"]\n",
    "datasets = [train_paths, val_paths, test_paths]\n",
    "\n",
    "for i in xrange(0, 3):\n",
    "    print \"Model evaluation (\" + captions[i] + \")\"\n",
    "\n",
    "    base_scores = []\n",
    "    for path in datasets[i]:\n",
    "        pesq = test_model_on_wav(path, \"\", autoencoder,\n",
    "                                 save_recons = False,\n",
    "                                 verbose = False,\n",
    "                                 argmax = True)[2]\n",
    "        base_scores.append(pesq)\n",
    "\n",
    "    print \"Mean:\", np.mean(base_scores)\n",
    "    print \"Max: \", np.max(base_scores)\n",
    "    print \"Min: \", np.min(base_scores)\n",
    "    print \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(val_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "352.71564"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "K.eval(encoder.layers[-1].SOFTMAX_TEMP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
