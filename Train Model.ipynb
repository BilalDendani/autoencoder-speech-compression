{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import random\n",
    "import h5py\n",
    "from keras import backend as K\n",
    "from utility import *\n",
    "from keras.models import *\n",
    "from keras.layers import *\n",
    "from keras.layers.core import *\n",
    "from keras.layers.normalization import *\n",
    "from keras.optimizers import *\n",
    "from keras.callbacks import *\n",
    "from keras.regularizers import *\n",
    "from keras.initializers import *\n",
    "from keras.models import load_model\n",
    "from keras.losses import *\n",
    "from sklearn import metrics\n",
    "from sklearn.preprocessing import *\n",
    "from scipy.fftpack import dct, idct\n",
    "from keras.activations import softmax\n",
    "from sklearn.cluster import KMeans, MiniBatchKMeans\n",
    "import scipy.io.wavfile as sciwav\n",
    "import multiprocessing\n",
    "\n",
    "import os\n",
    "import random\n",
    "import time\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "\n",
    "import operator\n",
    "import math\n",
    "import re\n",
    "\n",
    "# for reproducibility\n",
    "np.random.seed(1337) \n",
    "random.seed(1337)\n",
    "\n",
    "# increase recursion limit for adaptive VQ\n",
    "import sys\n",
    "sys.setrecursionlimit(40000)\n",
    "\n",
    "np.set_printoptions(formatter={'float_kind':'{:4f}'.format})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# control amount of GPU memory used\n",
    "import tensorflow as tf\n",
    "from keras.backend.tensorflow_backend import set_session\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth=True\n",
    "set_session(tf.Session(config=config))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# external custom code I wrote\n",
    "from load_data import *\n",
    "from windowing import *\n",
    "from pesq import *\n",
    "from consts import *\n",
    "from nn_blocks import *\n",
    "from transforms import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# randomly shuffle data before partitioning into training/validation?\n",
    "RANDOM_SHUFFLE = True\n",
    "\n",
    "# number of speech files for train, val, and test\n",
    "TRAIN_SIZE = 1000\n",
    "VAL_SIZE = 100\n",
    "TEST_SIZE = 500\n",
    "\n",
    "# during training, we evaluate PESQ and RMSE and such on full speech files every epoch, which\n",
    "# is kind of expensive. so instead of selecting the full training and validation set, we\n",
    "# randomly select this many waveforms\n",
    "TRAIN_EVALUATE = 100\n",
    "VAL_EVALUATE = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "[train_paths, val_paths, test_paths], \\\n",
    "[train_waveforms, val_waveforms, test_waveforms], \\\n",
    "[train_procwave, val_procwave, test_procwave], \\\n",
    "[train_wparams, val_wparams, test_wparams], \\\n",
    "[train_windows, val_windows, test_windows] = load_data(TRAIN_SIZE, VAL_SIZE, TEST_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# flatten all of the train windows into vectors\n",
    "train_processed = np.array([i for z in train_windows for i in z])\n",
    "train_processed = np.reshape(train_processed, (train_processed.shape[0], WINDOW_SIZE, 1))\n",
    "\n",
    "# randomly shuffle data, if we want to\n",
    "if (RANDOM_SHUFFLE):\n",
    "    train_processed = np.random.permutation(train_processed)\n",
    "    \n",
    "print train_processed.shape\n",
    "print np.mean(train_processed, axis=None)\n",
    "print np.std(train_processed, axis=None)\n",
    "print np.min(train_processed, axis = None)\n",
    "print np.max(train_processed, axis = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_dim = (WINDOW_SIZE, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# softmax hardness variable\n",
    "tau = K.variable(0.0001, name = \"hardness\")\n",
    "\n",
    "NBINS = 32\n",
    "BINS_INIT = np.linspace(-1.0, 1.0, NBINS).reshape((NBINS,))\n",
    "QUANT_BINS = K.variable(BINS_INIT, name = 'QUANT_BINS')\n",
    "\n",
    "DOWNSAMPLE_FACTOR = 2\n",
    "CHANNEL_SIZE = WINDOW_SIZE / DOWNSAMPLE_FACTOR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def unquantize_batch(one_hot):\n",
    "    out = K.dot(K.variable(one_hot), K.expand_dims(QUANT_BINS))\n",
    "    out = K.reshape(out, (out.shape[0], -1))\n",
    "    return K.eval(out)\n",
    "\n",
    "def unquantize_vec(one_hot):\n",
    "    out = K.dot(K.variable(one_hot), K.expand_dims(QUANT_BINS))\n",
    "    out = K.reshape(out, (-1,))\n",
    "    return K.eval(out)\n",
    "\n",
    "class SoftmaxQuantization(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(SoftmaxQuantization, self).__init__(**kwargs)\n",
    "   \n",
    "    def build(self, input_shape):\n",
    "        self.SOFTMAX_TEMP = K.variable(500.0)\n",
    "        self.trainable_weights = [QUANT_BINS,\n",
    "                                  self.SOFTMAX_TEMP]\n",
    "        super(SoftmaxQuantization, self).build(input_shape)\n",
    "        \n",
    "    def call(self, x, mask=None):\n",
    "        # x is an array: [BATCH x WINDOW_SIZE]\n",
    "        # x_r becomes: [BATCH x WINDOW_SIZE x 1]\n",
    "        x_r = K.reshape(x, (-1, x.shape[1], 1))\n",
    "\n",
    "        # QUANT_BINS is an array: [NBINS]\n",
    "        # q_r becomes: [1 x 1 x NBINS]\n",
    "        q_r = K.reshape(QUANT_BINS, (1, 1, QUANT_BINS.shape[0]))\n",
    "\n",
    "        # get L1 distance from each element to each of the bins\n",
    "        # dist is: [BATCH x WINDOW_SIZE x NBINS]\n",
    "        dist = K.abs(x_r - q_r)\n",
    "\n",
    "        # turn into softmax probabilities, which we return\n",
    "        probs = softmax(self.SOFTMAX_TEMP * -dist)\n",
    "        return probs\n",
    "    \n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (input_shape[0], input_shape[1], NBINS)\n",
    "\n",
    "\n",
    "class SoftmaxDequantization(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(SoftmaxDequantization, self).__init__(**kwargs)\n",
    "        self.supports_masking = False\n",
    "    \n",
    "    def build(self, input_shape):\n",
    "        self.trainable_weights = []\n",
    "        super(SoftmaxDequantization, self).build(input_shape)\n",
    "    \n",
    "    def call(self, x, mask=None):\n",
    "        out = K.dot(x, K.expand_dims(QUANT_BINS))\n",
    "        out = K.reshape(out, (-1, out.shape[1]))\n",
    "        return out\n",
    "    \n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (input_shape[0], input_shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "NUM_MFCC_COEFFS = 64\n",
    "\n",
    "# precompute Mel filterbank\n",
    "MEL_FILTERBANK_NPY = melFilterBank(NUM_MFCC_COEFFS).transpose()\n",
    "MEL_FILTERBANK = K.variable(MEL_FILTERBANK_NPY)\n",
    "\n",
    "# we precompute matrices for MFCC calculation\n",
    "DFT_REAL, DFT_IMAG = generate_dft_mats(WINDOW_SIZE)\n",
    "MFCC_DCT = generate_dct_mat(NUM_MFCC_COEFFS)\n",
    "\n",
    "# given a (symbolic Theano) array of size M x WINDOW_SIZE\n",
    "#     this returns an array M x N where each window has been replaced\n",
    "#     by some perceptual transform (in this case, MFCC coeffs)\n",
    "def perceptual_transform(x):\n",
    "    powerSpectrum = K.square(theano_dft_mag(x, DFT_REAL, DFT_IMAG))\n",
    "    filteredSpectrum = K.dot(powerSpectrum, MEL_FILTERBANK)\n",
    "    logSpectrum = K.log(filteredSpectrum + K.epsilon())\n",
    "    \n",
    "    #mfccs = theano_dct(logSpectrum, MFCC_DCT)[:, 1:-32]\n",
    "    return logSpectrum\n",
    "\n",
    "# perceptual loss function\n",
    "def perceptual_distance(y_true, y_pred):\n",
    "    y_true = K.reshape(y_true, (-1, WINDOW_SIZE))\n",
    "    y_pred = K.reshape(y_pred, (-1, WINDOW_SIZE))\n",
    "    \n",
    "    pvec_true = perceptual_transform(y_true)\n",
    "    pvec_pred = perceptual_transform(y_pred)\n",
    "    \n",
    "    return rmse(pvec_true, pvec_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------------------------\n",
    "# autoencoder: takes an audio window, compresses it, and tries to reconstruct it\n",
    "# ---------------------------------------------------------------------------\n",
    "def autoencoder_structure(dim):   \n",
    "    # - - - - - - - - - - - - - - - - - - - - -\n",
    "    # parameters\n",
    "    # - - - - - - - - - - - - - - - - - - - - -   \n",
    "    NCHAN = 32\n",
    "    FILT_SIZE = 9\n",
    "    \n",
    "    # - - - - - - - - - - - - - - - - - - - - -\n",
    "    # encoder\n",
    "    # - - - - - - - - - - - - - - - - - - - - -\n",
    "    enc_input = Input(shape = dim)\n",
    "    enc = enc_input\n",
    "    \n",
    "    enc = Reshape(dim, input_shape = dim)(enc)  \n",
    "    \n",
    "    enc = channel_change_block(NCHAN, FILT_SIZE)(enc)\n",
    "    enc = residual_block(NCHAN, FILT_SIZE, 1)(enc)\n",
    "    enc = residual_block(NCHAN, FILT_SIZE, 2)(enc)\n",
    "    enc = residual_block(NCHAN, FILT_SIZE, 4)(enc)\n",
    "    enc = downsample_block(NCHAN, FILT_SIZE)(enc)\n",
    "    enc = residual_block(NCHAN, FILT_SIZE, 1)(enc)\n",
    "    enc = residual_block(NCHAN, FILT_SIZE, 2)(enc)\n",
    "    enc = residual_block(NCHAN, FILT_SIZE, 4)(enc)\n",
    "    enc = channel_change_block(1, FILT_SIZE)(enc)\n",
    "    \n",
    "    enc = Reshape((CHANNEL_SIZE,))(enc)\n",
    "    #enc = Activation('tanh')(enc)\n",
    "    \n",
    "    # softmax quantization\n",
    "    enc = SoftmaxQuantization()(enc)\n",
    "    \n",
    "    enc = Model(inputs = enc_input, outputs = enc)\n",
    "    \n",
    "    # - - - - - - - - - - - - - - - - - - - - -\n",
    "    # decoder\n",
    "    # - - - - - - - - - - - - - - - - - - - - -\n",
    "    dec_input = Input(shape = (CHANNEL_SIZE, NBINS))\n",
    "    dec = dec_input\n",
    "    \n",
    "    dec = SoftmaxDequantization()(dec)\n",
    "    \n",
    "    # increase number of channels via convolution\n",
    "    dec = Reshape((CHANNEL_SIZE, 1))(dec)\n",
    "    \n",
    "    dec = channel_change_block(NCHAN, FILT_SIZE)(dec)\n",
    "    dec = residual_block(NCHAN, FILT_SIZE, 1)(dec)\n",
    "    dec = residual_block(NCHAN, FILT_SIZE, 2)(dec)\n",
    "    dec = residual_block(NCHAN, FILT_SIZE, 4)(dec)\n",
    "    dec = upsample_block(NCHAN, FILT_SIZE)(dec)\n",
    "    dec = residual_block(NCHAN, FILT_SIZE, 1)(dec)\n",
    "    dec = residual_block(NCHAN, FILT_SIZE, 2)(dec)\n",
    "    dec = residual_block(NCHAN, FILT_SIZE, 4)(dec)\n",
    "    dec = channel_change_block(1, FILT_SIZE)(dec)\n",
    "\n",
    "    #dec = Activation('tanh')(dec)    \n",
    "    dec = Model(inputs = dec_input, outputs = dec)\n",
    "    \n",
    "    # return both encoder and decoder\n",
    "    return enc, dec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# we can compute the entropy of a batch directly\n",
    "def code_entropy(placeholder, code):\n",
    "    all_onehots = K.reshape(code, (-1, NBINS))\n",
    "    onehot_hist = K.sum(all_onehots, axis = 0)\n",
    "    onehot_hist /= K.sum(onehot_hist)\n",
    "\n",
    "    entropy = -K.sum(onehot_hist * K.log(onehot_hist + K.epsilon()) / K.log(2.0))\n",
    "    loss = tau * entropy\n",
    "    return loss\n",
    "\n",
    "def code_sparsity(placeholder, code):\n",
    "    sparsity = K.mean(K.sum(K.sqrt(code + K.epsilon()), axis = -1), axis = -1) - 1.0\n",
    "    return sparsity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# map for load_model\n",
    "KERAS_LOAD_MAP = {'PhaseShiftUp1D' : PhaseShiftUp1D,\n",
    "                  'code_entropy' : code_entropy,\n",
    "                  'code_sparsity' : code_sparsity,\n",
    "                  'rmse' : rmse,\n",
    "                  'SoftmaxQuantization' : SoftmaxQuantization,\n",
    "                  'SoftmaxDequantization' : SoftmaxDequantization,\n",
    "                  'NBINS' : NBINS,\n",
    "                  'QUANT_BINS' : QUANT_BINS,\n",
    "                  'MEL_FILTERBANK' : MEL_FILTERBANK,\n",
    "                  'DFT_REAL' : DFT_REAL,\n",
    "                  'DFT_IMAG' : DFT_IMAG,\n",
    "                  'MFCC_DCT' : MFCC_DCT,\n",
    "                  'theano_dft_mag' : theano_dft_mag,\n",
    "                  'theano_dct' : theano_dct,\n",
    "                  'perceptual_transform' : perceptual_transform,\n",
    "                  'perceptual_distance' : perceptual_distance}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# construct autoencoder\n",
    "ac_input = Input(shape = input_dim)\n",
    "\n",
    "encoder, decoder = autoencoder_structure(input_dim)\n",
    "ac_reconstructed = decoder(encoder(ac_input))\n",
    "autoencoder = Model(inputs = [ac_input], outputs = [ac_reconstructed])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# model parameters\n",
    "loss_weights = [30.0, 1.0, 5.0, 1.0]\n",
    "loss_functions = [rmse, perceptual_distance, code_sparsity, code_entropy]\n",
    "n_recons = 2\n",
    "n_code = 2\n",
    "assert(n_recons + n_code == len(loss_weights))\n",
    "assert(len(loss_weights) == len(loss_functions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# model specification\n",
    "model_input = Input(shape = input_dim)\n",
    "model_embedding = encoder(model_input)\n",
    "model_reconstructed = decoder(model_embedding)\n",
    "\n",
    "model = Model(inputs = [model_input], outputs = [model_reconstructed] * n_recons + \\\n",
    "                                            [model_embedding] * n_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.compile(loss = loss_functions,\n",
    "              loss_weights = loss_weights,\n",
    "              optimizer = Adam())\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# return desired and reconstructed waveforms, from speech windows\n",
    "def run_model_on_windows(windows, wparams, autoencoder, argmax = False):\n",
    "    # first, get desired reconstruction\n",
    "    desired = reconstruct_from_windows(windows, OVERLAP_SIZE, OVERLAP_FUNC)\n",
    "    desired = unpreprocess_waveform(desired, wparams)\n",
    "    desired = np.clip(desired, -32767, 32767)\n",
    "    \n",
    "    # then, run NN on windows to get our model's reconstruction\n",
    "    transformed = np.reshape(windows, (windows.shape[0], WINDOW_SIZE, 1))\n",
    "    enc = autoencoder.layers[1]\n",
    "    \n",
    "    embed = enc.predict(transformed, batch_size = 128, verbose = 0)\n",
    "    if (argmax):\n",
    "        for wnd in xrange(0, embed.shape[0]):\n",
    "            max_idxs = np.argmax(embed[wnd], axis = -1)\n",
    "            embed[wnd] = np.eye(NBINS)[max_idxs]\n",
    "\n",
    "    dec = autoencoder.layers[2]\n",
    "    autoencOutput = dec.predict(embed, batch_size = 128, verbose = 0)\n",
    "    autoencOutput = np.reshape(autoencOutput, (autoencOutput.shape[0], WINDOW_SIZE))\n",
    "    recons = reconstruct_from_windows(autoencOutput, OVERLAP_SIZE, OVERLAP_FUNC)\n",
    "    recons = unpreprocess_waveform(recons, wparams)\n",
    "    recons = np.clip(recons, -32767, 32767)\n",
    "    \n",
    "    return desired, recons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# return evaluation metrics, given desired and reconstructed waveforms\n",
    "def evaluation_metrics(desired, recons):\n",
    "    pesq = run_pesq_waveforms(desired, recons)\n",
    "    \n",
    "    # return some metrics, as well as the two waveforms\n",
    "    metrics = [\n",
    "        mse(recons, desired),\n",
    "        avgErr(recons, desired),\n",
    "        pesq\n",
    "    ]\n",
    "    \n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# test model on a set of speech windows (which should originally have been extracted in\n",
    "# order from some speech waveform)\n",
    "def test_model_on_windows(windows, wparams, autoencoder, argmax = False):\n",
    "    # compute PESQ between desired and reconstructed waveforms\n",
    "    desired, recons = run_model_on_windows(windows, wparams, autoencoder, argmax)\n",
    "    return evaluation_metrics(desired, recons), desired, recons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# test model given the filename for a .wav file\n",
    "def test_model_on_wav(wave_filename, prefix, autoencoder,\n",
    "                      lead = \"\", save_recons = True, verbose = True,\n",
    "                      argmax = False):\n",
    "    [rate, data] = sciwav.read(wave_filename)\n",
    "    data = data.astype(np.float32)\n",
    "    processed_wave, wparams = preprocess_waveform(data)\n",
    "    windows = extract_windows(processed_wave, STEP_SIZE, OVERLAP_SIZE)\n",
    "    \n",
    "    metrics, desired, recons = test_model_on_windows(windows, wparams, autoencoder, argmax)\n",
    "    \n",
    "    if (save_recons):\n",
    "        outFilename = prefix + \"_output.wav\"\n",
    "        sciwav.write(outFilename, SAMPLE_RATE, recons.astype(np.int16))\n",
    "    \n",
    "    if (verbose):\n",
    "        print lead + \"MSE:        \", metrics[0]\n",
    "        print lead + \"Avg err:    \", metrics[1]\n",
    "        print lead + \"PESQ:       \", metrics[2]\n",
    "    \n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def save_model(prefix = 'best'):\n",
    "    os.system('rm ./' + prefix + '_model.h5')\n",
    "    os.system('rm ./' + prefix + '_auto.h5')\n",
    "    os.system('rm ./' + prefix + '_quant_bins.npy')\n",
    "    \n",
    "    model.save('./' + prefix + '_model.h5')\n",
    "    autoencoder.save('./' + prefix + '_auto.h5')\n",
    "    np.save('./' + prefix + '_quant_bins.npy', K.eval(QUANT_BINS))\n",
    "    \n",
    "    f = h5py.File('best_model.h5', 'r+')\n",
    "    del f['optimizer_weights']\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# get untrained baseline for model\n",
    "test_model_on_wav(\"./SA1.wav\", \"./train_output/SA1_uninit\", autoencoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def evaluate_training(autoencoder, lead = \"\"):\n",
    "    def set_evaluation(windows, wparams, eval_idxs):\n",
    "        before_after_pairs = np.array([run_model_on_windows(windows[i],\n",
    "                                                    wparams[i],\n",
    "                                                    autoencoder,\n",
    "                                                    argmax = True)\n",
    "                                       for i in eval_idxs])\n",
    "        \n",
    "        NUM_THREADS = 8\n",
    "        list_range = np.arange(0, len(eval_idxs))\n",
    "        slices = [list_range[i:None:NUM_THREADS]\n",
    "                  for i in xrange(0, NUM_THREADS)]\n",
    "        \n",
    "        def thread_func(pairs, q):\n",
    "            for p in pairs:\n",
    "                q.put(evaluation_metrics(p[0], p[1]))\n",
    "                \n",
    "        q = multiprocessing.Queue()\n",
    "        threads = [multiprocessing.Process(target = thread_func,\n",
    "                                           args = (before_after_pairs[slices[i]], q))\n",
    "                   for i in xrange(0, NUM_THREADS)]\n",
    "        [t.start() for t in threads]\n",
    "        [t.join() for t in threads]\n",
    "        \n",
    "        return np.array([q.get() for i in list_range])\n",
    "    \n",
    "    train_eval_idxs = random.sample(range(0, len(train_windows)), TRAIN_EVALUATE)\n",
    "    val_eval_idxs = random.sample(range(0, len(val_windows)), VAL_EVALUATE)\n",
    "    \n",
    "    print lead + \"Format: [MSE, avg err, PESQ]\"\n",
    "    \n",
    "    # train set evaluation\n",
    "    train_metrics = set_evaluation(train_windows, train_wparams,\n",
    "                                   train_eval_idxs)\n",
    "    print lead + \"    Train: (mean)\", np.mean(train_metrics, axis = 0)\n",
    "    print lead + \"    Train: (max) \", np.max(train_metrics, axis = 0)\n",
    "    print lead + \"    Train: (min) \", np.min(train_metrics, axis = 0)\n",
    "    \n",
    "    # validation set evaluation\n",
    "    val_metrics = set_evaluation(val_windows, val_wparams,\n",
    "                                 val_eval_idxs)\n",
    "    print lead + \"    Val:   (mean)\", np.mean(val_metrics, axis = 0)\n",
    "    print lead + \"    Val:   (max) \", np.max(val_metrics, axis = 0)\n",
    "    print lead + \"    Val:   (min) \", np.min(val_metrics, axis = 0)\n",
    "    \n",
    "    # returns mean PESQ on validation\n",
    "    return np.mean(val_metrics, axis = 0)[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''\n",
    "startTime = time.time()\n",
    "evaluate_training(autoencoder)\n",
    "elapsed = time.time() - startTime\n",
    "print \"Total time for epoch: \" + str(elapsed) + \"s\"\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train = np.copy(train_processed)\n",
    "ntrain = X_train.shape[0]\n",
    "\n",
    "BATCH_SIZE = 128\n",
    "NUM_EPOCHS = 200\n",
    "\n",
    "ORIG_BITRATE = 256.00\n",
    "TARGET_BITRATE = 16.00\n",
    "PRE_ENTROPY_RATE = ORIG_BITRATE / DOWNSAMPLE_FACTOR\n",
    "\n",
    "TARGET_ENTROPY = (TARGET_BITRATE / PRE_ENTROPY_RATE * 16.0)\n",
    "TARGET_ENTROPY *= (STEP_SIZE / float(WINDOW_SIZE))\n",
    "TARGET_ENTROPY_FUZZ = 0.1\n",
    "\n",
    "TAU_CHANGE_RATE = 0.0125\n",
    "MIN_TAU = 0.0125\n",
    "\n",
    "STARTING_LR = 0.001\n",
    "\n",
    "print \"Target entropy:\", TARGET_ENTROPY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "best_val_pesq = 0.0\n",
    "K.set_value(tau, MIN_TAU)\n",
    "T_i = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "np.set_printoptions(formatter={'float_kind':'{:4f}'.format})\n",
    "lead = \"    \"\n",
    "\n",
    "for epoch in range(1, NUM_EPOCHS + 1):\n",
    "    print \"Epoch \" + str(epoch) + \":\"\n",
    "\n",
    "    # present batches randomly each epoch\n",
    "    lis = range(0, ntrain, BATCH_SIZE)\n",
    "    random.shuffle(lis)\n",
    "    num_batches = len(lis)\n",
    "    \n",
    "    # keep track of start time and current batch #\n",
    "    i = 0\n",
    "    startTime = time.time()\n",
    "    for idx in lis:\n",
    "        # cosine annealing for model's learning rate\n",
    "        train_pct = T_i / float(NUM_EPOCHS)\n",
    "        opt_lr = 0.5 * STARTING_LR * (1 + math.cos(3.14159 * train_pct))\n",
    "        T_i += (1.0 / num_batches)\n",
    "        K.set_value(model.optimizer.lr, opt_lr)\n",
    "        \n",
    "        batch = X_train[idx:idx+BATCH_SIZE, :,  :]\n",
    "        nbatch = batch.shape[0]\n",
    "               \n",
    "        # train autoencoder\n",
    "        a_y = [batch] * n_recons + \\\n",
    "              [np.zeros((nbatch, WINDOW_SIZE, NBINS))] * n_code       \n",
    "\n",
    "        a_losses = model.train_on_batch(batch, a_y)\n",
    "        \n",
    "        # print statistics every 10 batches so we know what's going on\n",
    "        if (i % 10 == 0):\n",
    "            printStr = \"        \\r\" + lead + str(i * BATCH_SIZE) + \": \"\n",
    "            print printStr,\n",
    "            \n",
    "            loss_arr = np.asarray(a_losses)\n",
    "            print loss_arr,\n",
    "            \n",
    "            if (len(loss_weights) > 1 and len(loss_arr) > 1):\n",
    "                for w in xrange(0, len(loss_weights)):\n",
    "                    loss_arr[w + 1] *= loss_weights[w]\n",
    "                print loss_arr,\n",
    "            \n",
    "            print K.get_value(tau), opt_lr,\n",
    "        \n",
    "        i += 1\n",
    "    print \"\"\n",
    "    \n",
    "    # print elapsed time for epoch\n",
    "    elapsed = time.time() - startTime\n",
    "    print lead + \"Total time for epoch: \" + str(elapsed) + \"s\"\n",
    "    \n",
    "    # ---------------------------------------------------------\n",
    "    # estimate code entropy from random samples (if quantization is on)\n",
    "    # ---------------------------------------------------------\n",
    "    NUM = 500\n",
    "    rows = np.random.randint(X_train.shape[0], size = NUM)\n",
    "    code = encoder.predict(X_train[rows, :], verbose = 0)\n",
    "    probs = np.reshape(code, (code.shape[0] * code.shape[1], NBINS))\n",
    "    hist = np.sum(probs, axis = 0)\n",
    "    hist /= np.sum(hist)\n",
    "\n",
    "    entropy = 0\n",
    "    for i in hist:\n",
    "        if (i < 1e-4): continue\n",
    "        entropy += i * math.log(i, 2)\n",
    "    entropy = -entropy\n",
    "\n",
    "    print lead + \"----------------\"\n",
    "    print lead + \"Code entropy:\", entropy\n",
    "    \n",
    "    # ---------------------------------------------------------\n",
    "    # handle updating entropy weight (tau)\n",
    "    # ---------------------------------------------------------\n",
    "    old_tau = K.get_value(tau)\n",
    "\n",
    "    if (entropy < TARGET_ENTROPY - TARGET_ENTROPY_FUZZ):\n",
    "        new_tau = old_tau - TAU_CHANGE_RATE\n",
    "        if (new_tau <= MIN_TAU):\n",
    "            new_tau = MIN_TAU\n",
    "\n",
    "        K.set_value(tau, new_tau)\n",
    "        print lead + \"Updated tau from\", old_tau, \"to\", new_tau\n",
    "    elif (entropy > TARGET_ENTROPY + TARGET_ENTROPY_FUZZ):\n",
    "        new_tau = old_tau + TAU_CHANGE_RATE\n",
    "\n",
    "        K.set_value(tau, new_tau)\n",
    "        print lead + \"Updated tau from\", old_tau, \"to\", new_tau\n",
    "    else:\n",
    "        print lead + \"Tau stays at\", old_tau\n",
    "    \n",
    "    # ---------------------------------------------------------\n",
    "    # evaluate autoencoder on training/validation data evey epoch\n",
    "    # ---------------------------------------------------------\n",
    "    startTime = time.time()\n",
    "    print lead + \"----------------\"\n",
    "    print lead + \"Evaluating autoencoder...\"\n",
    "    \n",
    "    \n",
    "    metrics = test_model_on_wav(\"./SA1.wav\", \"./train_output/SA1_train_epoch\" + str(epoch),\n",
    "                              autoencoder, lead = lead, verbose = False, argmax = False)\n",
    "    print lead + \"SA1:         \", metrics\n",
    "    metrics = test_model_on_wav(\"./SA1.wav\", \"./train_output/SA1_train_epoch\" + str(epoch),\n",
    "                              autoencoder, lead = lead, verbose = False, argmax = True)\n",
    "    print lead + \"SA1 (arg):   \", metrics\n",
    "    \n",
    "    metrics_tst = test_model_on_wav(\"./SX383.wav\", \"./train_output/SX383_train_epoch\" + str(epoch),\n",
    "                                  autoencoder, lead = lead, verbose = False, argmax = False)\n",
    "    print lead + \"SX383:       \", metrics_tst\n",
    "    metrics = test_model_on_wav(\"./SX383.wav\", \"./train_output/SX383_train_epoch\" + str(epoch),\n",
    "                              autoencoder, lead = lead, verbose = False, argmax = True)\n",
    "    print lead + \"SX383 (arg): \", metrics\n",
    "    \n",
    "    val_pesq = evaluate_training(autoencoder, lead)\n",
    "    if (val_pesq > best_val_pesq and entropy <= TARGET_ENTROPY):\n",
    "        print lead + \"NEW best model! Validation mean-PESQ\", val_pesq\n",
    "\n",
    "        print lead + \"Saving model...\"\n",
    "        save_model()\n",
    "        best_val_pesq = val_pesq\n",
    "        patience_epoch = epoch\n",
    "    else:\n",
    "        print lead + \"Best validation mean-PESQ seen:\", best_val_pesq\n",
    "\n",
    "    \n",
    "    elapsed = time.time() - startTime\n",
    "    print lead + \"Total time for evaluation: \" + str(elapsed) + \"s\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if False:\n",
    "    model = load_model('best_model.h5', KERAS_LOAD_MAP)\n",
    "    autoencoder = load_model('best_auto.h5', KERAS_LOAD_MAP)\n",
    "    encoder = autoencoder.layers[1]\n",
    "    decoder = autoencoder.layers[2]\n",
    "    QUANT_BINS = K.variable(np.load('best_quant_bins.npy'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "enc = model.layers[1].layers\n",
    "dec = model.layers[2].layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#for i in xrange(0, len(enc)):\n",
    "#    print i, enc[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "test_model_on_wav(\"./SA1.wav\", \"SA1_final\", autoencoder)\n",
    "test_model_on_wav(\"./SA1.wav\", \"SA1_final\", autoencoder, argmax = True)\n",
    "\n",
    "test_model_on_wav(\"./SX383.wav\", \"SX383_final\", autoencoder)\n",
    "test_model_on_wav(\"./SX383.wav\", \"SX383_final\", autoencoder, argmax = True)\n",
    "\n",
    "test_model_on_wav(\"./fiveYears.wav\", \"fy_final\", autoencoder)\n",
    "test_model_on_wav(\"./fiveYears.wav\", \"fy_final\", autoencoder, argmax = True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "all_embed = encoder.predict(X_train[:10000], batch_size = BATCH_SIZE, verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "probs = np.reshape(all_embed, (all_embed.shape[0] * all_embed.shape[1], NBINS))\n",
    "hist = np.sum(probs, axis = 0)\n",
    "hist /= np.sum(hist)\n",
    "\n",
    "sample_hist_bins = np.linspace(0, NBINS - 1, NBINS)\n",
    "plt.bar(sample_hist_bins, hist, align = 'center', width = 1)\n",
    "plt.show()\n",
    "\n",
    "entropy = 0\n",
    "for i in hist:\n",
    "    if (i < 1e-4): continue\n",
    "    entropy += i * math.log(i, 2)\n",
    "entropy = -entropy\n",
    "print \"Entropy of distribution:\", entropy\n",
    "\n",
    "print \"Bins:\"\n",
    "print K.eval(QUANT_BINS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.plot(np.sort(np.array(K.eval(QUANT_BINS)).flatten()))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "[rate, data] = sciwav.read(\"./SA1.wav\")\n",
    "data = data.astype(np.float32)\n",
    "processedWave, wparams = preprocess_waveform(data)\n",
    "windows = extract_windows(processedWave, STEP_SIZE, OVERLAP_SIZE)\n",
    "\n",
    "transformed = np.reshape(windows, (windows.shape[0], WINDOW_SIZE, 1))\n",
    "embed = encoder.predict(transformed, batch_size = BATCH_SIZE, verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "recons = decoder.predict(embed, batch_size = BATCH_SIZE, verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "K.eval(enc[-1].SOFTMAX_TEMP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "max_pct = np.max(embed[25], axis = -1)\n",
    "print max_pct\n",
    "print np.argmax(embed[25], axis = -1)\n",
    "print np.sum(max_pct > 0.98) / float(max_pct.size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "embed_max = np.max(embed, axis = -1)\n",
    "print np.mean(embed_max)\n",
    "print np.sum(embed_max > 0.98) / float(embed_max.size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "idx = 25\n",
    "\n",
    "orig = windows[idx].flatten()\n",
    "recn = recons[idx].flatten()\n",
    "\n",
    "print \"Original\"\n",
    "plt.plot(orig)\n",
    "ylim = plt.gca().get_ylim()\n",
    "plt.show()\n",
    "\n",
    "print \"Reconstruction\"\n",
    "plt.plot(recn)\n",
    "plt.ylim(ylim)\n",
    "plt.show()\n",
    "\n",
    "print \"Code (argmax)\"\n",
    "argmax_code_vec = embed[idx]\n",
    "embed_sum = np.sum(embed[idx], axis = -1)\n",
    "argmax_code_vec = np.eye(NBINS)[np.argmax(argmax_code_vec, axis = -1)]\n",
    "argmax_code_vec[embed_sum < 0.95] = np.zeros(NBINS)\n",
    "argmax_code_vec = unquantize_vec(argmax_code_vec)\n",
    "plt.plot(argmax_code_vec)\n",
    "plt.show()\n",
    "\n",
    "print \"Code (non-argmax)\"\n",
    "na_code_vec = embed[idx]\n",
    "na_code_vec = unquantize_vec(na_code_vec)\n",
    "plt.plot(na_code_vec)\n",
    "plt.show()\n",
    "\n",
    "print \"Difference\"\n",
    "plt.plot(abs(argmax_code_vec - na_code_vec))\n",
    "plt.show()\n",
    "    \n",
    "print \"Error\"\n",
    "plt.plot(abs(orig - recn))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
