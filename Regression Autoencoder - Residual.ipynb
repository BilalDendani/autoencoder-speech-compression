{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n",
      "Using gpu device 1: GeForce GTX 690 (CNMeM is disabled, cuDNN 5105)\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import random\n",
    "import h5py\n",
    "from keras.datasets import cifar10\n",
    "from keras.models import *\n",
    "from keras.layers import *\n",
    "from keras.layers.core import *\n",
    "from keras.layers.normalization import *\n",
    "from keras.optimizers import *\n",
    "from keras.callbacks import *\n",
    "from keras import backend as K\n",
    "from keras.regularizers import *\n",
    "import theano.tensor as T\n",
    "import theano\n",
    "from theano.tensor.shared_randomstreams import RandomStreams\n",
    "from sklearn import metrics\n",
    "from skimage.measure import compare_ssim\n",
    "from scipy.misc import toimage\n",
    "from sklearn.preprocessing import *\n",
    "\n",
    "import os\n",
    "import random\n",
    "import time\n",
    "from skimage import io, exposure, feature, color, transform\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "\n",
    "import scipy.signal as sig\n",
    "import operator\n",
    "import math\n",
    "\n",
    "# for reproducibility\n",
    "np.random.seed(1337) \n",
    "random.seed(1337)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# parameters for sliding window, and window function (Hann)\n",
    "STEP_SIZE = 480\n",
    "OVERLAP_SIZE = 32\n",
    "WINDOW_SIZE = STEP_SIZE + OVERLAP_SIZE\n",
    "OVERLAP_FUNC = sig.hann(OVERLAP_SIZE * 2)\n",
    "\n",
    "# directory that contains TIMIT files\n",
    "TIMIT_DIR = \"/home/sri/Desktop/timit\"\n",
    "\n",
    "# randomly shuffle data before partitioning into training/validation?\n",
    "RANDOM_SHUFFLE = True\n",
    "\n",
    "# sample rate of input file (used in MFCC calculation)\n",
    "SAMPLE_RATE = 16000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from load_TIMIT import *\n",
    "from windowingFunctions import *\n",
    "from utility import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading in .wav files...\n",
      "0: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MKLR0/SA1.WAV\r",
      "1: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MKLR0/SX339.WAV\r",
      "2: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MKLR0/SI1059.WAV\r",
      "3: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MKLR0/SI1689.WAV\r",
      "4: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MKLR0/SX429.WAV\r",
      "5: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MKLR0/SX69.WAV\r",
      "6: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MKLR0/SX159.WAV\r",
      "7: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MKLR0/SI2319.WAV\r",
      "8: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MKLR0/SA2.WAV\r",
      "9: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MKLR0/SX249.WAV\r",
      "10: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MCRE0/SX221.WAV\r",
      "11: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MCRE0/SA1.WAV\r",
      "12: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MCRE0/SX41.WAV\r",
      "13: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MCRE0/SI1751.WAV\r",
      "14: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MCRE0/SI1725.WAV\r",
      "15: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MCRE0/SX401.WAV\r",
      "16: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MCRE0/SX24.WAV\r",
      "17: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MCRE0/SA2.WAV\r",
      "18: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MCRE0/SI1121.WAV\r",
      "19: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MCRE0/SX131.WAV\r",
      "20: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MJRA0/SA1.WAV\r",
      "21: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MJRA0/SX336.WAV\r",
      "22: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MJRA0/SI1236.WAV\r",
      "23: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MJRA0/SI606.WAV\r",
      "24: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MJRA0/SI1866.WAV\r",
      "25: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MJRA0/SX156.WAV\r",
      "26: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MJRA0/SA2.WAV\r",
      "27: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MJRA0/SX426.WAV\r",
      "28: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MJRA0/SX246.WAV\r",
      "29: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MJRA0/SX66.WAV\r",
      "30: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MDLR0/SA1.WAV\r",
      "31: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MDLR0/SI1233.WAV\r",
      "32: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MDLR0/SI603.WAV\r",
      "33: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MDLR0/SX333.WAV\r",
      "34: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MDLR0/SX153.WAV\r",
      "35: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MDLR0/SI1863.WAV\r",
      "36: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MDLR0/SX243.WAV\r",
      "37: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MDLR0/SA2.WAV\r",
      "38: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MDLR0/SX423.WAV\r",
      "39: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MDLR0/SX63.WAV\r",
      "40: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MTMN0/SA1.WAV\r",
      "41: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MTMN0/SX434.WAV\r",
      "42: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MTMN0/SX74.WAV\r",
      "43: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MTMN0/SX254.WAV\r",
      "44: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MTMN0/SI1064.WAV\r",
      "45: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MTMN0/SX344.WAV\r",
      "46: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MTMN0/SX164.WAV\r",
      "47: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MTMN0/SA2.WAV\r",
      "48: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MTMN0/SI582.WAV\r",
      "49: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MTMN0/SI2324.WAV\r",
      "50: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/FMKC0/SX82.WAV\r",
      "51: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/FMKC0/SI1041.WAV\r",
      "52: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/FMKC0/SX442.WAV\r",
      "53: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/FMKC0/SA1.WAV\r",
      "54: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/FMKC0/SI1702.WAV\r",
      "55: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/FMKC0/SX262.WAV\r",
      "56: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/FMKC0/SX172.WAV\r",
      "57: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/FMKC0/SX352.WAV\r",
      "58: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/FMKC0/SA2.WAV\r",
      "59: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/FMKC0/SI1072.WAV\r",
      "60: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MDLR1/SI669.WAV\r",
      "61: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MDLR1/SA1.WAV\r",
      "62: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MDLR1/SX39.WAV\r",
      "63: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MDLR1/SX309.WAV\r",
      "64: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MDLR1/SI1929.WAV\r",
      "65: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MDLR1/SX129.WAV\r",
      "66: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MDLR1/SI1299.WAV\r",
      "67: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MDLR1/SX219.WAV\r",
      "68: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MDLR1/SX399.WAV\r",
      "69: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MDLR1/SA2.WAV\r",
      "70: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MTML0/SI2325.WAV\r",
      "71: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MTML0/SX435.WAV\r",
      "72: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MTML0/SX75.WAV\r",
      "73: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MTML0/SA1.WAV\r",
      "74: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MTML0/SI1695.WAV\r",
      "75: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MTML0/SX345.WAV\r",
      "76: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MTML0/SI1065.WAV\r",
      "77: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MTML0/SX255.WAV\r",
      "78: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MTML0/SX165.WAV\r",
      "79: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MTML0/SA2.WAV\r",
      "80: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MCLK0/SI2290.WAV\r",
      "81: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MCLK0/SA1.WAV\r",
      "82: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MCLK0/SI650.WAV\r",
      "83: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MCLK0/SI1660.WAV\r",
      "84: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MCLK0/SX220.WAV\r",
      "85: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MCLK0/SX310.WAV\r",
      "86: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MCLK0/SX40.WAV\r",
      "87: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MCLK0/SX400.WAV\r",
      "88: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MCLK0/SA2.WAV\r",
      "89: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MCLK0/SX130.WAV\r",
      "90: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MSAH1/SX149.WAV\r",
      "91: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MSAH1/SA1.WAV\r",
      "92: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MSAH1/SI2309.WAV\r",
      "93: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MSAH1/SX419.WAV\r",
      "94: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MSAH1/SX239.WAV\r",
      "95: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MSAH1/SX329.WAV\r",
      "96: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MSAH1/SI1679.WAV\r",
      "97: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MSAH1/SX59.WAV\r",
      "98: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MSAH1/SA2.WAV\r",
      "99: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MSAH1/SI1049.WAV\r",
      "100: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MMDG0/SI520.WAV\r",
      "101: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MMDG0/SA1.WAV\r",
      "102: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MMDG0/SX160.WAV\r",
      "103: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MMDG0/SX340.WAV\r",
      "104: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MMDG0/SI2035.WAV\r",
      "105: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MMDG0/SX250.WAV\r",
      "106: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MMDG0/SX430.WAV\r",
      "107: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MMDG0/SX70.WAV\r",
      "108: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MMDG0/SA2.WAV\r",
      "109: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MMDG0/SI1780.WAV\r",
      "110: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MBBR0/SA1.WAV\r",
      "111: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MBBR0/SX65.WAV\r",
      "112: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MBBR0/SX425.WAV\r",
      "113: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MBBR0/SI1685.WAV\r",
      "114: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MBBR0/SI1055.WAV\r",
      "115: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MBBR0/SX335.WAV\r",
      "116: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MBBR0/SA2.WAV\r",
      "117: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MBBR0/SX245.WAV\r",
      "118: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MBBR0/SI2315.WAV\r",
      "119: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MBBR0/SX155.WAV\r",
      "120: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/FMAH1/SA1.WAV\r",
      "121: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/FMAH1/SX339.WAV\r",
      "122: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/FMAH1/SI879.WAV\r",
      "123: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/FMAH1/SX429.WAV\r",
      "124: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/FMAH1/SI1509.WAV\r",
      "125: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/FMAH1/SX69.WAV\r",
      "126: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/FMAH1/SX159.WAV\r",
      "127: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/FMAH1/SI2139.WAV\r",
      "128: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/FMAH1/SA2.WAV\r",
      "129: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/FMAH1/SX249.WAV\r",
      "130: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MDKS0/SX166.WAV\r",
      "131: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MDKS0/SA1.WAV\r",
      "132: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MDKS0/SX346.WAV\r",
      "133: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MDKS0/SI2326.WAV\r",
      "134: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MDKS0/SI1696.WAV\r",
      "135: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MDKS0/SX256.WAV\r",
      "136: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MDKS0/SI1066.WAV\r",
      "137: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MDKS0/SA2.WAV\r",
      "138: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MDKS0/SX76.WAV\r",
      "139: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MDKS0/SX436.WAV\r",
      "140: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MDPB0/SX56.WAV\r",
      "141: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MDPB0/SA1.WAV\r",
      "142: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MDPB0/SI866.WAV\r",
      "143: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MDPB0/SX146.WAV\r",
      "144: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MDPB0/SX326.WAV\r",
      "145: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MDPB0/SI2126.WAV\r",
      "146: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MDPB0/SX416.WAV\r",
      "147: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MDPB0/SX236.WAV\r",
      "148: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MDPB0/SI1760.WAV\r",
      "149: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MDPB0/SA2.WAV\r",
      "150: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/FJRP1/SX82.WAV\r",
      "151: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/FJRP1/SX442.WAV\r",
      "152: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/FJRP1/SA1.WAV\r",
      "153: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/FJRP1/SI2062.WAV\r",
      "154: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/FJRP1/SI1432.WAV\r",
      "155: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/FJRP1/SX262.WAV\r",
      "156: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/FJRP1/SI802.WAV\r",
      "157: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/FJRP1/SX172.WAV\r",
      "158: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/FJRP1/SX352.WAV\r",
      "159: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/FJRP1/SA2.WAV\r",
      "160: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MBOM0/SX311.WAV\r",
      "161: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MBOM0/SA1.WAV\r",
      "162: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MBOM0/SI2274.WAV\r",
      "163: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MBOM0/SX114.WAV\r",
      "164: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MBOM0/SX384.WAV\r",
      "165: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MBOM0/SX294.WAV\r",
      "166: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MBOM0/SI1014.WAV\r",
      "167: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MBOM0/SA2.WAV\r",
      "168: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MBOM0/SI1644.WAV\r",
      "169: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MBOM0/SX204.WAV\r",
      "170: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MTAB0/SX42.WAV\r",
      "171: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MTAB0/SX222.WAV\r",
      "172: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MTAB0/SA1.WAV\r",
      "173: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MTAB0/SX312.WAV\r",
      "174: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MTAB0/SI1572.WAV\r",
      "175: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MTAB0/SI942.WAV\r",
      "176: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MTAB0/SX132.WAV\r",
      "177: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MTAB0/SA2.WAV\r",
      "178: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MTAB0/SI2202.WAV\r",
      "179: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MTAB0/SX402.WAV\r",
      "180: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MBAR0/SX149.WAV\r",
      "181: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MBAR0/SA1.WAV\r",
      "182: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MBAR0/SI1949.WAV\r",
      "183: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MBAR0/SI689.WAV\r",
      "184: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MBAR0/SI1319.WAV\r",
      "185: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MBAR0/SX419.WAV\r",
      "186: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MBAR0/SX239.WAV\r",
      "187: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MBAR0/SX329.WAV\r",
      "188: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MBAR0/SX59.WAV\r",
      "189: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MBAR0/SA2.WAV\r",
      "190: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MTKD0/SI1187.WAV\r",
      "191: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MTKD0/SX17.WAV\r",
      "192: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MTKD0/SX107.WAV\r",
      "193: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MTKD0/SA1.WAV\r",
      "194: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MTKD0/SI630.WAV\r",
      "195: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MTKD0/SI1817.WAV\r",
      "196: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MTKD0/SX287.WAV\r",
      "197: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MTKD0/SX197.WAV\r",
      "198: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MTKD0/SA2.WAV\r",
      "199: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MTKD0/SX377.WAV\r",
      "200: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/FBLV0/SX248.WAV\r",
      "201: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/FBLV0/SX428.WAV\r",
      "202: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/FBLV0/SA1.WAV\r",
      "203: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/FBLV0/SI2318.WAV\r",
      "204: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/FBLV0/SX68.WAV\r",
      "205: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/FBLV0/SI1058.WAV\r",
      "206: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/FBLV0/SX338.WAV\r",
      "207: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/FBLV0/SX158.WAV\r",
      "208: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/FBLV0/SI1688.WAV\r",
      "209: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/FBLV0/SA2.WAV\r",
      "210: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MGSL0/SI534.WAV\r",
      "211: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MGSL0/SI1164.WAV\r",
      "212: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MGSL0/SA1.WAV\r",
      "213: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MGSL0/SX354.WAV\r",
      "214: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MGSL0/SX174.WAV\r",
      "215: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MGSL0/SX264.WAV\r",
      "216: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MGSL0/SI797.WAV\r",
      "217: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MGSL0/SX444.WAV\r",
      "218: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MGSL0/SX84.WAV\r",
      "219: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MGSL0/SA2.WAV\r",
      "220: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MAFM0/SA1.WAV\r",
      "221: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MAFM0/SX39.WAV\r",
      "222: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MAFM0/SI2199.WAV\r",
      "223: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MAFM0/SX309.WAV\r",
      "224: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MAFM0/SI1569.WAV\r",
      "225: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MAFM0/SX129.WAV\r",
      "226: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MAFM0/SX219.WAV\r",
      "227: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MAFM0/SI939.WAV\r",
      "228: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MAFM0/SX399.WAV\r",
      "229: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MAFM0/SA2.WAV\r",
      "230: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MKAG0/SI1609.WAV\r",
      "231: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MKAG0/SX30.WAV\r",
      "232: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MKAG0/SX169.WAV\r",
      "233: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MKAG0/SA1.WAV\r",
      "234: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MKAG0/SX439.WAV\r",
      "235: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MKAG0/SI979.WAV\r",
      "236: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MKAG0/SX259.WAV\r",
      "237: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MKAG0/SA2.WAV\r",
      "238: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MKAG0/SI2239.WAV\r",
      "239: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MKAG0/SX79.WAV\r",
      "240: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MRPC1/SX42.WAV\r",
      "241: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MRPC1/SX222.WAV\r",
      "242: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MRPC1/SI2112.WAV\r",
      "243: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MRPC1/SA1.WAV\r",
      "244: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MRPC1/SX312.WAV\r",
      "245: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MRPC1/SI2026.WAV\r",
      "246: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MRPC1/SX132.WAV\r",
      "247: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MRPC1/SA2.WAV\r",
      "248: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MRPC1/SI1482.WAV\r",
      "249: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MRPC1/SX402.WAV\r",
      "250: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MMWS1/SX81.WAV\r",
      "251: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MMWS1/SA1.WAV\r",
      "252: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MMWS1/SX351.WAV\r",
      "253: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MMWS1/SI2331.WAV\r",
      "254: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MMWS1/SX27.WAV\r",
      "255: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MMWS1/SI1071.WAV\r",
      "256: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MMWS1/SX441.WAV\r",
      "257: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MMWS1/SX261.WAV\r",
      "258: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MMWS1/SI1701.WAV\r",
      "259: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MMWS1/SA2.WAV\r",
      "260: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/FLEH0/SA1.WAV\r",
      "261: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/FLEH0/SX241.WAV\r",
      "262: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/FLEH0/SX331.WAV\r",
      "263: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/FLEH0/SX61.WAV\r",
      "264: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/FLEH0/SX151.WAV\r",
      "265: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/FLEH0/SI1681.WAV\r",
      "266: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/FLEH0/SX421.WAV\r",
      "267: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/FLEH0/SI2311.WAV\r",
      "268: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/FLEH0/SA2.WAV\r",
      "269: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/FLEH0/SI1051.WAV\r",
      "270: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/FCJS0/SX167.WAV\r",
      "271: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/FCJS0/SX437.WAV\r",
      "272: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/FCJS0/SA1.WAV\r",
      "273: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/FCJS0/SX77.WAV\r",
      "274: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/FCJS0/SX257.WAV\r",
      "275: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/FCJS0/SX347.WAV\r",
      "276: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/FCJS0/SI1607.WAV\r",
      "277: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/FCJS0/SA2.WAV\r",
      "278: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/FCJS0/SI977.WAV\r",
      "279: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/FCJS0/SI2237.WAV\r",
      "280: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MVRW0/SI1485.WAV\r",
      "281: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MVRW0/SA1.WAV\r",
      "282: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MVRW0/SI855.WAV\r",
      "283: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MVRW0/SX225.WAV\r",
      "284: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MVRW0/SX405.WAV\r",
      "285: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MVRW0/SA2.WAV\r",
      "286: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MVRW0/SX45.WAV\r",
      "287: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MVRW0/SX315.WAV\r",
      "288: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MVRW0/SX135.WAV\r",
      "289: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MVRW0/SI2115.WAV\r",
      "290: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MNTW0/SI1068.WAV\r",
      "291: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MNTW0/SA1.WAV\r",
      "292: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MNTW0/SX78.WAV\r",
      "293: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MNTW0/SX202.WAV\r",
      "294: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MNTW0/SX258.WAV\r",
      "295: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MNTW0/SX168.WAV\r",
      "296: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MNTW0/SI1698.WAV\r",
      "297: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MNTW0/SI2328.WAV\r",
      "298: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MNTW0/SA2.WAV\r",
      "299: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MNTW0/SX348.WAV\r",
      "300: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MCTH0/SI579.WAV\r",
      "301: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MCTH0/SA1.WAV\r",
      "302: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MCTH0/SX39.WAV\r",
      "303: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MCTH0/SX309.WAV\r",
      "304: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MCTH0/SX129.WAV\r",
      "305: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MCTH0/SX219.WAV\r",
      "306: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MCTH0/SX399.WAV\r",
      "307: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MCTH0/SI1839.WAV\r",
      "308: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MCTH0/SA2.WAV\r",
      "309: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MCTH0/SI1209.WAV\r",
      "310: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/FSPM0/SA1.WAV\r",
      "311: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/FSPM0/SX161.WAV\r",
      "312: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/FSPM0/SX71.WAV\r",
      "313: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/FSPM0/SX431.WAV\r",
      "314: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/FSPM0/SX341.WAV\r",
      "315: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/FSPM0/SI611.WAV\r",
      "316: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/FSPM0/SX251.WAV\r",
      "317: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/FSPM0/SI1241.WAV\r",
      "318: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/FSPM0/SA2.WAV\r",
      "319: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/FSPM0/SI1871.WAV\r",
      "320: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MGAW0/SA1.WAV\r",
      "321: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MGAW0/SX355.WAV\r",
      "322: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MGAW0/SX445.WAV\r",
      "323: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MGAW0/SX265.WAV\r",
      "324: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MGAW0/SX85.WAV\r",
      "325: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MGAW0/SI1165.WAV\r",
      "326: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MGAW0/SI1802.WAV\r",
      "327: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MGAW0/SX175.WAV\r",
      "328: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MGAW0/SI535.WAV\r",
      "329: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MGAW0/SA2.WAV\r",
      "330: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MKDB0/SX62.WAV\r",
      "331: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MKDB0/SI872.WAV\r",
      "332: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MKDB0/SA1.WAV\r",
      "333: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MKDB0/SI2132.WAV\r",
      "334: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MKDB0/SI588.WAV\r",
      "335: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MKDB0/SX152.WAV\r",
      "336: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MKDB0/SX332.WAV\r",
      "337: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MKDB0/SX242.WAV\r",
      "338: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MKDB0/SA2.WAV\r",
      "339: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MKDB0/SX422.WAV\r",
      "340: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MRLJ1/SA1.WAV\r",
      "341: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MRLJ1/SI1671.WAV\r",
      "342: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MRLJ1/SX321.WAV\r",
      "343: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MRLJ1/SI2332.WAV\r",
      "344: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MRLJ1/SX141.WAV\r",
      "345: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MRLJ1/SX231.WAV\r",
      "346: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MRLJ1/SX411.WAV\r",
      "347: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MRLJ1/SX51.WAV\r",
      "348: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MRLJ1/SA2.WAV\r",
      "349: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MRLJ1/SI2301.WAV\r",
      "350: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/FCRZ0/SA1.WAV\r",
      "351: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/FCRZ0/SX433.WAV\r",
      "352: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/FCRZ0/SX343.WAV\r",
      "353: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/FCRZ0/SX73.WAV\r",
      "354: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/FCRZ0/SX163.WAV\r",
      "355: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/FCRZ0/SI793.WAV\r",
      "356: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/FCRZ0/SI1913.WAV\r",
      "357: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/FCRZ0/SA2.WAV\r",
      "358: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/FCRZ0/SX253.WAV\r",
      "359: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/FCRZ0/SI2053.WAV\r",
      "360: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MFXS0/SX234.WAV\r",
      "361: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MFXS0/SA1.WAV\r",
      "362: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MFXS0/SX324.WAV\r",
      "363: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MFXS0/SI2225.WAV\r",
      "364: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MFXS0/SI2304.WAV\r",
      "365: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MFXS0/SI1674.WAV\r",
      "366: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MFXS0/SX54.WAV\r",
      "367: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MFXS0/SX414.WAV\r",
      "368: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MFXS0/SX144.WAV\r",
      "369: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MFXS0/SA2.WAV\r",
      "370: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/FREH0/SA1.WAV\r",
      "371: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/FREH0/SX235.WAV\r",
      "372: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/FREH0/SX145.WAV\r",
      "373: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/FREH0/SX55.WAV\r",
      "374: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/FREH0/SX325.WAV\r",
      "375: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/FREH0/SX415.WAV\r",
      "376: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/FREH0/SI685.WAV\r",
      "377: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/FREH0/SI1315.WAV\r",
      "378: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/FREH0/SA2.WAV\r",
      "379: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/FREH0/SI1945.WAV\r",
      "380: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MADD0/SA1.WAV\r",
      "381: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MADD0/SI1295.WAV\r",
      "382: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MADD0/SI538.WAV\r",
      "383: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MADD0/SI1798.WAV\r",
      "384: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MADD0/SX268.WAV\r",
      "385: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MADD0/SX178.WAV\r",
      "386: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MADD0/SA2.WAV\r",
      "387: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MADD0/SX448.WAV\r",
      "388: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MADD0/SX358.WAV\r",
      "389: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MADD0/SX88.WAV\r",
      "390: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MTLC0/SX127.WAV\r",
      "391: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MTLC0/SA1.WAV\r",
      "392: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MTLC0/SX217.WAV\r",
      "393: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MTLC0/SI847.WAV\r",
      "394: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MTLC0/SX307.WAV\r",
      "395: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MTLC0/SI1477.WAV\r",
      "396: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MTLC0/SX397.WAV\r",
      "397: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MTLC0/SA2.WAV\r",
      "398: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MTLC0/SX37.WAV\r",
      "399: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MTLC0/SI1313.WAV\r",
      "400: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MFXV0/SA1.WAV\r",
      "401: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MFXV0/SI1005.WAV\r",
      "402: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MFXV0/SX15.WAV\r",
      "403: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MFXV0/SI1342.WAV\r",
      "404: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MFXV0/SX285.WAV\r",
      "405: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MFXV0/SI1635.WAV\r",
      "406: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MFXV0/SX105.WAV\r",
      "407: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MFXV0/SX195.WAV\r",
      "408: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MFXV0/SA2.WAV\r",
      "409: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MFXV0/SX375.WAV\r",
      "410: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MREM0/SI961.WAV\r",
      "411: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MREM0/SA1.WAV\r",
      "412: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MREM0/SX241.WAV\r",
      "413: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MREM0/SI1591.WAV\r",
      "414: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MREM0/SX331.WAV\r",
      "415: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MREM0/SX61.WAV\r",
      "416: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MREM0/SX151.WAV\r",
      "417: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MREM0/SX421.WAV\r",
      "418: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MREM0/SA2.WAV\r",
      "419: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MREM0/SI511.WAV\r",
      "420: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MJJM0/SI1251.WAV\r",
      "421: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MJJM0/SX17.WAV\r",
      "422: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MJJM0/SX107.WAV\r",
      "423: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MJJM0/SA1.WAV\r",
      "424: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MJJM0/SI1457.WAV\r",
      "425: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MJJM0/SX287.WAV\r",
      "426: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MJJM0/SI827.WAV\r",
      "427: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MJJM0/SX197.WAV\r",
      "428: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MJJM0/SA2.WAV\r",
      "429: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MJJM0/SX377.WAV\r",
      "430: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MPFU0/SA1.WAV\r",
      "431: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MPFU0/SI1888.WAV\r",
      "432: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MPFU0/SX268.WAV\r",
      "433: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MPFU0/SI628.WAV\r",
      "434: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MPFU0/SX178.WAV\r",
      "435: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MPFU0/SA2.WAV\r",
      "436: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MPFU0/SX448.WAV\r",
      "437: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MPFU0/SX358.WAV\r",
      "438: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MPFU0/SX88.WAV\r",
      "439: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MPFU0/SI1258.WAV\r",
      "440: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MWRE0/SX157.WAV\r",
      "441: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MWRE0/SA1.WAV\r",
      "442: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MWRE0/SX337.WAV\r",
      "443: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MWRE0/SX67.WAV\r",
      "444: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MWRE0/SI1687.WAV\r",
      "445: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MWRE0/SI2317.WAV\r",
      "446: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MWRE0/SX427.WAV\r",
      "447: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MWRE0/SA2.WAV\r",
      "448: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MWRE0/SI1057.WAV\r",
      "449: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MWRE0/SX247.WAV\r",
      "450: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MDED0/SA1.WAV\r",
      "451: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MDED0/SI1170.WAV\r",
      "452: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MDED0/SX270.WAV\r",
      "453: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MDED0/SI1800.WAV\r",
      "454: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MDED0/SX360.WAV\r",
      "455: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MDED0/SX450.WAV\r",
      "456: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MDED0/SX90.WAV\r",
      "457: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MDED0/SI540.WAV\r",
      "458: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MDED0/SA2.WAV\r",
      "459: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MDED0/SX180.WAV\r",
      "460: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MJFR0/SX435.WAV\r",
      "461: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MJFR0/SX75.WAV\r",
      "462: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MJFR0/SA1.WAV\r",
      "463: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MJFR0/SI1605.WAV\r",
      "464: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MJFR0/SX345.WAV\r",
      "465: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MJFR0/SX255.WAV\r",
      "466: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MJFR0/SI975.WAV\r",
      "467: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MJFR0/SX165.WAV\r",
      "468: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MJFR0/SI2235.WAV\r",
      "469: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MJFR0/SA2.WAV\r",
      "470: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MBML0/SI539.WAV\r",
      "471: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MBML0/SA1.WAV\r",
      "472: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MBML0/SI1169.WAV\r",
      "473: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MBML0/SX359.WAV\r",
      "474: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MBML0/SX449.WAV\r",
      "475: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MBML0/SX179.WAV\r",
      "476: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MBML0/SX269.WAV\r",
      "477: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MBML0/SA2.WAV\r",
      "478: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MBML0/SI1799.WAV\r",
      "479: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MBML0/SX89.WAV\r",
      "480: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/FJHK0/SA1.WAV\r",
      "481: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/FJHK0/SX32.WAV\r",
      "482: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/FJHK0/SX212.WAV\r",
      "483: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/FJHK0/SX392.WAV\r",
      "484: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/FJHK0/SI1652.WAV\r",
      "485: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/FJHK0/SX302.WAV\r",
      "486: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/FJHK0/SX122.WAV\r",
      "487: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/FJHK0/SI2282.WAV\r",
      "488: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/FJHK0/SI1022.WAV\r",
      "489: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/FJHK0/SA2.WAV\r",
      "490: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/FKSR0/SA1.WAV\r",
      "491: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/FKSR0/SX161.WAV\r",
      "492: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/FKSR0/SX217.WAV\r",
      "493: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/FKSR0/SI1747.WAV\r",
      "494: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/FKSR0/SI1117.WAV\r",
      "495: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/FKSR0/SX366.WAV\r",
      "496: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/FKSR0/SX397.WAV\r",
      "497: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/FKSR0/SA2.WAV\r",
      "498: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/FKSR0/SX37.WAV\r",
      "499: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/FKSR0/SI487.WAV\r",
      "500: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/FLET0/SX237.WAV\r",
      "501: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/FLET0/SA1.WAV\r",
      "502: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/FLET0/SX277.WAV\r",
      "503: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/FLET0/SX147.WAV\r",
      "504: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/FLET0/SI1137.WAV\r",
      "505: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/FLET0/SX417.WAV\r",
      "506: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/FLET0/SI507.WAV\r",
      "507: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/FLET0/SI1767.WAV\r",
      "508: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/FLET0/SA2.WAV\r",
      "509: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/FLET0/SX57.WAV\r",
      "510: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MTWH1/SI882.WAV\r",
      "511: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MTWH1/SA1.WAV\r",
      "512: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MTWH1/SX252.WAV\r",
      "513: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MTWH1/SX342.WAV\r",
      "514: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MTWH1/SI1512.WAV\r",
      "515: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MTWH1/SI2142.WAV\r",
      "516: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MTWH1/SX162.WAV\r",
      "517: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MTWH1/SX72.WAV\r",
      "518: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MTWH1/SX432.WAV\r",
      "519: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MTWH1/SA2.WAV\r",
      "520: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MHBS0/SI1575.WAV\r",
      "521: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MHBS0/SA1.WAV\r",
      "522: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MHBS0/SX225.WAV\r",
      "523: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MHBS0/SX405.WAV\r",
      "524: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MHBS0/SI945.WAV\r",
      "525: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MHBS0/SA2.WAV\r",
      "526: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MHBS0/SX45.WAV\r",
      "527: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MHBS0/SX315.WAV\r",
      "528: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MHBS0/SX135.WAV\r",
      "529: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MHBS0/SI2205.WAV\r",
      "530: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/FJSK0/SX62.WAV\r",
      "531: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/FJSK0/SA1.WAV\r",
      "532: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/FJSK0/SI1682.WAV\r",
      "533: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/FJSK0/SX152.WAV\r",
      "534: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/FJSK0/SX332.WAV\r",
      "535: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/FJSK0/SI1052.WAV\r",
      "536: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/FJSK0/SX242.WAV\r",
      "537: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/FJSK0/SI2312.WAV\r",
      "538: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/FJSK0/SA2.WAV\r",
      "539: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/FJSK0/SX422.WAV\r",
      "540: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MDCM0/SA1.WAV\r",
      "541: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MDCM0/SX220.WAV\r",
      "542: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MDCM0/SI850.WAV\r",
      "543: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MDCM0/SI1480.WAV\r",
      "544: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MDCM0/SX310.WAV\r",
      "545: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MDCM0/SI2110.WAV\r",
      "546: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MDCM0/SX40.WAV\r",
      "547: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MDCM0/SX400.WAV\r",
      "548: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MDCM0/SA2.WAV\r",
      "549: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MDCM0/SX130.WAV\r",
      "550: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MDLM0/SI1234.WAV\r",
      "551: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MDLM0/SX64.WAV\r",
      "552: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MDLM0/SI604.WAV\r",
      "553: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MDLM0/SA1.WAV\r",
      "554: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MDLM0/SI1864.WAV\r",
      "555: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MDLM0/SX334.WAV\r",
      "556: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MDLM0/SX244.WAV\r",
      "557: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MDLM0/SX424.WAV\r",
      "558: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MDLM0/SA2.WAV\r",
      "559: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MDLM0/SX154.WAV\r",
      "560: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MSDB0/SI2267.WAV\r",
      "561: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MSDB0/SX17.WAV\r",
      "562: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MSDB0/SX107.WAV\r",
      "563: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MSDB0/SA1.WAV\r",
      "564: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MSDB0/SI1007.WAV\r",
      "565: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MSDB0/SX287.WAV\r",
      "566: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MSDB0/SX197.WAV\r",
      "567: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MSDB0/SA2.WAV\r",
      "568: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MSDB0/SI1637.WAV\r",
      "569: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MSDB0/SX377.WAV\r",
      "570: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MSES0/SX149.WAV\r",
      "571: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MSES0/SA1.WAV\r",
      "572: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MSES0/SI1589.WAV\r",
      "573: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MSES0/SX419.WAV\r",
      "574: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MSES0/SX239.WAV\r",
      "575: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MSES0/SX329.WAV\r",
      "576: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MSES0/SX59.WAV\r",
      "577: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MSES0/SI2219.WAV\r",
      "578: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MSES0/SI2216.WAV\r",
      "579: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MSES0/SA2.WAV\r",
      "580: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MRMH0/SI2281.WAV\r",
      "581: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MRMH0/SA1.WAV\r",
      "582: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MRMH0/SX121.WAV\r",
      "583: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MRMH0/SX391.WAV\r",
      "584: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MRMH0/SI1021.WAV\r",
      "585: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MRMH0/SX211.WAV\r",
      "586: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MRMH0/SI1349.WAV\r",
      "587: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MRMH0/SX301.WAV\r",
      "588: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MRMH0/SA2.WAV\r",
      "589: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MRMH0/SX31.WAV\r",
      "590: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/FKDE0/SA1.WAV\r",
      "591: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/FKDE0/SX241.WAV\r",
      "592: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/FKDE0/SI1771.WAV\r",
      "593: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/FKDE0/SX331.WAV\r",
      "594: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/FKDE0/SX61.WAV\r",
      "595: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/FKDE0/SI2221.WAV\r",
      "596: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/FKDE0/SX151.WAV\r",
      "597: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/FKDE0/SX421.WAV\r",
      "598: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/FKDE0/SA2.WAV\r",
      "599: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/FKDE0/SI1141.WAV\r",
      "600: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MPAR0/SA1.WAV\r",
      "601: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MPAR0/SX136.WAV\r",
      "602: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MPAR0/SX406.WAV\r",
      "603: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MPAR0/SI2206.WAV\r",
      "604: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MPAR0/SI946.WAV\r",
      "605: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MPAR0/SX316.WAV\r",
      "606: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MPAR0/SX226.WAV\r",
      "607: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MPAR0/SX46.WAV\r",
      "608: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MPAR0/SI1576.WAV\r",
      "609: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MPAR0/SA2.WAV\r",
      "610: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/FPAB1/SA1.WAV\r",
      "611: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/FPAB1/SX121.WAV\r",
      "612: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/FPAB1/SX391.WAV\r",
      "613: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/FPAB1/SX211.WAV\r",
      "614: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/FPAB1/SI2101.WAV\r",
      "615: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/FPAB1/SI1471.WAV\r",
      "616: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/FPAB1/SX301.WAV\r",
      "617: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/FPAB1/SA2.WAV\r",
      "618: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/FPAB1/SI841.WAV\r",
      "619: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/FPAB1/SX31.WAV\r",
      "620: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MWRP0/SI1443.WAV\r",
      "621: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MWRP0/SA1.WAV\r",
      "622: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MWRP0/SX273.WAV\r",
      "623: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MWRP0/SI2073.WAV\r",
      "624: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MWRP0/SI1525.WAV\r",
      "625: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MWRP0/SX183.WAV\r",
      "626: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MWRP0/SX93.WAV\r",
      "627: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MWRP0/SX3.WAV\r",
      "628: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MWRP0/SA2.WAV\r",
      "629: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MWRP0/SX363.WAV\r",
      "630: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MRMG0/SA1.WAV\r",
      "631: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MRMG0/SX270.WAV\r",
      "632: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MRMG0/SI2340.WAV\r",
      "633: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MRMG0/SX360.WAV\r",
      "634: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MRMG0/SX450.WAV\r",
      "635: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MRMG0/SX90.WAV\r",
      "636: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MRMG0/SI1080.WAV\r",
      "637: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MRMG0/SA2.WAV\r",
      "638: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MRMG0/SI1710.WAV\r",
      "639: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MRMG0/SX180.WAV\r",
      "640: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/FVKB0/SX169.WAV\r",
      "641: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/FVKB0/SA1.WAV\r",
      "642: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/FVKB0/SX439.WAV\r",
      "643: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/FVKB0/SX259.WAV\r",
      "644: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/FVKB0/SI529.WAV\r",
      "645: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/FVKB0/SA2.WAV\r",
      "646: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/FVKB0/SX79.WAV\r",
      "647: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/FVKB0/SI1159.WAV\r",
      "648: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/FVKB0/SX349.WAV\r",
      "649: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/FVKB0/SI1789.WAV\r",
      "650: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MDLC1/SA1.WAV\r",
      "651: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MDLC1/SX355.WAV\r",
      "652: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MDLC1/SX445.WAV\r",
      "653: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MDLC1/SX265.WAV\r",
      "654: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MDLC1/SX85.WAV\r",
      "655: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MDLC1/SI2144.WAV\r",
      "656: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MDLC1/SX175.WAV\r",
      "657: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MDLC1/SI2065.WAV\r",
      "658: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MDLC1/SA2.WAV\r",
      "659: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MDLC1/SI1435.WAV\r",
      "660: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/FPAC0/SI2011.WAV\r",
      "661: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/FPAC0/SA1.WAV\r",
      "662: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/FPAC0/SX121.WAV\r",
      "663: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/FPAC0/SX391.WAV\r",
      "664: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/FPAC0/SI661.WAV\r",
      "665: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/FPAC0/SX211.WAV\r",
      "666: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/FPAC0/SI1921.WAV\r",
      "667: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/FPAC0/SX301.WAV\r",
      "668: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/FPAC0/SA2.WAV\r",
      "669: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/FPAC0/SX31.WAV\r",
      "670: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MBTH0/SA1.WAV\r",
      "671: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MBTH0/SX32.WAV\r",
      "672: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MBTH0/SI505.WAV\r",
      "673: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MBTH0/SX212.WAV\r",
      "674: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MBTH0/SX392.WAV\r",
      "675: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MBTH0/SI757.WAV\r",
      "676: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MBTH0/SX302.WAV\r",
      "677: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MBTH0/SI2102.WAV\r",
      "678: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MBTH0/SX122.WAV\r",
      "679: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MBTH0/SA2.WAV\r",
      "680: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MGAR0/SX42.WAV\r",
      "681: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MGAR0/SX222.WAV\r",
      "682: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MGAR0/SA1.WAV\r",
      "683: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MGAR0/SX312.WAV\r",
      "684: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MGAR0/SI1694.WAV\r",
      "685: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MGAR0/SI1212.WAV\r",
      "686: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MGAR0/SX132.WAV\r",
      "687: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MGAR0/SA2.WAV\r",
      "688: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MGAR0/SI1842.WAV\r",
      "689: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MGAR0/SX402.WAV\r",
      "690: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/FJEN0/SX237.WAV\r",
      "691: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/FJEN0/SA1.WAV\r",
      "692: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/FJEN0/SI2307.WAV\r",
      "693: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/FJEN0/SX147.WAV\r",
      "694: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/FJEN0/SX327.WAV\r",
      "695: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/FJEN0/SX417.WAV\r",
      "696: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/FJEN0/SI1047.WAV\r",
      "697: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/FJEN0/SA2.WAV\r",
      "698: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/FJEN0/SI1677.WAV\r",
      "699: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/FJEN0/SX57.WAV\r",
      "700: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MTPR0/SA1.WAV\r",
      "701: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MTPR0/SX160.WAV\r",
      "702: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MTPR0/SX340.WAV\r",
      "703: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MTPR0/SI2230.WAV\r",
      "704: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MTPR0/SX250.WAV\r",
      "705: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MTPR0/SX430.WAV\r",
      "706: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MTPR0/SX70.WAV\r",
      "707: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MTPR0/SA2.WAV\r",
      "708: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MTPR0/SI506.WAV\r",
      "709: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MTPR0/SI1600.WAV\r",
      "710: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MJAI0/SI682.WAV\r",
      "711: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MJAI0/SA1.WAV\r",
      "712: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MJAI0/SI710.WAV\r",
      "713: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MJAI0/SX434.WAV\r",
      "714: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MJAI0/SX74.WAV\r",
      "715: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MJAI0/SX254.WAV\r",
      "716: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MJAI0/SI1604.WAV\r",
      "717: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MJAI0/SX344.WAV\r",
      "718: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MJAI0/SX164.WAV\r",
      "719: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MJAI0/SA2.WAV\r",
      "720: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MAEO0/SA1.WAV\r",
      "721: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MAEO0/SX336.WAV\r",
      "722: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MAEO0/SI1326.WAV\r",
      "723: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MAEO0/SI1956.WAV\r",
      "724: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MAEO0/SI1655.WAV\r",
      "725: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MAEO0/SX156.WAV\r",
      "726: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MAEO0/SA2.WAV\r",
      "727: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MAEO0/SX426.WAV\r",
      "728: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MAEO0/SX246.WAV\r",
      "729: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MAEO0/SX66.WAV\r",
      "730: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MJDG0/SA1.WAV\r",
      "731: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MJDG0/SX232.WAV\r",
      "732: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MJDG0/SX142.WAV\r",
      "733: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MJDG0/SX412.WAV\r",
      "734: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MJDG0/SX52.WAV\r",
      "735: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MJDG0/SI1672.WAV\r",
      "736: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MJDG0/SI1705.WAV\r",
      "737: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MJDG0/SI1042.WAV\r",
      "738: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MJDG0/SA2.WAV\r",
      "739: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MJDG0/SX322.WAV\r",
      "740: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MTER0/SX167.WAV\r",
      "741: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MTER0/SX17.WAV\r",
      "742: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MTER0/SX437.WAV\r",
      "743: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MTER0/SA1.WAV\r",
      "744: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MTER0/SX77.WAV\r",
      "745: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MTER0/SI1787.WAV\r",
      "746: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MTER0/SX257.WAV\r",
      "747: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MTER0/SI527.WAV\r",
      "748: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MTER0/SI1157.WAV\r",
      "749: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MTER0/SA2.WAV\r",
      "750: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MHXL0/SX62.WAV\r",
      "751: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MHXL0/SI612.WAV\r",
      "752: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MHXL0/SA1.WAV\r",
      "753: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MHXL0/SI1772.WAV\r",
      "754: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MHXL0/SI512.WAV\r",
      "755: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MHXL0/SX152.WAV\r",
      "756: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MHXL0/SX332.WAV\r",
      "757: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MHXL0/SX242.WAV\r",
      "758: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MHXL0/SA2.WAV\r",
      "759: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MHXL0/SX422.WAV\r",
      "760: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MGAK0/SI1666.WAV\r",
      "761: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MGAK0/SA1.WAV\r",
      "762: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MGAK0/SX136.WAV\r",
      "763: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MGAK0/SX406.WAV\r",
      "764: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MGAK0/SX316.WAV\r",
      "765: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MGAK0/SX226.WAV\r",
      "766: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MGAK0/SX46.WAV\r",
      "767: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MGAK0/SI1036.WAV\r",
      "768: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MGAK0/SA2.WAV\r",
      "769: /home/sri/Desktop/timit/TIMIT/TRAIN/DR7/MGAK0/SI2296.WAV\r",
      "770: /home/sri/Desktop/timit/TIMIT/TRAIN/DR1/MPGR0/SA1.WAV\r",
      "771: /home/sri/Desktop/timit/TIMIT/TRAIN/DR1/MPGR0/SX240.WAV\r",
      "772: /home/sri/Desktop/timit/TIMIT/TRAIN/DR1/MPGR0/SI780.WAV\r",
      "773: /home/sri/Desktop/timit/TIMIT/TRAIN/DR1/MPGR0/SI1410.WAV\r",
      "774: /home/sri/Desktop/timit/TIMIT/TRAIN/DR1/MPGR0/SX60.WAV\r",
      "775: /home/sri/Desktop/timit/TIMIT/TRAIN/DR1/MPGR0/SI2040.WAV\r",
      "776: /home/sri/Desktop/timit/TIMIT/TRAIN/DR1/MPGR0/SA2.WAV\r",
      "777: /home/sri/Desktop/timit/TIMIT/TRAIN/DR1/MPGR0/SX420.WAV\r",
      "778: /home/sri/Desktop/timit/TIMIT/TRAIN/DR1/MPGR0/SX330.WAV\r",
      "779: /home/sri/Desktop/timit/TIMIT/TRAIN/DR1/MPGR0/SX150.WAV\r",
      "780: /home/sri/Desktop/timit/TIMIT/TRAIN/DR1/FJSP0/SI1763.WAV\r",
      "781: /home/sri/Desktop/timit/TIMIT/TRAIN/DR1/FJSP0/SA1.WAV\r",
      "782: /home/sri/Desktop/timit/TIMIT/TRAIN/DR1/FJSP0/SX354.WAV\r",
      "783: /home/sri/Desktop/timit/TIMIT/TRAIN/DR1/FJSP0/SX174.WAV\r",
      "784: /home/sri/Desktop/timit/TIMIT/TRAIN/DR1/FJSP0/SI1434.WAV\r",
      "785: /home/sri/Desktop/timit/TIMIT/TRAIN/DR1/FJSP0/SX264.WAV\r",
      "786: /home/sri/Desktop/timit/TIMIT/TRAIN/DR1/FJSP0/SX444.WAV\r",
      "787: /home/sri/Desktop/timit/TIMIT/TRAIN/DR1/FJSP0/SX84.WAV\r",
      "788: /home/sri/Desktop/timit/TIMIT/TRAIN/DR1/FJSP0/SA2.WAV\r",
      "789: /home/sri/Desktop/timit/TIMIT/TRAIN/DR1/FJSP0/SI804.WAV\r",
      "790: /home/sri/Desktop/timit/TIMIT/TRAIN/DR1/MRWS0/SA1.WAV\r",
      "791: /home/sri/Desktop/timit/TIMIT/TRAIN/DR1/MRWS0/SI1102.WAV\r",
      "792: /home/sri/Desktop/timit/TIMIT/TRAIN/DR1/MRWS0/SI472.WAV\r",
      "793: /home/sri/Desktop/timit/TIMIT/TRAIN/DR1/MRWS0/SX112.WAV\r",
      "794: /home/sri/Desktop/timit/TIMIT/TRAIN/DR1/MRWS0/SX202.WAV\r",
      "795: /home/sri/Desktop/timit/TIMIT/TRAIN/DR1/MRWS0/SX22.WAV\r",
      "796: /home/sri/Desktop/timit/TIMIT/TRAIN/DR1/MRWS0/SX292.WAV\r",
      "797: /home/sri/Desktop/timit/TIMIT/TRAIN/DR1/MRWS0/SI1732.WAV\r",
      "798: /home/sri/Desktop/timit/TIMIT/TRAIN/DR1/MRWS0/SX382.WAV\r",
      "799: /home/sri/Desktop/timit/TIMIT/TRAIN/DR1/MRWS0/SA2.WAV\r",
      "800: /home/sri/Desktop/timit/TIMIT/TRAIN/DR1/MJWT0/SA1.WAV\r",
      "801: /home/sri/Desktop/timit/TIMIT/TRAIN/DR1/MJWT0/SI1291.WAV\r",
      "802: /home/sri/Desktop/timit/TIMIT/TRAIN/DR1/MJWT0/SX121.WAV\r",
      "803: /home/sri/Desktop/timit/TIMIT/TRAIN/DR1/MJWT0/SI1381.WAV\r",
      "804: /home/sri/Desktop/timit/TIMIT/TRAIN/DR1/MJWT0/SX391.WAV\r",
      "805: /home/sri/Desktop/timit/TIMIT/TRAIN/DR1/MJWT0/SX211.WAV\r",
      "806: /home/sri/Desktop/timit/TIMIT/TRAIN/DR1/MJWT0/SX301.WAV\r",
      "807: /home/sri/Desktop/timit/TIMIT/TRAIN/DR1/MJWT0/SA2.WAV\r",
      "808: /home/sri/Desktop/timit/TIMIT/TRAIN/DR1/MJWT0/SI751.WAV\r",
      "809: /home/sri/Desktop/timit/TIMIT/TRAIN/DR1/MJWT0/SX31.WAV\r",
      "810: /home/sri/Desktop/timit/TIMIT/TRAIN/DR1/MKLW0/SX311.WAV\r",
      "811: /home/sri/Desktop/timit/TIMIT/TRAIN/DR1/MKLW0/SX221.WAV\r",
      "812: /home/sri/Desktop/timit/TIMIT/TRAIN/DR1/MKLW0/SI1844.WAV\r",
      "813: /home/sri/Desktop/timit/TIMIT/TRAIN/DR1/MKLW0/SA1.WAV\r",
      "814: /home/sri/Desktop/timit/TIMIT/TRAIN/DR1/MKLW0/SI1571.WAV\r",
      "815: /home/sri/Desktop/timit/TIMIT/TRAIN/DR1/MKLW0/SX41.WAV\r",
      "816: /home/sri/Desktop/timit/TIMIT/TRAIN/DR1/MKLW0/SI2201.WAV\r",
      "817: /home/sri/Desktop/timit/TIMIT/TRAIN/DR1/MKLW0/SX401.WAV\r",
      "818: /home/sri/Desktop/timit/TIMIT/TRAIN/DR1/MKLW0/SA2.WAV\r",
      "819: /home/sri/Desktop/timit/TIMIT/TRAIN/DR1/MKLW0/SX131.WAV\r",
      "820: /home/sri/Desktop/timit/TIMIT/TRAIN/DR1/FETB0/SX248.WAV\r",
      "821: /home/sri/Desktop/timit/TIMIT/TRAIN/DR1/FETB0/SX428.WAV\r",
      "822: /home/sri/Desktop/timit/TIMIT/TRAIN/DR1/FETB0/SA1.WAV\r",
      "823: /home/sri/Desktop/timit/TIMIT/TRAIN/DR1/FETB0/SX68.WAV\r",
      "824: /home/sri/Desktop/timit/TIMIT/TRAIN/DR1/FETB0/SI1778.WAV\r",
      "825: /home/sri/Desktop/timit/TIMIT/TRAIN/DR1/FETB0/SI518.WAV\r",
      "826: /home/sri/Desktop/timit/TIMIT/TRAIN/DR1/FETB0/SX338.WAV\r",
      "827: /home/sri/Desktop/timit/TIMIT/TRAIN/DR1/FETB0/SI1148.WAV\r",
      "828: /home/sri/Desktop/timit/TIMIT/TRAIN/DR1/FETB0/SX158.WAV\r",
      "829: /home/sri/Desktop/timit/TIMIT/TRAIN/DR1/FETB0/SA2.WAV\r",
      "830: /home/sri/Desktop/timit/TIMIT/TRAIN/DR1/FDML0/SI1779.WAV\r",
      "831: /home/sri/Desktop/timit/TIMIT/TRAIN/DR1/FDML0/SA1.WAV\r",
      "832: /home/sri/Desktop/timit/TIMIT/TRAIN/DR1/FDML0/SI1149.WAV\r",
      "833: /home/sri/Desktop/timit/TIMIT/TRAIN/DR1/FDML0/SX339.WAV\r",
      "834: /home/sri/Desktop/timit/TIMIT/TRAIN/DR1/FDML0/SX429.WAV\r",
      "835: /home/sri/Desktop/timit/TIMIT/TRAIN/DR1/FDML0/SX69.WAV\r",
      "836: /home/sri/Desktop/timit/TIMIT/TRAIN/DR1/FDML0/SX159.WAV\r",
      "837: /home/sri/Desktop/timit/TIMIT/TRAIN/DR1/FDML0/SI2075.WAV\r",
      "838: /home/sri/Desktop/timit/TIMIT/TRAIN/DR1/FDML0/SA2.WAV\r",
      "839: /home/sri/Desktop/timit/TIMIT/TRAIN/DR1/FDML0/SX249.WAV\r",
      "840: /home/sri/Desktop/timit/TIMIT/TRAIN/DR1/FTBR0/SX111.WAV\r",
      "841: /home/sri/Desktop/timit/TIMIT/TRAIN/DR1/FTBR0/SI921.WAV\r",
      "842: /home/sri/Desktop/timit/TIMIT/TRAIN/DR1/FTBR0/SA1.WAV\r",
      "843: /home/sri/Desktop/timit/TIMIT/TRAIN/DR1/FTBR0/SX21.WAV\r",
      "844: /home/sri/Desktop/timit/TIMIT/TRAIN/DR1/FTBR0/SX201.WAV\r",
      "845: /home/sri/Desktop/timit/TIMIT/TRAIN/DR1/FTBR0/SI1402.WAV\r",
      "846: /home/sri/Desktop/timit/TIMIT/TRAIN/DR1/FTBR0/SA2.WAV\r",
      "847: /home/sri/Desktop/timit/TIMIT/TRAIN/DR1/FTBR0/SI2181.WAV\r",
      "848: /home/sri/Desktop/timit/TIMIT/TRAIN/DR1/FTBR0/SX291.WAV\r",
      "849: /home/sri/Desktop/timit/TIMIT/TRAIN/DR1/FTBR0/SX381.WAV\r",
      "850: /home/sri/Desktop/timit/TIMIT/TRAIN/DR1/FECD0/SX248.WAV\r",
      "851: /home/sri/Desktop/timit/TIMIT/TRAIN/DR1/FECD0/SX428.WAV\r",
      "852: /home/sri/Desktop/timit/TIMIT/TRAIN/DR1/FECD0/SA1.WAV\r",
      "853: /home/sri/Desktop/timit/TIMIT/TRAIN/DR1/FECD0/SX68.WAV\r",
      "854: /home/sri/Desktop/timit/TIMIT/TRAIN/DR1/FECD0/SX338.WAV\r",
      "855: /home/sri/Desktop/timit/TIMIT/TRAIN/DR1/FECD0/SX158.WAV\r",
      "856: /home/sri/Desktop/timit/TIMIT/TRAIN/DR1/FECD0/SI1418.WAV\r",
      "857: /home/sri/Desktop/timit/TIMIT/TRAIN/DR1/FECD0/SI788.WAV\r",
      "858: /home/sri/Desktop/timit/TIMIT/TRAIN/DR1/FECD0/SA2.WAV\r",
      "859: /home/sri/Desktop/timit/TIMIT/TRAIN/DR1/FECD0/SI2048.WAV\r",
      "860: /home/sri/Desktop/timit/TIMIT/TRAIN/DR1/FSMA0/SX271.WAV\r",
      "861: /home/sri/Desktop/timit/TIMIT/TRAIN/DR1/FSMA0/SA1.WAV\r",
      "862: /home/sri/Desktop/timit/TIMIT/TRAIN/DR1/FSMA0/SX91.WAV\r",
      "863: /home/sri/Desktop/timit/TIMIT/TRAIN/DR1/FSMA0/SX451.WAV\r",
      "864: /home/sri/Desktop/timit/TIMIT/TRAIN/DR1/FSMA0/SI991.WAV\r",
      "865: /home/sri/Desktop/timit/TIMIT/TRAIN/DR1/FSMA0/SX181.WAV\r",
      "866: /home/sri/Desktop/timit/TIMIT/TRAIN/DR1/FSMA0/SA2.WAV\r",
      "867: /home/sri/Desktop/timit/TIMIT/TRAIN/DR1/FSMA0/SI2251.WAV\r",
      "868: /home/sri/Desktop/timit/TIMIT/TRAIN/DR1/FSMA0/SI1621.WAV\r",
      "869: /home/sri/Desktop/timit/TIMIT/TRAIN/DR1/FSMA0/SX361.WAV\r",
      "870: /home/sri/Desktop/timit/TIMIT/TRAIN/DR1/MPSW0/SX167.WAV\r",
      "871: /home/sri/Desktop/timit/TIMIT/TRAIN/DR1/MPSW0/SI1697.WAV\r",
      "872: /home/sri/Desktop/timit/TIMIT/TRAIN/DR1/MPSW0/SX437.WAV\r",
      "873: /home/sri/Desktop/timit/TIMIT/TRAIN/DR1/MPSW0/SA1.WAV\r",
      "874: /home/sri/Desktop/timit/TIMIT/TRAIN/DR1/MPSW0/SX77.WAV\r",
      "875: /home/sri/Desktop/timit/TIMIT/TRAIN/DR1/MPSW0/SX257.WAV\r",
      "876: /home/sri/Desktop/timit/TIMIT/TRAIN/DR1/MPSW0/SI2327.WAV\r",
      "877: /home/sri/Desktop/timit/TIMIT/TRAIN/DR1/MPSW0/SX24.WAV\r",
      "878: /home/sri/Desktop/timit/TIMIT/TRAIN/DR1/MPSW0/SA2.WAV\r",
      "879: /home/sri/Desktop/timit/TIMIT/TRAIN/DR1/MPSW0/SI1067.WAV\r",
      "880: /home/sri/Desktop/timit/TIMIT/TRAIN/DR1/MMRP0/SX234.WAV\r",
      "881: /home/sri/Desktop/timit/TIMIT/TRAIN/DR1/MMRP0/SA1.WAV\r",
      "882: /home/sri/Desktop/timit/TIMIT/TRAIN/DR1/MMRP0/SX324.WAV\r",
      "883: /home/sri/Desktop/timit/TIMIT/TRAIN/DR1/MMRP0/SI774.WAV\r",
      "884: /home/sri/Desktop/timit/TIMIT/TRAIN/DR1/MMRP0/SI2034.WAV\r",
      "885: /home/sri/Desktop/timit/TIMIT/TRAIN/DR1/MMRP0/SI717.WAV\r",
      "886: /home/sri/Desktop/timit/TIMIT/TRAIN/DR1/MMRP0/SX54.WAV\r",
      "887: /home/sri/Desktop/timit/TIMIT/TRAIN/DR1/MMRP0/SX414.WAV\r",
      "888: /home/sri/Desktop/timit/TIMIT/TRAIN/DR1/MMRP0/SX144.WAV\r",
      "889: /home/sri/Desktop/timit/TIMIT/TRAIN/DR1/MMRP0/SA2.WAV\r",
      "890: /home/sri/Desktop/timit/TIMIT/TRAIN/DR1/MTJS0/SA1.WAV\r",
      "891: /home/sri/Desktop/timit/TIMIT/TRAIN/DR1/MTJS0/SI1192.WAV\r",
      "892: /home/sri/Desktop/timit/TIMIT/TRAIN/DR1/MTJS0/SX112.WAV\r",
      "893: /home/sri/Desktop/timit/TIMIT/TRAIN/DR1/MTJS0/SX202.WAV\r",
      "894: /home/sri/Desktop/timit/TIMIT/TRAIN/DR1/MTJS0/SX22.WAV\r",
      "895: /home/sri/Desktop/timit/TIMIT/TRAIN/DR1/MTJS0/SX292.WAV\r",
      "896: /home/sri/Desktop/timit/TIMIT/TRAIN/DR1/MTJS0/SI562.WAV\r",
      "897: /home/sri/Desktop/timit/TIMIT/TRAIN/DR1/MTJS0/SX382.WAV\r",
      "898: /home/sri/Desktop/timit/TIMIT/TRAIN/DR1/MTJS0/SI1822.WAV\r",
      "899: /home/sri/Desktop/timit/TIMIT/TRAIN/DR1/MTJS0/SA2.WAV\r",
      "900: /home/sri/Desktop/timit/TIMIT/TRAIN/DR1/MPGH0/SI1554.WAV\r",
      "901: /home/sri/Desktop/timit/TIMIT/TRAIN/DR1/MPGH0/SA1.WAV\r",
      "902: /home/sri/Desktop/timit/TIMIT/TRAIN/DR1/MPGH0/SX114.WAV\r",
      "903: /home/sri/Desktop/timit/TIMIT/TRAIN/DR1/MPGH0/SX384.WAV\r",
      "904: /home/sri/Desktop/timit/TIMIT/TRAIN/DR1/MPGH0/SX294.WAV\r",
      "905: /home/sri/Desktop/timit/TIMIT/TRAIN/DR1/MPGH0/SI675.WAV\r",
      "906: /home/sri/Desktop/timit/TIMIT/TRAIN/DR1/MPGH0/SI924.WAV\r",
      "907: /home/sri/Desktop/timit/TIMIT/TRAIN/DR1/MPGH0/SX24.WAV\r",
      "908: /home/sri/Desktop/timit/TIMIT/TRAIN/DR1/MPGH0/SA2.WAV\r",
      "909: /home/sri/Desktop/timit/TIMIT/TRAIN/DR1/MPGH0/SX204.WAV\r",
      "910: /home/sri/Desktop/timit/TIMIT/TRAIN/DR1/MEDR0/SA1.WAV\r",
      "911: /home/sri/Desktop/timit/TIMIT/TRAIN/DR1/MEDR0/SX114.WAV\r",
      "912: /home/sri/Desktop/timit/TIMIT/TRAIN/DR1/MEDR0/SX384.WAV\r",
      "913: /home/sri/Desktop/timit/TIMIT/TRAIN/DR1/MEDR0/SX294.WAV\r",
      "914: /home/sri/Desktop/timit/TIMIT/TRAIN/DR1/MEDR0/SI744.WAV\r",
      "915: /home/sri/Desktop/timit/TIMIT/TRAIN/DR1/MEDR0/SX24.WAV\r",
      "916: /home/sri/Desktop/timit/TIMIT/TRAIN/DR1/MEDR0/SI2004.WAV\r",
      "917: /home/sri/Desktop/timit/TIMIT/TRAIN/DR1/MEDR0/SI1374.WAV\r",
      "918: /home/sri/Desktop/timit/TIMIT/TRAIN/DR1/MEDR0/SA2.WAV\r",
      "919: /home/sri/Desktop/timit/TIMIT/TRAIN/DR1/MEDR0/SX204.WAV\r",
      "920: /home/sri/Desktop/timit/TIMIT/TRAIN/DR1/MKLS0/SI2067.WAV\r",
      "921: /home/sri/Desktop/timit/TIMIT/TRAIN/DR1/MKLS0/SA1.WAV\r",
      "922: /home/sri/Desktop/timit/TIMIT/TRAIN/DR1/MKLS0/SX87.WAV\r",
      "923: /home/sri/Desktop/timit/TIMIT/TRAIN/DR1/MKLS0/SX357.WAV\r",
      "924: /home/sri/Desktop/timit/TIMIT/TRAIN/DR1/MKLS0/SX447.WAV\r",
      "925: /home/sri/Desktop/timit/TIMIT/TRAIN/DR1/MKLS0/SX177.WAV\r",
      "926: /home/sri/Desktop/timit/TIMIT/TRAIN/DR1/MKLS0/SI1533.WAV\r",
      "927: /home/sri/Desktop/timit/TIMIT/TRAIN/DR1/MKLS0/SX267.WAV\r",
      "928: /home/sri/Desktop/timit/TIMIT/TRAIN/DR1/MKLS0/SA2.WAV\r",
      "929: /home/sri/Desktop/timit/TIMIT/TRAIN/DR1/MKLS0/SI1437.WAV\r",
      "930: /home/sri/Desktop/timit/TIMIT/TRAIN/DR1/MDAC0/SX271.WAV\r",
      "931: /home/sri/Desktop/timit/TIMIT/TRAIN/DR1/MDAC0/SI1837.WAV\r",
      "932: /home/sri/Desktop/timit/TIMIT/TRAIN/DR1/MDAC0/SA1.WAV\r",
      "933: /home/sri/Desktop/timit/TIMIT/TRAIN/DR1/MDAC0/SI1261.WAV\r",
      "934: /home/sri/Desktop/timit/TIMIT/TRAIN/DR1/MDAC0/SX91.WAV\r",
      "935: /home/sri/Desktop/timit/TIMIT/TRAIN/DR1/MDAC0/SI631.WAV\r",
      "936: /home/sri/Desktop/timit/TIMIT/TRAIN/DR1/MDAC0/SX451.WAV\r",
      "937: /home/sri/Desktop/timit/TIMIT/TRAIN/DR1/MDAC0/SX181.WAV\r",
      "938: /home/sri/Desktop/timit/TIMIT/TRAIN/DR1/MDAC0/SA2.WAV\r",
      "939: /home/sri/Desktop/timit/TIMIT/TRAIN/DR1/MDAC0/SX361.WAV\r",
      "940: /home/sri/Desktop/timit/TIMIT/TRAIN/DR1/FDAW0/SI2036.WAV\r",
      "941: /home/sri/Desktop/timit/TIMIT/TRAIN/DR1/FDAW0/SX56.WAV\r",
      "942: /home/sri/Desktop/timit/TIMIT/TRAIN/DR1/FDAW0/SA1.WAV\r",
      "943: /home/sri/Desktop/timit/TIMIT/TRAIN/DR1/FDAW0/SX146.WAV\r",
      "944: /home/sri/Desktop/timit/TIMIT/TRAIN/DR1/FDAW0/SX326.WAV\r",
      "945: /home/sri/Desktop/timit/TIMIT/TRAIN/DR1/FDAW0/SI1271.WAV\r",
      "946: /home/sri/Desktop/timit/TIMIT/TRAIN/DR1/FDAW0/SX416.WAV\r",
      "947: /home/sri/Desktop/timit/TIMIT/TRAIN/DR1/FDAW0/SX236.WAV\r",
      "948: /home/sri/Desktop/timit/TIMIT/TRAIN/DR1/FDAW0/SI1406.WAV\r",
      "949: /home/sri/Desktop/timit/TIMIT/TRAIN/DR1/FDAW0/SA2.WAV\r",
      "950: /home/sri/Desktop/timit/TIMIT/TRAIN/DR1/MRCG0/SI798.WAV\r",
      "951: /home/sri/Desktop/timit/TIMIT/TRAIN/DR1/MRCG0/SA1.WAV\r",
      "952: /home/sri/Desktop/timit/TIMIT/TRAIN/DR1/MRCG0/SX78.WAV\r",
      "953: /home/sri/Desktop/timit/TIMIT/TRAIN/DR1/MRCG0/SI2058.WAV\r",
      "954: /home/sri/Desktop/timit/TIMIT/TRAIN/DR1/MRCG0/SX258.WAV\r",
      "955: /home/sri/Desktop/timit/TIMIT/TRAIN/DR1/MRCG0/SX438.WAV\r",
      "956: /home/sri/Desktop/timit/TIMIT/TRAIN/DR1/MRCG0/SX168.WAV\r",
      "957: /home/sri/Desktop/timit/TIMIT/TRAIN/DR1/MRCG0/SA2.WAV\r",
      "958: /home/sri/Desktop/timit/TIMIT/TRAIN/DR1/MRCG0/SI1428.WAV\r",
      "959: /home/sri/Desktop/timit/TIMIT/TRAIN/DR1/MRCG0/SX348.WAV\r",
      "960: /home/sri/Desktop/timit/TIMIT/TRAIN/DR1/MRAI0/SI792.WAV\r",
      "961: /home/sri/Desktop/timit/TIMIT/TRAIN/DR1/MRAI0/SA1.WAV\r",
      "962: /home/sri/Desktop/timit/TIMIT/TRAIN/DR1/MRAI0/SX252.WAV\r",
      "963: /home/sri/Desktop/timit/TIMIT/TRAIN/DR1/MRAI0/SX342.WAV\r",
      "964: /home/sri/Desktop/timit/TIMIT/TRAIN/DR1/MRAI0/SI2052.WAV\r",
      "965: /home/sri/Desktop/timit/TIMIT/TRAIN/DR1/MRAI0/SX162.WAV\r",
      "966: /home/sri/Desktop/timit/TIMIT/TRAIN/DR1/MRAI0/SX72.WAV\r",
      "967: /home/sri/Desktop/timit/TIMIT/TRAIN/DR1/MRAI0/SX432.WAV\r",
      "968: /home/sri/Desktop/timit/TIMIT/TRAIN/DR1/MRAI0/SA2.WAV\r",
      "969: /home/sri/Desktop/timit/TIMIT/TRAIN/DR1/MRAI0/SI1954.WAV\r",
      "970: /home/sri/Desktop/timit/TIMIT/TRAIN/DR1/FCJF0/SX127.WAV\r",
      "971: /home/sri/Desktop/timit/TIMIT/TRAIN/DR1/FCJF0/SA1.WAV\r",
      "972: /home/sri/Desktop/timit/TIMIT/TRAIN/DR1/FCJF0/SX217.WAV\r",
      "973: /home/sri/Desktop/timit/TIMIT/TRAIN/DR1/FCJF0/SI648.WAV\r",
      "974: /home/sri/Desktop/timit/TIMIT/TRAIN/DR1/FCJF0/SX307.WAV\r",
      "975: /home/sri/Desktop/timit/TIMIT/TRAIN/DR1/FCJF0/SI1657.WAV\r",
      "976: /home/sri/Desktop/timit/TIMIT/TRAIN/DR1/FCJF0/SX397.WAV\r",
      "977: /home/sri/Desktop/timit/TIMIT/TRAIN/DR1/FCJF0/SA2.WAV\r",
      "978: /home/sri/Desktop/timit/TIMIT/TRAIN/DR1/FCJF0/SX37.WAV\r",
      "979: /home/sri/Desktop/timit/TIMIT/TRAIN/DR1/FCJF0/SI1027.WAV\r",
      "980: /home/sri/Desktop/timit/TIMIT/TRAIN/DR1/MTPF0/SI1865.WAV\r",
      "981: /home/sri/Desktop/timit/TIMIT/TRAIN/DR1/MTPF0/SA1.WAV\r",
      "982: /home/sri/Desktop/timit/TIMIT/TRAIN/DR1/MTPF0/SX65.WAV\r",
      "983: /home/sri/Desktop/timit/TIMIT/TRAIN/DR1/MTPF0/SX425.WAV\r",
      "984: /home/sri/Desktop/timit/TIMIT/TRAIN/DR1/MTPF0/SI605.WAV\r",
      "985: /home/sri/Desktop/timit/TIMIT/TRAIN/DR1/MTPF0/SI1235.WAV\r",
      "986: /home/sri/Desktop/timit/TIMIT/TRAIN/DR1/MTPF0/SX335.WAV\r",
      "987: /home/sri/Desktop/timit/TIMIT/TRAIN/DR1/MTPF0/SA2.WAV\r",
      "988: /home/sri/Desktop/timit/TIMIT/TRAIN/DR1/MTPF0/SX245.WAV\r",
      "989: /home/sri/Desktop/timit/TIMIT/TRAIN/DR1/MTPF0/SX155.WAV\r",
      "990: /home/sri/Desktop/timit/TIMIT/TRAIN/DR1/FVMH0/SA1.WAV\r",
      "991: /home/sri/Desktop/timit/TIMIT/TRAIN/DR1/FVMH0/SI2096.WAV\r",
      "992: /home/sri/Desktop/timit/TIMIT/TRAIN/DR1/FVMH0/SX116.WAV\r",
      "993: /home/sri/Desktop/timit/TIMIT/TRAIN/DR1/FVMH0/SI836.WAV\r",
      "994: /home/sri/Desktop/timit/TIMIT/TRAIN/DR1/FVMH0/SX26.WAV\r",
      "995: /home/sri/Desktop/timit/TIMIT/TRAIN/DR1/FVMH0/SX386.WAV\r",
      "996: /home/sri/Desktop/timit/TIMIT/TRAIN/DR1/FVMH0/SI1466.WAV\r",
      "997: /home/sri/Desktop/timit/TIMIT/TRAIN/DR1/FVMH0/SA2.WAV\r",
      "998: /home/sri/Desktop/timit/TIMIT/TRAIN/DR1/FVMH0/SX206.WAV\r",
      "999: /home/sri/Desktop/timit/TIMIT/TRAIN/DR1/FVMH0/SX296.WAV\r"
     ]
    }
   ],
   "source": [
    "# read in 100 WAVs from TIMIT training set\n",
    "rawWaveforms = load_TIMIT_train(TIMIT_DIR, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''\n",
    "# waveform preprocessing\n",
    "def preprocessWaveform(waveform):   \n",
    "    return waveform, ()\n",
    "   \n",
    "def unpreprocessWaveform(waveform, params):\n",
    "    return waveform\n",
    "\n",
    "\n",
    "\n",
    "# window preprocessing\n",
    "def preprocessWindows(windows):\n",
    "    # scale window between -1 and 1\n",
    "    processed = np.copy(windows)\n",
    "   \n",
    "    mn = np.min(processed, axis = 1)\n",
    "    mx = np.max(processed, axis = 1)\n",
    "\n",
    "    maxabs = np.maximum(np.abs(mn), np.abs(mx))\n",
    "\n",
    "    for i in xrange(0, processed.shape[0]):\n",
    "        processed[i] /= maxabs[i]\n",
    "    #processed *= 0.98\n",
    "   \n",
    "    #processed = (processed + 1.0) / 2.0\n",
    "   \n",
    "    return processed, (maxabs,)\n",
    "\n",
    "def unpreprocessWindows(windows, params):\n",
    "    # scale window from [-1, 1] to [-32768, 32768]\n",
    "    scl = params[0]\n",
    "   \n",
    "    unprocessed = np.copy(windows)\n",
    "    #unprocessed /= 0.98\n",
    "   \n",
    "    #nprocessed = (unprocessed * 2.0) - 1.0\n",
    "   \n",
    "    for i in xrange(0, unprocessed.shape[0]):\n",
    "        unprocessed[i] *= scl[i]\n",
    "\n",
    "    return unprocessed\n",
    "#'''\n",
    "\n",
    "#'''\n",
    "# waveform preprocessing\n",
    "def preprocessWaveform(waveform):\n",
    "    # scale window between -1 and 1\n",
    "    mn = np.min(waveform)\n",
    "    mx = np.max(waveform)\n",
    "    maxabs = np.maximum(np.abs(mn), np.abs(mx))\n",
    "        \n",
    "    return np.copy(waveform) / maxabs, (maxabs,)\n",
    "   \n",
    "def unpreprocessWaveform(waveform, params):\n",
    "    return np.copy(waveform) * params[0]\n",
    "\n",
    "\n",
    "\n",
    "# window preprocessing\n",
    "def preprocessWindows(windows):\n",
    "    return windows, ()\n",
    "\n",
    "def unpreprocessWindows(windows, params):\n",
    "    return windows\n",
    "#'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# waveform preprocessing\n",
    "processedWaveforms = np.copy(rawWaveforms)\n",
    "\n",
    "# we maximize the volume of every waveform\n",
    "for i in xrange(0, len(processedWaveforms)):\n",
    "    processedWaveforms[i], _ = preprocessWaveform(rawWaveforms[i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw windows shape:  (101135, 512)\n",
      "Max:  1.0\n",
      "Min:  -1.0\n"
     ]
    }
   ],
   "source": [
    "# extract windows\n",
    "rawWindows = extractWindowsMultiple(processedWaveforms, STEP_SIZE, OVERLAP_SIZE,\n",
    "                                    collapse = True)\n",
    "\n",
    "# randomly shuffle data\n",
    "if (RANDOM_SHUFFLE):\n",
    "    rawWindows = np.random.permutation(rawWindows)\n",
    "\n",
    "print \"Raw windows shape: \", rawWindows.shape\n",
    "print \"Max: \", np.amax(rawWindows)\n",
    "print \"Min: \", np.amin(rawWindows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aug windows shape:  (101135, 512)\n"
     ]
    }
   ],
   "source": [
    "# data augmentation goes here, at some point\n",
    "augWindows = np.copy(rawWindows)\n",
    "\n",
    "print \"Aug windows shape: \", augWindows.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "processedWindows, pwParams = preprocessWindows(augWindows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# reshape into vector form\n",
    "processedWindows = np.reshape(processedWindows, (processedWindows.shape[0], WINDOW_SIZE, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(101135, 512, 1)\n",
      "9.77603e-07\n",
      "0.099766\n",
      "-1.0\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "print processedWindows.shape\n",
    "\n",
    "print np.mean(processedWindows, axis=None)\n",
    "print np.std(processedWindows, axis=None)\n",
    "print np.min(processedWindows, axis = None)\n",
    "print np.max(processedWindows, axis = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class PhaseShiftDown1D(Layer):\n",
    "    \"\"\" PhaseShiftDown1D\n",
    "    Takes vector of size: B x nS x F\n",
    "    And returns vector: B x S x nF\n",
    "    \"\"\"\n",
    "    def __init__(self, n, **kwargs):\n",
    "        super(PhaseShiftDown1D, self).__init__(**kwargs)\n",
    "        self.n = n\n",
    "    \n",
    "    def build(self, input_shape):\n",
    "        # no trainable parameters\n",
    "        self.trainable_weights = []\n",
    "    \n",
    "    def call(self, x, mask=None):\n",
    "        r = T.reshape(x, (x.shape[0], x.shape[1] / self.n, self.n, x.shape[2]))\n",
    "        r = T.transpose(r, (0, 1, 3, 2))\n",
    "        r = T.reshape(r, (x.shape[0], x.shape[1] / self.n, x.shape[2] * self.n))\n",
    "        return r\n",
    "\n",
    "    def get_output_shape_for(self, input_shape):\n",
    "        return (input_shape[0], input_shape[1] / self.n, input_shape[2] * self.n)\n",
    "    \n",
    "    def get_config(self):\n",
    "        config = {'n' : self.n}\n",
    "        base_config = super(PhaseShiftDown1D, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class PhaseShiftUp1D(Layer):\n",
    "    \"\"\" PhaseShiftUp1D\n",
    "    Takes vector of size: B x S x nF\n",
    "    And returns vector: B x nS x F\n",
    "    \"\"\"\n",
    "    def __init__(self, n, **kwargs):\n",
    "        super(PhaseShiftUp1D, self).__init__(**kwargs)\n",
    "        self.n = n\n",
    "    \n",
    "    def build(self, input_shape):\n",
    "        # no trainable parameters\n",
    "        self.trainable_weights = []\n",
    "    \n",
    "    def call(self, x, mask=None):\n",
    "        r = T.reshape(x, (x.shape[0], x.shape[1], x.shape[2] / self.n, self.n))\n",
    "        r = T.transpose(r, (0, 1, 3, 2))\n",
    "        r = T.reshape(r, (x.shape[0], x.shape[1] * self.n, x.shape[2] / self.n))\n",
    "        return r\n",
    "\n",
    "    def get_output_shape_for(self, input_shape):\n",
    "        return (input_shape[0], input_shape[1] * self.n, input_shape[2] / self.n)\n",
    "    \n",
    "    def get_config(self):\n",
    "        config = {'n' : self.n}\n",
    "        base_config = super(PhaseShiftUp1D, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# quantizes [-1.0, 1.0] into a certain number of uniform bins\n",
    "class UniformQuantizer(Layer):\n",
    "    def __init__(self, nbins, noise_in_train = True, **kwargs):\n",
    "        self.supports_masking = True\n",
    "        self.nbins = nbins\n",
    "        self.noise_in_train = noise_in_train\n",
    "        self.uses_learning_phase = True\n",
    "        super(UniformQuantizer, self).__init__(**kwargs)\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        # during training, add uniform noise the width of one bin\n",
    "        noise_x = x + K.random_uniform(shape = K.shape(x),\n",
    "                                       low = -(0.5 / (self.nbins - 1)),\n",
    "                                       high = 0.5 / (self.nbins - 1))\n",
    "        \n",
    "        # during testing, actually quantize the thing\n",
    "        qnt_x = (x + 1.0) / 2.0\n",
    "        qnt_x = K.round(qnt_x * float(self.nbins - 1)) / float(self.nbins - 1)\n",
    "        qnt_x = (qnt_x * 2.0) - 1.0\n",
    "        \n",
    "        if (self.noise_in_train):\n",
    "            return K.in_train_phase(noise_x, qnt_x)\n",
    "        else:\n",
    "            return K.in_train_phase(x, qnt_x)\n",
    "\n",
    "    def get_config(self):\n",
    "        config = {'nbins': self.nbins}\n",
    "        base_config = super(UniformQuantizer, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class SelectMax(T.Op):\n",
    "    # properties attribute\n",
    "    __props__ = ()\n",
    "    \n",
    "    def __init__(self, n):\n",
    "        super(SelectMax, self).__init__()\n",
    "        self.n = n\n",
    "        \n",
    "    def make_node(self, x):\n",
    "        assert hasattr(self, '_props'), \"Your version of theano is too old to support __props__.\"\n",
    "        x = T.as_tensor_variable(x)\n",
    "        return theano.Apply(self, [x], [x.type()])\n",
    "    \n",
    "    def perform(self, node, inputs, output_storage):\n",
    "        x, = inputs\n",
    "        z, = output_storage\n",
    "        \n",
    "        zero_idxs = np.argsort(np.abs(x), axis = -1)[:, :-self.n]\n",
    "        col_idxs = np.arange(x.shape[0])[:, None]\n",
    "        out = np.copy(x)\n",
    "        x[col_idxs, zero_idxs] = 0\n",
    "        z[0] = x\n",
    "    \n",
    "    def grad(self, input, output_gradients):\n",
    "        x, = input\n",
    "        g, = output_gradients\n",
    "        \n",
    "        zero_idxs = T.argsort(T.abs_(x), axis = -1)[:, :-self.n, None]\n",
    "        \n",
    "        idnt = T.eye(x.shape[-1]) * 0.99\n",
    "        \n",
    "        grad_mult = idnt[zero_idxs]\n",
    "        grad_mult = T.sum(grad_mult, axis = -3)[:, 0, :]\n",
    "        grad_mult = 1.0 - grad_mult\n",
    "        \n",
    "        grad_mult = grad_mult + 0.01\n",
    "        \n",
    "        out_grad = g * grad_mult\n",
    "        \n",
    "        return [out_grad]\n",
    "        \n",
    "    def infer_shape(self, node, i0_shapes):\n",
    "        # output shape is same as input shape\n",
    "        return i0_shapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# only works with theano dawg\n",
    "class OrthogonalDense(Layer):\n",
    "    def __init__(self, tied_to = None, has_bias = False, **kwargs):\n",
    "        super(OrthogonalDense, self).__init__(**kwargs)\n",
    "        self.tied_to = tied_to\n",
    "        self.has_bias = has_bias\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        input_dim = input_shape[1]\n",
    "        \n",
    "        self.n = input_dim\n",
    "        \n",
    "        # we store a full matrix, but use it as a skew-symmetric one\n",
    "        if (self.tied_to is None):\n",
    "            self.W = np.random.uniform(-0.001, 0.001, (self.n, self.n))\n",
    "            self.W = K.variable(self.W)\n",
    "            self.trainable_weights = [self.W]\n",
    "            \n",
    "            if (self.has_bias):\n",
    "                self.b = K.variable(np.zeros((self.n,)))\n",
    "                self.trainable_weights = [self.W, self.b]\n",
    "    \n",
    "    def produce_unitary(self):\n",
    "        # produce skew-symmetric matrix from W\n",
    "        upper = T.triu(self.W, k = 1)\n",
    "        skew = upper - K.transpose(upper)\n",
    "        \n",
    "        # compute approximate matrix exponential\n",
    "        a = K.eye(self.n)\n",
    "        unitary = K.zeros((self.n, self.n))\n",
    "        \n",
    "        for i in xrange(0, 11):\n",
    "            c = 1.0 / math.factorial(i)\n",
    "\n",
    "            unitary += c * a\n",
    "            a = T.dot(a, skew)\n",
    "        \n",
    "        return unitary\n",
    "        \n",
    "    def call(self, x, mask=None):\n",
    "        if (self.tied_to is None):\n",
    "            unitary = self.produce_unitary()\n",
    "            out = K.dot(x, unitary)\n",
    "            return ((out + self.b) if self.has_bias else out)\n",
    "        else:\n",
    "            #unitary = T.nlinalg.matrix_inverse(self.tied_to.produce_unitary())\n",
    "            unitary = K.transpose(self.tied_to.produce_unitary())\n",
    "            inp = (x - self.tied_to.b) if self.tied_to.has_bias else x\n",
    "            return K.dot(inp, unitary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class CodeRound(T.Op):\n",
    "    # properties attribute\n",
    "    __props__ = ()\n",
    "    \n",
    "    def __init__(self, nbins):\n",
    "        self.nbins = nbins\n",
    "        super(CodeRound, self).__init__()\n",
    "        \n",
    "    def make_node(self, x):\n",
    "        assert hasattr(self, '_props'), \"Your version of theano is too old to support __props__.\"\n",
    "        x = T.as_tensor_variable(x)\n",
    "        return theano.Apply(self, [x], [x.type()])\n",
    "    \n",
    "    def perform(self, node, inputs, output_storage):\n",
    "        x, = inputs\n",
    "        z, = output_storage\n",
    "        \n",
    "        s = (x + 1.0) / 2.0\n",
    "        s = np.round(s * float(self.nbins - 1)) / float(self.nbins - 1)\n",
    "        s = (s * 2.0) - 1.0\n",
    "        \n",
    "        z[0] = s\n",
    "    \n",
    "    def grad(self, input, output_gradients):\n",
    "        # pass through gradients unchanged\n",
    "        x, = input\n",
    "        g, = output_gradients\n",
    "        return [g]\n",
    "        \n",
    "    def infer_shape(self, node, i0_shapes):\n",
    "        # output shape is same as input shape\n",
    "        return i0_shapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# only works with theano dawg\n",
    "class TiedDense(Layer):\n",
    "    def __init__(self, has_bias = False, tied_to = None, inverse = False, **kwargs):\n",
    "        super(TiedDense, self).__init__(**kwargs)\n",
    "        self.has_bias = has_bias\n",
    "        self.tied_to = tied_to\n",
    "        self.inverse = inverse\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        input_dim = input_shape[1]\n",
    "        \n",
    "        self.n = input_dim\n",
    "        \n",
    "        # we store a full matrix, but use it as a skew-symmetric one\n",
    "        if (self.tied_to is None):\n",
    "            self.W = np.eye(self.n) + np.random.uniform(-0.001, 0.001, (self.n, self.n))\n",
    "            self.W = K.variable(self.W)\n",
    "            \n",
    "            if (self.has_bias):\n",
    "                self.b = np.random.uniform(-0.001, 0.001, (self.n,))\n",
    "                self.b = K.variable(self.b)\n",
    "                self.trainable_weights = [self.W, self.b]     \n",
    "            else:\n",
    "                self.b = np.zeros((self.n,))\n",
    "                self.b = K.variable(self.b)\n",
    "                self.trainable_weights = [self.W]\n",
    "        \n",
    "    def call(self, x, mask=None):\n",
    "        if (self.tied_to is None):\n",
    "            return K.dot(x + self.b, self.W) + self.b\n",
    "        else:\n",
    "            if (self.inverse):\n",
    "                w = T.nlinalg.matrix_inverse(self.tied_to.W)\n",
    "            else:\n",
    "                w = K.transpose(self.tied_to.W)\n",
    "            return K.dot(x - self.tied_to.b, w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class VariableSelectMax(T.Op):\n",
    "    # properties attribute\n",
    "    __props__ = ()\n",
    "    \n",
    "    def __init__(self, n):\n",
    "        super(VariableSelectMax, self).__init__()\n",
    "        self.n = n\n",
    "        \n",
    "    def make_node(self, x):\n",
    "        assert hasattr(self, '_props'), \"Your version of theano is too old to support __props__.\"\n",
    "        x = T.as_tensor_variable(x)\n",
    "        return theano.Apply(self, [x], [x.type()])\n",
    "    \n",
    "    def perform(self, node, inputs, output_storage):\n",
    "        x, = inputs\n",
    "        z, = output_storage\n",
    "        \n",
    "        zero_idxs = np.argsort(np.abs(x), axis = -1)[:, :-K.get_value(self.n)]\n",
    "        col_idxs = np.arange(x.shape[0])[:, None]\n",
    "        out = np.copy(x)\n",
    "        x[col_idxs, zero_idxs] = 0\n",
    "        z[0] = x\n",
    "    \n",
    "    def grad(self, input, output_gradients):\n",
    "        x, = input\n",
    "        g, = output_gradients\n",
    "        \n",
    "        zero_idxs = T.argsort(T.abs_(x), axis = -1)[:, :-K.get_value(self.n), None]\n",
    "        \n",
    "        idnt = T.eye(x.shape[-1]) * 0.99\n",
    "        \n",
    "        grad_mult = idnt[zero_idxs]\n",
    "        grad_mult = T.sum(grad_mult, axis = -3)[:, 0, :]\n",
    "        grad_mult = 1.0 - grad_mult\n",
    "        \n",
    "        grad_mult = grad_mult + 0.01\n",
    "        \n",
    "        out_grad = g * grad_mult\n",
    "        \n",
    "        return [out_grad]\n",
    "        \n",
    "    def infer_shape(self, node, i0_shapes):\n",
    "        # output shape is same as input shape\n",
    "        return i0_shapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 to 6    wt 1.0\n",
      "6 to 12    wt 1.1\n",
      "12 to 20    wt 1.21\n",
      "20 to 29    wt 1.331\n",
      "29 to 39    wt 1.4641\n",
      "39 to 50    wt 1.61051\n",
      "50 to 63    wt 1.771561\n",
      "63 to 77    wt 1.9487171\n",
      "77 to 94    wt 2.14358881\n",
      "94 to 113    wt 2.357947691\n",
      "113 to 134    wt 2.5937424601\n",
      "134 to 158    wt 2.85311670611\n",
      "158 to 185    wt 3.13842837672\n",
      "185 to 216    wt 3.45227121439\n",
      "216 to 251    wt 3.79749833583\n",
      "251 to 291    wt 4.17724816942\n",
      "291 to 336    wt 4.59497298636\n",
      "336 to 387    wt 5.05447028499\n",
      "387 to 446    wt 5.55991731349\n",
      "446 to 512    wt 6.11590904484\n"
     ]
    }
   ],
   "source": [
    "NUM_MEL_BINS = 20\n",
    "\n",
    "def freqToMel(freq):\n",
    "    return 1127.01048 * math.log(1 + freq / 700.0)\n",
    "\n",
    "def melToFreq(mel):\n",
    "    return 700 * (math.exp(mel / 1127.01048) - 1)\n",
    "\n",
    "def generateMelWindows():\n",
    "    minHz = 0\n",
    "    maxHz = SAMPLE_RATE / 2\n",
    "    numDCTBins = WINDOW_SIZE\n",
    "    \n",
    "    minMel = freqToMel(minHz)\n",
    "    maxMel = freqToMel(maxHz)\n",
    "\n",
    "    # evenly spaced bins between minMel and maxMel\n",
    "    melRange = np.arange(0, NUM_MEL_BINS + 1).astype('float32')\n",
    "    melIdxs = melRange * (maxMel - minMel) / (NUM_MEL_BINS) + minMel\n",
    "    \n",
    "    # convert back to freq / dct domain\n",
    "    for i in xrange(0, NUM_MEL_BINS + 1):\n",
    "        melIdxs[i] = melToFreq(melIdxs[i])\n",
    "        melIdxs[i] = math.floor(numDCTBins * melIdxs[i] / maxHz)\n",
    "    melIdxs = melIdxs.astype(np.int32)\n",
    "    \n",
    "    return melIdxs\n",
    "\n",
    "BIN_WEIGHTS = []\n",
    "for i in xrange(0, NUM_MEL_BINS):\n",
    "    BIN_WEIGHTS.append(1.0 * math.pow(1.1, i))\n",
    "\n",
    "melIdxs = generateMelWindows()\n",
    "for i in xrange(0, NUM_MEL_BINS):\n",
    "    print melIdxs[i], \"to\", melIdxs[i + 1], \"   wt\", BIN_WEIGHTS[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[ 1. ]\n",
      "  [ 1.5]\n",
      "  [ 2. ]\n",
      "  [ 2.5]\n",
      "  [ 3. ]\n",
      "  [ 3.5]\n",
      "  [ 4. ]\n",
      "  [ 4.5]\n",
      "  [ 5. ]\n",
      "  [ 5.5]\n",
      "  [ 6. ]\n",
      "  [ 6. ]]\n",
      "\n",
      " [[ 5. ]\n",
      "  [ 6. ]\n",
      "  [ 7. ]\n",
      "  [ 8. ]\n",
      "  [ 9. ]\n",
      "  [ 4. ]\n",
      "  [-1. ]\n",
      "  [ 0.5]\n",
      "  [ 2. ]\n",
      "  [ 1. ]\n",
      "  [ 0. ]\n",
      "  [ 0. ]]]\n"
     ]
    }
   ],
   "source": [
    "x = K.variable([[1, 2, 3, 4, 5, 6], [5, 7, 9, -1, 2, 0]])\n",
    "x = K.reshape(x, (x.shape[0], x.shape[1], 1))\n",
    "\n",
    "\n",
    "r = T.repeat(x, 2, axis = 1)\n",
    "s = T.roll(r, -1, axis = 1)\n",
    "u = ((r[:, :-1] + s[:, :-1]) / 2.0)\n",
    "u = T.concatenate((u, r[:, -1:]), axis = 1)\n",
    "print u.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAhcAAAFkCAYAAACThxm6AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nVxYXkiQpVxYXkiQpV/8fxWaBxKhMJZQAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f41419b1890>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# leaky piecewise linear function\n",
    "\n",
    "x = np.linspace(-2.0, 2.0, 100)\n",
    "\n",
    "idxs = (abs(x) > 0.5).astype('int')\n",
    "zeroed_out = x * idxs\n",
    "residual = 0.1 * x * (1 - idxs)\n",
    "y = zeroed_out + residual\n",
    "\n",
    "plt.plot(x, y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.1  0.2  3.   0.  -0.1  4.   9.   0.1  9. ]\n"
     ]
    }
   ],
   "source": [
    "thresh = 2\n",
    "alpha = 0.1\n",
    "\n",
    "x = K.variable([1, 2, 3, 0, -1, 4, 9, 1, 9])\n",
    "\n",
    "idxs = (K.abs(x) > thresh)\n",
    "zeroed_out = x * idxs\n",
    "residual = alpha * x * (1 - idxs)\n",
    "\n",
    "print (zeroed_out + residual).eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "input_4 (InputLayer)             (None, 512, 1)        0                                            \n",
      "____________________________________________________________________________________________________\n",
      "sequential_1 (Sequential)        (None, 1)             271345      input_4[0][0]                    \n",
      "====================================================================================================\n",
      "Total params: 271,345\n",
      "Trainable params: 271,345\n",
      "Non-trainable params: 0\n",
      "____________________________________________________________________________________________________\n",
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "input_1 (InputLayer)             (None, 512, 1)        0                                            \n",
      "____________________________________________________________________________________________________\n",
      "model_1 (Model)                  (None, 128)           240545      input_1[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "model_2 (Model)                  (None, 512, 1)        228257      model_1[1][0]                    \n",
      "====================================================================================================\n",
      "Total params: 468,802\n",
      "Trainable params: 468,802\n",
      "Non-trainable params: 0\n",
      "____________________________________________________________________________________________________\n",
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "input_1 (InputLayer)             (None, 512, 1)        0                                            \n",
      "____________________________________________________________________________________________________\n",
      "model_1 (Model)                  (None, 128)           240545      input_1[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "model_2 (Model)                  (None, 512, 1)        228257      model_1[1][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "sequential_1 (Sequential)        (None, 1)             271345      model_2[1][0]                    \n",
      "====================================================================================================\n",
      "Total params: 740,147\n",
      "Trainable params: 740,147\n",
      "Non-trainable params: 0\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.activations import softmax, sigmoid\n",
    "from scipy.fftpack import dct, idct\n",
    "\n",
    "# lambda to compute MSE between 2 vectors\n",
    "def mse_lambda(vects):\n",
    "    x, y = vects\n",
    "    return K.mean(K.square(x - y))\n",
    "\n",
    "# freeze weights for stacked training\n",
    "def make_trainable(net, val):\n",
    "    net.trainable = val\n",
    "    for l in net.layers:\n",
    "        l.trainable = val\n",
    "\n",
    "# we generate a new optimizer of the same kind for every model\n",
    "# we train\n",
    "def opti():\n",
    "    #return Adam(lr = 0.0002, beta_1 = 0.5)\n",
    "    return Adadelta()\n",
    "\n",
    "input_dim = (WINDOW_SIZE, 1)\n",
    "input_size = np.prod(input_dim)\n",
    "bottleneck_size = WINDOW_SIZE / 4\n",
    "\n",
    "\n",
    "tau = K.variable(1.0, name = 'temperature')\n",
    "anneal_rate = 0.02\n",
    "min_temperature = 0.1\n",
    "    \n",
    "\n",
    "def linear_upsample_1d(x):\n",
    "    r = T.repeat(x, 2, axis = 1)\n",
    "    s = T.roll(r, -1, axis = 1)\n",
    "    u = ((r[:, :-1] + s[:, :-1]) / 2.0)\n",
    "    u = T.concatenate((u, r[:, -1:]), axis = 1)\n",
    "    return u\n",
    "def linear_upsample_shape(shape):\n",
    "    return (shape[0], shape[1] * 2, shape[2])\n",
    "\n",
    "def LinearUpSampling1D():\n",
    "    return Lambda(linear_upsample_1d, output_shape = linear_upsample_shape)\n",
    "\n",
    "\n",
    "\n",
    "BNORM_GEN = False\n",
    "BNORM_DSC = False\n",
    "\n",
    "thresh = 0.5\n",
    "alpha = 0.1\n",
    "def leaky_piecewise_linear(x):\n",
    "    return K.maximum(1.0 - K.abs(x), -0.01 * K.abs(x))\n",
    "    #idxs = (K.abs(x) > thresh)\n",
    "    #zeroed_out = x * idxs\n",
    "    #residual = alpha * x * (1 - idxs)\n",
    "    #return zeroed_out + residual\n",
    "\n",
    "def activation():\n",
    "    #return Activation('linear')\n",
    "    #return PReLU(init = 'he_normal')\n",
    "    #return Lambda(leaky_piecewise_linear)\n",
    "    #return ELU()\n",
    "    return LeakyReLU(0.3)\n",
    "    \n",
    "res_init = 'he_uniform'\n",
    "\n",
    "def residual_block(output_dim = 64, filt_size = 5, conv = True):\n",
    "    def f(input):\n",
    "        #bn1 = BatchNormalization()(input)\n",
    "        #act1 = activation()(bn1)\n",
    "        \n",
    "        conv1 = input\n",
    "        conv1 = Convolution1D(output_dim, filt_size, border_mode = 'same',\n",
    "                          init = res_init, activation = 'linear',\n",
    "                          bias = True)(conv1)\n",
    "        if (BNORM_GEN): conv1 = BatchNormalization()(conv1)\n",
    "        conv1 = activation()(conv1)\n",
    "                \n",
    "        conv2 = conv1\n",
    "        conv2 = Convolution1D(output_dim, filt_size, border_mode = 'same',\n",
    "                          init = res_init, activation = 'linear',\n",
    "                          bias = True)(conv2)\n",
    "        \n",
    "        #'''\n",
    "        residual = conv2\n",
    "        if (conv):\n",
    "            shortcut = Lambda(lambda x : K.repeat_elements(x, output_dim, axis = -1),\n",
    "                              output_shape = (lambda s : (s[0], s[1], output_dim)))(input)\n",
    "        else:\n",
    "            shortcut = input\n",
    "        \n",
    "        m = merge([shortcut, residual], mode = 'sum')\n",
    "        return m\n",
    "    \n",
    "    return f\n",
    "\n",
    "\n",
    "# total # layers: NUM_RES_BLOCKS * 4\n",
    "NUM_RES_BLOCKS = 8\n",
    "NBINS = 101\n",
    "nselect = K.variable(511, dtype = 'int32')\n",
    "NCHAN = 32\n",
    "TIMES_DOWNSAMPLE = 2\n",
    "FILT_SIZE = 9\n",
    "FILT_MID = FILT_SIZE / 2 + 1\n",
    "\n",
    "\n",
    "def dct_weights(n):\n",
    "    weights = dct(np.eye(n), norm = 'ortho')\n",
    "    bias = np.zeros(n)\n",
    "    return [weights, bias]\n",
    "\n",
    "def idct_weights(n):\n",
    "    weights = idct(np.eye(n), norm = 'ortho')\n",
    "    bias = np.zeros(n)\n",
    "    return [weights, bias]\n",
    "\n",
    "    \n",
    "# ---------------------------------------------------------------------------\n",
    "# autoencoder: takes an audio window, compresses it, and tries to reconstruct it\n",
    "# ---------------------------------------------------------------------------\n",
    "def autoencoder_structure(dim):\n",
    "    # convolutional layers have weight matrices of shape:\n",
    "    #     (filter_length, 1, input_dim, nb_filter)\n",
    "    # and biases of size (nb_filter,)\n",
    "    \n",
    "    # weights for an \"replication convolution\" (which just\n",
    "    # replicates the input across all channels)\n",
    "    #     filter of length 5, going from 1 channel to NCHAN\n",
    "    def replicate_conv():\n",
    "        weights = np.zeros((FILT_SIZE, 1, 1, NCHAN))\n",
    "        weights[FILT_MID] = np.ones((1, 1, NCHAN))\n",
    "        weights += np.random.uniform(-0.01, 0.01, weights.shape)\n",
    "        biases = np.zeros((NCHAN,))\n",
    "        return [weights, biases]\n",
    "    \n",
    "    \n",
    "    # weights for an \"average convolution\" (which just\n",
    "    # replicates the input across all channels)\n",
    "    #     filter of length 1, going from NCHAN channels to 1\n",
    "    def average_conv():\n",
    "        weights = np.ones((1, 1, NCHAN, 1)) / float(NCHAN)\n",
    "        weights += np.random.uniform(-0.01, 0.01, weights.shape)\n",
    "        biases = np.zeros((1,))\n",
    "        return [weights, biases]\n",
    "    \n",
    "    \n",
    "    # weights for a \"phase shift up\" convolution (by default,\n",
    "    # performs an upsample)\n",
    "    #     filter of length 5, going from NCHAN to NCHAN * 2\n",
    "    def shift_up_conv():\n",
    "        weights = np.zeros((FILT_SIZE, 1, NCHAN, NCHAN * 2))\n",
    "        weights[FILT_MID, 0] = np.repeat(np.eye(NCHAN), 2, axis = 1)\n",
    "        weights += np.random.uniform(-0.01, 0.01, weights.shape)\n",
    "        biases = np.zeros((NCHAN * 2,))\n",
    "        return [weights, biases]\n",
    "    \n",
    "    \n",
    "    # weights for a \"phase shift down\" convolution (by default,\n",
    "    # performs an average)\n",
    "    #     filter of length 5, going from NCHAN to NCHAN / 2\n",
    "    def shift_down_conv():\n",
    "        weights = np.zeros((FILT_SIZE, 1, NCHAN, NCHAN / 2))\n",
    "        weights[FILT_MID, 0] = np.repeat(np.eye(NCHAN / 2), 2, axis = 0) / 2.0\n",
    "        weights += np.random.uniform(-0.01, 0.01, weights.shape)\n",
    "        biases = np.zeros((NCHAN / 2,))\n",
    "        return [weights, biases]\n",
    "    \n",
    "    \n",
    "    # weights for a \"identity\" convolution (by default,\n",
    "    # does nothing to its input)\n",
    "    #     filter of length 5, going from NCHAN to NCHAN\n",
    "    #     (when combined with subsampling, this is a nearest-neighbor\n",
    "    #      downsample)\n",
    "    def identity_conv():\n",
    "        weights = np.zeros((FILT_SIZE, 1, NCHAN, NCHAN))\n",
    "        weights[FILT_MID, 0] = np.eye(NCHAN)\n",
    "        weights += np.random.uniform(-0.01, 0.01, weights.shape)\n",
    "        biases = np.zeros((NCHAN,))\n",
    "        return [weights, biases]\n",
    "    \n",
    "    # identity matrix plus random noise\n",
    "    def identity_mat(n):\n",
    "        weights = np.eye(n) + np.random.uniform(-0.01, 0.01, (n, n))\n",
    "        biases = np.zeros(n)\n",
    "        return [weights, biases]\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    enc_input = Input(shape = dim)\n",
    "    enc = Reshape(dim, input_shape = dim)(enc_input)\n",
    "    \n",
    "    # increase number of channels via convolution\n",
    "    repeated = Lambda(lambda x : K.repeat_elements(x, NCHAN, axis = -1),\n",
    "                      output_shape = (lambda s : (s[0], s[1], NCHAN)))(enc)\n",
    "    res = Convolution1D(NCHAN, FILT_SIZE, border_mode = 'same',\n",
    "                          activation = 'linear',\n",
    "                          init = res_init,\n",
    "                          bias = True)(enc)\n",
    "    res = activation()(res)\n",
    "    enc = merge([repeated, res], mode = 'sum')\n",
    "    \n",
    "    # downsample\n",
    "    for i in xrange(0, TIMES_DOWNSAMPLE):\n",
    "        orig = enc\n",
    "        \n",
    "        res = Convolution1D(NCHAN, FILT_SIZE, border_mode = 'same',\n",
    "                              init = res_init, activation = 'linear',\n",
    "                              bias = True)(orig)\n",
    "        if (BNORM_GEN): res = BatchNormalization()(res)\n",
    "        res = activation()(res)\n",
    "        \n",
    "        enc = merge([orig, res], mode = 'sum')        \n",
    "        enc = AveragePooling1D(2)(enc)\n",
    "\n",
    "    \n",
    "    # residual blocks\n",
    "    for i in xrange(0, NUM_RES_BLOCKS):\n",
    "        enc = residual_block(NCHAN, FILT_SIZE, False)(enc)\n",
    "    \n",
    "    # convolution across feature maps\n",
    "    enc = Convolution1D(1, 1, border_mode = 'same',\n",
    "                              bias = True,\n",
    "                              weights = average_conv())(enc)\n",
    "\n",
    "    enc = Reshape((bottleneck_size,))(enc)\n",
    "    \n",
    "    #enc = Dense(bottleneck_size, init = 'identity')(enc)\n",
    "    enc = Activation('tanh')(enc)\n",
    "    \n",
    "    #enc = UniformQuantizer(NBINS, True)(enc)\n",
    "    enc = Lambda(lambda x : CodeRound(NBINS)(x))(enc)\n",
    "    \n",
    "    enc = Model(input = enc_input, output = enc)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    dec_input = Input(shape = (bottleneck_size,))\n",
    "    dec = Reshape((bottleneck_size,), input_shape = (bottleneck_size,))(dec_input)\n",
    "    \n",
    "    #dec = Dense(bottleneck_size, init = 'identity')(dec)\n",
    "    \n",
    "    dec = Reshape((bottleneck_size, 1,))(dec)\n",
    "    # increase number of channels via convolution\n",
    "    repeated = Lambda(lambda x : K.repeat_elements(x, NCHAN, axis = -1),\n",
    "                      output_shape = (lambda s : (s[0], s[1], NCHAN)))(dec)\n",
    "    res = Convolution1D(NCHAN, FILT_SIZE, border_mode = 'same',\n",
    "                          activation = 'linear',\n",
    "                          init = res_init,\n",
    "                          bias = True)(dec)\n",
    "    res = activation()(res)\n",
    "    dec = merge([repeated, res], mode = 'sum')\n",
    "    \n",
    "    # residual blocks\n",
    "    for i in xrange(0, NUM_RES_BLOCKS):\n",
    "        dec = residual_block(NCHAN, FILT_SIZE, False)(dec)\n",
    "        \n",
    "    # upsample\n",
    "    for i in xrange(0, TIMES_DOWNSAMPLE):\n",
    "        up = LinearUpSampling1D()(dec)\n",
    "        \n",
    "        res = Convolution1D(NCHAN, FILT_SIZE, border_mode = 'same',\n",
    "                              init = res_init, activation = 'linear',\n",
    "                              bias = True)(up)\n",
    "        if (BNORM_GEN): res = BatchNormalization()(res)\n",
    "        res = activation()(res)\n",
    "        \n",
    "        dec = merge([up, res], mode = 'sum')\n",
    "    \n",
    "    # convolution across feature maps\n",
    "    dec = Convolution1D(1, 1, border_mode = 'same',\n",
    "                              activation = 'linear',\n",
    "                              bias = True,\n",
    "                              weights = average_conv())(dec)\n",
    "    dec = Activation('tanh')(dec)\n",
    "    \n",
    "    dec = Model(input = dec_input, output = dec)\n",
    "    \n",
    "    return enc, dec\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# discriminator: tries to differentiate between original and reconstructed samples\n",
    "# ---------------------------------------------------------------------------\n",
    "DSC_FILTS = 16\n",
    "DSC_DENSE = 32\n",
    "DSC_FILT_SIZE = 5\n",
    "def discriminator_structure(dim):\n",
    "    dsc = Sequential()\n",
    "    \n",
    "    # discriminator works in frequency space\n",
    "    '''\n",
    "    dsc.add(Reshape((WINDOW_SIZE,), input_shape = dim))\n",
    "    transform = Dense(WINDOW_SIZE, weights = dct_weights(WINDOW_SIZE))\n",
    "    dsc.add(transform)\n",
    "    transform.trainable = False\n",
    "    dsc.add(Reshape((WINDOW_SIZE, 1)))\n",
    "    #'''\n",
    "    \n",
    "    #'''\n",
    "    dsc.add(Convolution1D(DSC_FILTS, DSC_FILT_SIZE, border_mode='same',\n",
    "                                    init = 'he_uniform',\n",
    "                                    input_shape = dim, activation = 'linear'))\n",
    "    if (BNORM_DSC): dsc.add(BatchNormalization())\n",
    "    dsc.add(LeakyReLU(0.3))\n",
    "    dsc.add(SpatialDropout1D(0.1))\n",
    "    \n",
    "    dsc.add(Convolution1D(DSC_FILTS, DSC_FILT_SIZE, border_mode='same',\n",
    "                                    init = 'he_uniform',\n",
    "                                    activation = 'linear'))\n",
    "    if (BNORM_DSC): dsc.add(BatchNormalization())\n",
    "    dsc.add(LeakyReLU(0.3))\n",
    "    dsc_feats = dsc\n",
    "    dsc.add(AveragePooling1D(2))\n",
    "    dsc.add(SpatialDropout1D(0.1))\n",
    "    \n",
    "    dsc.add(Convolution1D(DSC_FILTS * 2, DSC_FILT_SIZE, border_mode='same',\n",
    "                                    init = 'he_uniform',\n",
    "                                    activation = 'linear'))\n",
    "    if (BNORM_DSC): dsc.add(BatchNormalization())\n",
    "    dsc.add(LeakyReLU(0.3))\n",
    "    dsc.add(SpatialDropout1D(0.1))\n",
    "    \n",
    "\n",
    "    dsc.add(Convolution1D(DSC_FILTS * 2, DSC_FILT_SIZE, border_mode='same',\n",
    "                                    init = 'he_uniform',\n",
    "                                    activation = 'linear'))\n",
    "    if (BNORM_DSC): dsc.add(BatchNormalization())\n",
    "    dsc.add(LeakyReLU(0.3))\n",
    "    dsc.add(SpatialDropout1D(0.1))\n",
    "    \n",
    "    dsc.add(Flatten())\n",
    "    \n",
    "    dsc.add(Dense(DSC_DENSE, init = 'he_uniform'))\n",
    "    dsc.add(LeakyReLU(0.3))\n",
    "    #'''\n",
    "    \n",
    "    dsc.add(Dense(1, activation = 'linear', init = 'he_uniform'))\n",
    "    \n",
    "    return dsc, dsc_feats\n",
    "\n",
    "\n",
    "# construct autoencoder to be used in adversarial training\n",
    "ac_input = Input(shape = input_dim)\n",
    "ac_enc, ac_dec = autoencoder_structure(input_dim)\n",
    "ac_embedding = ac_enc(ac_input)\n",
    "ac_reconstructed = ac_dec(ac_embedding)\n",
    "\n",
    "autoencoder = Model(input = [ac_input], output = [ac_reconstructed])\n",
    "autoencoder.compile(loss = 'mean_squared_error', optimizer = Adam())\n",
    "\n",
    "# construct discriminator: regular\n",
    "dsc_input_dim = (WINDOW_SIZE, 1)\n",
    "dsc_input = Input(shape = input_dim)\n",
    "dsc_struct, dscfeat_struct = discriminator_structure(dsc_input_dim)\n",
    "dsc_label = dsc_struct(dsc_input)\n",
    "ac_dsc_label = dsc_struct(ac_reconstructed)\n",
    "\n",
    "# get feature loss at intermediate layer of discriminator\n",
    "dscfeat_inp = dscfeat_struct(ac_input)\n",
    "dscfeat_rec = dscfeat_struct(ac_reconstructed)\n",
    "dscfeat_mse = Lambda(mse_lambda, output_shape = (1,))([dscfeat_inp, dscfeat_rec])\n",
    "\n",
    "\n",
    "\n",
    "clip_point = 0.0001\n",
    "k = 0.5\n",
    "def code_sparsity_constraint(placeholder, code):\n",
    "    clipped = K.clip(K.abs(code), clip_point, 1.0)\n",
    "    lk_norm = K.pow(clipped, k)\n",
    "    loss = K.mean(lk_norm)\n",
    "    \n",
    "    return K.repeat_elements(loss.reshape((1,)), code.shape[0], 0)\n",
    "\n",
    "\n",
    "\n",
    "def minimize_deviation(placeholder, code):\n",
    "    v = K.var(code)\n",
    "    clipped = K.clip(v, clip_point, 4.0)\n",
    "    loss = K.sqrt(clipped)\n",
    "    \n",
    "    return K.repeat_elements(loss.reshape((1,)), code.shape[0], 0)\n",
    "    \n",
    "\n",
    "\n",
    "dct_mat = K.variable(dct(np.eye(WINDOW_SIZE), norm = 'ortho'))\n",
    "\n",
    "def apply_dct(batch):\n",
    "    reshaped = batch.reshape((1, batch.shape[0], batch.shape[1]))\n",
    "    result = T.tensordot(dct_mat, reshaped, [[0], [2]])\n",
    "    result = result.reshape((result.shape[0], result.shape[2])).T\n",
    "    result = result.reshape((result.shape[0], result.shape[1]))\n",
    "    return result\n",
    "\n",
    "\n",
    "def mel_dct_loss(y_true, y_pred):\n",
    "    dct_true = apply_dct(y_true)\n",
    "    dct_pred = apply_dct(y_pred)\n",
    "    \n",
    "    # take average DCT loss over Mel bins\n",
    "    loss = None\n",
    "    for i in xrange(0, NUM_MEL_BINS):\n",
    "        start = melIdxs[i]\n",
    "        end = melIdxs[i + 1]\n",
    "        \n",
    "        sq = K.square(dct_pred[:, start:end] - dct_true[:, start:end])\n",
    "        mse = K.mean(sq, axis = -1)\n",
    "        mse = K.clip(mse, 0.000001, 10000.0)\n",
    "        if (loss is None):\n",
    "            loss = K.sqrt(mse) * BIN_WEIGHTS[i]\n",
    "            #loss = mse\n",
    "        else:\n",
    "            loss += K.sqrt(mse) * BIN_WEIGHTS[i]\n",
    "            #loss += mse\n",
    "    loss /= float(NUM_MEL_BINS)\n",
    "    \n",
    "    return loss\n",
    "\n",
    "\n",
    "\n",
    "def rmse(y_true, y_pred):\n",
    "    mse = K.mean(K.square(y_pred - y_true), axis = -1)\n",
    "    #return mse\n",
    "    mse = K.clip(mse, 0.000001, 10000.0)\n",
    "    return K.sqrt(mse)\n",
    "\n",
    "\n",
    "# compile model\n",
    "loss_weights = [250.0, 1.0, 25.0]\n",
    "loss_functions = [rmse, 'mean_absolute_error', minimize_deviation]\n",
    "n_recons = 1\n",
    "n_discrim = 1\n",
    "n_code = 1\n",
    "n_other = 0\n",
    "assert(n_recons + n_discrim + n_code + n_other == len(loss_weights))\n",
    "assert(len(loss_weights) == len(loss_functions))\n",
    "\n",
    "\n",
    "make_trainable(autoencoder, False)\n",
    "discriminator = Model(input = [dsc_input], output = [dsc_label])\n",
    "discriminator.compile(loss = ['mean_absolute_error'], optimizer = Adadelta())\n",
    "discriminator.summary()\n",
    "\n",
    "autoencoder.summary()\n",
    "\n",
    "make_trainable(discriminator, False)\n",
    "make_trainable(autoencoder, True)\n",
    "model = Model(input = [ac_input], output = [ac_reconstructed] * n_recons + \\\n",
    "                                           [ac_dsc_label] * n_discrim + \\\n",
    "                                           [ac_embedding] * n_code + \\\n",
    "                                           [dscfeat_mse] * n_other)\n",
    "model.compile(loss = loss_functions,\n",
    "              loss_weights = loss_weights,\n",
    "              optimizer = Adam())\n",
    "model.summary()\n",
    "\n",
    "\n",
    "\n",
    "X_train = np.copy(processedWindows)\n",
    "ntrain = X_train.shape[0]\n",
    "discrim_epoch = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def test_discriminator(discriminator, autoencoder, X, y, verbose = True):\n",
    "    # verify discriminator was trained properly\n",
    "    y_hat = discriminator.predict(X)\n",
    "    y_hat[y_hat >= 0.5] = 1\n",
    "    y_hat[y_hat < 0.5] = 0\n",
    "\n",
    "    n_total = y.shape[0]\n",
    "    n_correct = np.sum(np.ravel(y_hat) == y)\n",
    "\n",
    "    acc = n_correct * 100.0 / n_total\n",
    "    if (verbose):\n",
    "        print \"Discriminator accuracy: %0.02f pct (%d of %d) right\"%(acc, n_correct, n_total)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def autoencoderTest(waveFilename, prefix, autoencoder, verbose = True):\n",
    "    [rate, data] = sciwav.read(waveFilename)\n",
    "    data = data.astype(np.float32)\n",
    "    processedWave, wparams = preprocessWaveform(data)\n",
    "    windows = extractWindows(processedWave, STEP_SIZE, OVERLAP_SIZE)\n",
    "    \n",
    "\n",
    "    # first, write desired reconstruction\n",
    "    transformed, tparams = preprocessWindows(windows)\n",
    "    if (verbose):\n",
    "        print transformed.shape\n",
    "    \n",
    "    desired = unpreprocessWindows(transformed, tparams)\n",
    "    desired = reconstructFromWindows(desired, OVERLAP_SIZE, OVERLAP_FUNC)\n",
    "    desired = unpreprocessWaveform(desired, wparams)\n",
    "    desired = np.clip(desired, -32767, 32767)\n",
    "    #sciwav.write(prefix + \"_res_desired.wav\", rate, desired.astype(np.int16))\n",
    "    \n",
    "    # then, run NN on transformed windows\n",
    "    transformed, tparams = preprocessWindows(windows)\n",
    "    \n",
    "    transformed = np.reshape(transformed, (transformed.shape[0], WINDOW_SIZE, 1))\n",
    "    autoencOutput = autoencoder.predict(transformed, batch_size = 128, verbose = (1 if verbose else 0))\n",
    "    if (verbose):\n",
    "        print autoencOutput.shape\n",
    "    autoencOutput = np.reshape(autoencOutput, (autoencOutput.shape[0], WINDOW_SIZE))\n",
    "    \n",
    "    if (verbose):\n",
    "        print autoencOutput.shape\n",
    "    recons = unpreprocessWindows(autoencOutput, tparams)\n",
    "    recons = reconstructFromWindows(recons, OVERLAP_SIZE, OVERLAP_FUNC)\n",
    "    recons = unpreprocessWaveform(recons, wparams)\n",
    "    recons = np.clip(recons, -32767, 32767)\n",
    "    \n",
    "    sciwav.write(prefix + \"_output.wav\", rate, recons.astype(np.int16))\n",
    "    \n",
    "    metrics = [\n",
    "        np.max(desired),\n",
    "        np.min(desired),\n",
    "        np.max(recons),\n",
    "        np.min(recons),\n",
    "        mse(recons, desired),\n",
    "        avgErr(recons, desired)\n",
    "    ]\n",
    "    \n",
    "    if (verbose):\n",
    "        print \"Max/min desired:\", metrics[0], metrics[1]\n",
    "        print \"Max/min recons: \", metrics[2], metrics[3]\n",
    "        print waveFilename, \" mse: \", metrics[4]\n",
    "        print waveFilename, \" avg err: \", metrics[5]\n",
    "        \n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(112, 512)\n",
      "112/112 [==============================] - 0s\n",
      "(112, 512, 1)\n",
      "(112, 512)\n",
      "Max/min desired: 4899.0 -4013.0\n",
      "Max/min recons:  4220.78 -130.155\n",
      "./SA1.WAV  mse:  763968.0\n",
      "./SA1.WAV  avg err:  576.829\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[4899.0, -4013.0, 4220.7842, -130.15468, 763967.75, 576.82892]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "autoencoderTest(\"./SA1.WAV\", \"SA1_res_uninit_\", autoencoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# interleave numpy arrays of the same size along the first axis\n",
    "def interleave(arr):    \n",
    "    num = len(arr)\n",
    "    \n",
    "    r = np.empty(arr[0].shape)\n",
    "    r = np.repeat(r, num, axis = 0)\n",
    "    \n",
    "    for i in xrange(0, num):\n",
    "        r[i::num] = arr[i]\n",
    "\n",
    "    return r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1:\n",
      "    101120: 0.434404015541  [4.671542 0.012304 0.791059 0.032175] [4.671542 3.076097 0.791059 0.804385] \n",
      "    Total time for epoch: 423.487285137s\n",
      "    ----------------\n",
      "    Evaluated the discriminator: 60.5% d_acc\n",
      "    Total time for evaluation: 1.05453300476s\n",
      "    ----------------\n",
      "    Code histogram:\n",
      "       Entropy: 2.46674283057\n",
      "       Zero prob: 0.177266\n",
      "       Mask entropy: 0.674058576306\n",
      "       Pct. in last bins: 0.0\n",
      "       Avg # nonzero elts: 105.31\n",
      "    ----------------\n",
      "    Evaluating autoencoder...\n",
      "    Max/min desired: 4899.0 -4013.0\n",
      "    Max/min recons:  3858.17 -2670.38\n",
      "    MSE:      9211.91\n",
      "    Avg err:  54.6259\n",
      "    Total time for evaluation: 0.136986017227s\n",
      "Epoch 2:\n",
      "    101120: 0.397434145212  [4.524831 0.011707 0.893175 0.028201] [4.524831 2.926630 0.893175 0.705027] \n",
      "    Total time for epoch: 378.203215837s\n",
      "    ----------------\n",
      "    Evaluated the discriminator: 63.75% d_acc\n",
      "    Total time for evaluation: 0.263473033905s\n",
      "    ----------------\n",
      "    Code histogram:\n",
      "       Entropy: 2.60723925014\n",
      "       Zero prob: 0.331797\n",
      "       Mask entropy: 0.916751716449\n",
      "       Pct. in last bins: 0.0\n",
      "       Avg # nonzero elts: 85.53\n",
      "    ----------------\n",
      "    Evaluating autoencoder...\n",
      "    Max/min desired: 4899.0 -4013.0\n",
      "    Max/min recons:  4000.4 -3389.44\n",
      "    MSE:      7221.31\n",
      "    Avg err:  50.767\n",
      "    Total time for evaluation: 0.114060878754s\n",
      "Epoch 3:\n",
      "    101120: 0.454257041216  [4.570174 0.012228 0.651810 0.034452] [4.570174 3.057073 0.651810 0.861291] \n",
      "    Total time for epoch: 377.362974167s\n",
      "    ----------------\n",
      "    Evaluated the discriminator: 59.0% d_acc\n",
      "    Total time for evaluation: 0.261990070343s\n",
      "    ----------------\n",
      "    Code histogram:\n",
      "       Entropy: 2.49151352879\n",
      "       Zero prob: 0.300156\n",
      "       Mask entropy: 0.881481796722\n",
      "       Pct. in last bins: 0.0\n",
      "       Avg # nonzero elts: 89.58\n",
      "    ----------------\n",
      "    Evaluating autoencoder...\n",
      "    Max/min desired: 4899.0 -4013.0\n",
      "    Max/min recons:  4173.36 -3643.94\n",
      "    MSE:      6855.99\n",
      "    Avg err:  49.1951\n",
      "    Total time for evaluation: 0.113445043564s\n",
      "Epoch 4:\n",
      "    101120: 0.447825938463  [5.038318 0.013607 0.779740 0.034277] [5.038318 3.401664 0.779740 0.856914] \n",
      "    Total time for epoch: 377.187995911s\n",
      "    ----------------\n",
      "    Evaluated the discriminator: 56.5% d_acc\n",
      "    Total time for evaluation: 0.261434078217s\n",
      "    ----------------\n",
      "    Code histogram:\n",
      "       Entropy: 2.46575825098\n",
      "       Zero prob: 0.260469\n",
      "       Mask entropy: 0.827452900955\n",
      "       Pct. in last bins: 0.0\n",
      "       Avg # nonzero elts: 94.66\n",
      "    ----------------\n",
      "    Evaluating autoencoder...\n",
      "    Max/min desired: 4899.0 -4013.0\n",
      "    Max/min recons:  4461.63 -3466.44\n",
      "    MSE:      6656.77\n",
      "    Avg err:  49.9849\n",
      "    Total time for evaluation: 0.114120006561s\n",
      "Epoch 5:\n",
      "    101120: 0.473060905933  [4.865668 0.012469 0.866693 0.035270] [4.865668 3.117214 0.866693 0.881761] \n",
      "    Total time for epoch: 376.56900692s\n",
      "    ----------------\n",
      "    Evaluated the discriminator: 56.25% d_acc\n",
      "    Total time for evaluation: 0.260037899017s\n",
      "    ----------------\n",
      "    Code histogram:\n",
      "       Entropy: 2.5054786779\n",
      "       Zero prob: 0.233906\n",
      "       Mask entropy: 0.78475887543\n",
      "       Pct. in last bins: 0.0\n",
      "       Avg # nonzero elts: 98.06\n",
      "    ----------------\n",
      "    Evaluating autoencoder...\n",
      "    Max/min desired: 4899.0 -4013.0\n",
      "    Max/min recons:  4557.11 -3696.88\n",
      "    MSE:      7476.72\n",
      "    Avg err:  52.9644\n",
      "    Total time for evaluation: 0.114112854004s\n",
      "Epoch 6:\n",
      "    101120: 0.444848686457  [4.342679 0.011064 0.800736 0.031036] [4.342679 2.766042 0.800736 0.775901] \n",
      "    Total time for epoch: 376.342690945s\n",
      "    ----------------\n",
      "    Evaluated the discriminator: 53.75% d_acc\n",
      "    Total time for evaluation: 0.261435031891s\n",
      "    ----------------\n",
      "    Code histogram:\n",
      "       Entropy: 2.63139453368\n",
      "       Zero prob: 0.312969\n",
      "       Mask entropy: 0.896570711941\n",
      "       Pct. in last bins: 0.0\n",
      "       Avg # nonzero elts: 87.94\n",
      "    ----------------\n",
      "    Evaluating autoencoder...\n",
      "    Max/min desired: 4899.0 -4013.0\n",
      "    Max/min recons:  4482.79 -3459.82\n",
      "    MSE:      5994.83\n",
      "    Avg err:  47.2191\n",
      "    Total time for evaluation: 0.114224910736s\n",
      "Epoch 7:\n",
      "    101120: 0.42793995142  [4.556128 0.011639 0.850729 0.031823] [4.556128 2.909816 0.850729 0.795583] \n",
      "    Total time for epoch: 376.147242069s\n",
      "    ----------------\n",
      "    Evaluated the discriminator: 57.75% d_acc\n",
      "    Total time for evaluation: 0.258807182312s\n",
      "    ----------------\n",
      "    Code histogram:\n",
      "       Entropy: 2.52071459996\n",
      "       Zero prob: 0.3775\n",
      "       Mask entropy: 0.956257196247\n",
      "       Pct. in last bins: 0.0\n",
      "       Avg # nonzero elts: 79.68\n",
      "    ----------------\n",
      "    Evaluating autoencoder...\n",
      "    Max/min desired: 4899.0 -4013.0\n",
      "    Max/min recons:  4277.25 -3211.56\n",
      "    MSE:      6472.5\n",
      "    Avg err:  48.8176\n",
      "    Total time for evaluation: 0.113262891769s\n",
      "Epoch 8:\n",
      "    101120: 0.417797118425  [4.354460 0.010586 0.887949 0.032798] [4.354460 2.646563 0.887949 0.819947] \n",
      "    Total time for epoch: 374.231945992s\n",
      "    ----------------\n",
      "    Evaluated the discriminator: 59.75% d_acc\n",
      "    Total time for evaluation: 0.258997917175s\n",
      "    ----------------\n",
      "    Code histogram:\n",
      "       Entropy: 2.52251472749\n",
      "       Zero prob: 0.328086\n",
      "       Mask entropy: 0.912958811395\n",
      "       Pct. in last bins: 0.0\n",
      "       Avg # nonzero elts: 86.005\n",
      "    ----------------\n",
      "    Evaluating autoencoder...\n",
      "    Max/min desired: 4899.0 -4013.0\n",
      "    Max/min recons:  4178.72 -3462.6\n",
      "    MSE:      5358.42\n",
      "    Avg err:  45.0236\n",
      "    Total time for evaluation: 0.114809989929s\n",
      "Epoch 9:\n",
      "    101120: 0.455767512321  [4.163998 0.010713 0.652193 0.033344] [4.163998 2.678210 0.652193 0.833596] \n",
      "    Total time for epoch: 374.368177891s\n",
      "    ----------------\n",
      "    Evaluated the discriminator: 57.0% d_acc\n",
      "    Total time for evaluation: 0.258962869644s\n",
      "    ----------------\n",
      "    Code histogram:\n",
      "       Entropy: 2.66830021242\n",
      "       Zero prob: 0.338477\n",
      "       Mask entropy: 0.923353419768\n",
      "       Pct. in last bins: 0.0\n",
      "       Avg # nonzero elts: 84.675\n",
      "    ----------------\n",
      "    Evaluating autoencoder...\n",
      "    Max/min desired: 4899.0 -4013.0\n",
      "    Max/min recons:  4462.58 -3480.08\n",
      "    MSE:      5098.0\n",
      "    Avg err:  44.4708\n",
      "    Total time for evaluation: 0.113409996033s\n",
      "Epoch 10:\n",
      "    101120: 0.452944159508  [4.308314 0.011515 0.699042 0.029225] [4.308314 2.878654 0.699042 0.730619] \n",
      "    Total time for epoch: 374.430505991s\n",
      "    ----------------\n",
      "    Evaluated the discriminator: 58.0% d_acc\n",
      "    Total time for evaluation: 0.259289979935s\n",
      "    ----------------\n",
      "    Code histogram:\n",
      "       Entropy: 2.55813764694\n",
      "       Zero prob: 0.34918\n",
      "       Mask entropy: 0.933333310198\n",
      "       Pct. in last bins: 0.0\n",
      "       Avg # nonzero elts: 83.305\n",
      "    ----------------\n",
      "    Evaluating autoencoder...\n",
      "    Max/min desired: 4899.0 -4013.0\n",
      "    Max/min recons:  4298.87 -3536.26\n",
      "    MSE:      5427.26\n",
      "    Avg err:  44.7352\n",
      "    Total time for evaluation: 0.113430976868s\n",
      "Epoch 11:\n",
      "    101120: 0.467278838158  [4.316272 0.010631 0.858710 0.031997] [4.316272 2.657647 0.858710 0.799915] \n",
      "    Total time for epoch: 374.117367983s\n",
      "    ----------------\n",
      "    Evaluated the discriminator: 57.5% d_acc\n",
      "    Total time for evaluation: 0.258105993271s\n",
      "    ----------------\n",
      "    Code histogram:\n",
      "       Entropy: 2.5743757665\n",
      "       Zero prob: 0.409883\n",
      "       Mask entropy: 0.976438894325\n",
      "       Pct. in last bins: 0.0\n",
      "       Avg # nonzero elts: 75.535\n",
      "    ----------------\n",
      "    Evaluating autoencoder...\n",
      "    Max/min desired: 4899.0 -4013.0\n",
      "    Max/min recons:  4445.82 -3403.14\n",
      "    MSE:      4624.76\n",
      "    Avg err:  42.3738\n",
      "    Total time for evaluation: 0.11359000206s\n",
      "Epoch 12:\n",
      "    101120: 0.46668535471  [4.449361 0.011458 0.652117 0.037306] [4.449361 2.864583 0.652117 0.932662] \n",
      "    Total time for epoch: 374.358701944s\n",
      "    ----------------\n",
      "    Evaluated the discriminator: 56.5% d_acc\n",
      "    Total time for evaluation: 0.25931096077s\n",
      "    ----------------\n",
      "    Code histogram:\n",
      "       Entropy: 2.60758412647\n",
      "       Zero prob: 0.331055\n",
      "       Mask entropy: 0.91600031462\n",
      "       Pct. in last bins: 0.0\n",
      "       Avg # nonzero elts: 85.625\n",
      "    ----------------\n",
      "    Evaluating autoencoder...\n",
      "    Max/min desired: 4899.0 -4013.0\n",
      "    Max/min recons:  4633.35 -3584.81\n",
      "    MSE:      5774.34\n",
      "    Avg err:  46.9035\n",
      "    Total time for evaluation: 0.113571882248s\n",
      "Epoch 13:\n",
      "    101120: 0.47365039587  [4.460990 0.011599 0.686072 0.035011] [4.460990 2.899650 0.686072 0.875270] \n",
      "    Total time for epoch: 373.799386978s\n",
      "    ----------------\n",
      "    Evaluated the discriminator: 56.75% d_acc\n",
      "    Total time for evaluation: 0.258284091949s\n",
      "    ----------------\n",
      "    Code histogram:\n",
      "       Entropy: 2.58126840228\n",
      "       Zero prob: 0.333164\n",
      "       Mask entropy: 0.918126473785\n",
      "       Pct. in last bins: 0.0\n",
      "       Avg # nonzero elts: 85.355\n",
      "    ----------------\n",
      "    Evaluating autoencoder...\n",
      "    Max/min desired: 4899.0 -4013.0\n",
      "    Max/min recons:  4424.57 -3477.38\n",
      "    MSE:      4826.66\n",
      "    Avg err:  43.1265\n",
      "    Total time for evaluation: 0.113475084305s\n",
      "Epoch 14:\n",
      "    101120: 0.437344789505  [4.461649 0.011268 0.845656 0.031962] [4.461649 2.816949 0.845656 0.799044] \n",
      "    Total time for epoch: 373.770211935s\n",
      "    ----------------\n",
      "    Evaluated the discriminator: 58.5% d_acc\n",
      "    Total time for evaluation: 0.258105993271s\n",
      "    ----------------\n",
      "    Code histogram:\n",
      "       Entropy: 2.6608476189\n",
      "       Zero prob: 0.366445\n",
      "       Mask entropy: 0.947903546996\n",
      "       Pct. in last bins: 0.0\n",
      "       Avg # nonzero elts: 81.095\n",
      "    ----------------\n",
      "    Evaluating autoencoder...\n",
      "    Max/min desired: 4899.0 -4013.0\n",
      "    Max/min recons:  4481.35 -3557.85\n",
      "    MSE:      4495.19\n",
      "    Avg err:  41.843\n",
      "    Total time for evaluation: 0.113421916962s\n",
      "Epoch 15:\n",
      "    101120: 0.475130826235  [4.035941 0.009883 0.796289 0.030761] [4.035941 2.470634 0.796289 0.769018] \n",
      "    Total time for epoch: 373.393243074s\n",
      "    ----------------\n",
      "    Evaluated the discriminator: 55.0% d_acc\n",
      "    Total time for evaluation: 0.256458044052s\n",
      "    ----------------\n",
      "    Code histogram:\n",
      "       Entropy: 2.5712542738\n",
      "       Zero prob: 0.399648\n",
      "       Mask entropy: 0.970744566458\n",
      "       Pct. in last bins: 0.0\n",
      "       Avg # nonzero elts: 76.845\n",
      "    ----------------\n",
      "    Evaluating autoencoder...\n",
      "    Max/min desired: 4899.0 -4013.0\n",
      "    Max/min recons:  4395.73 -3399.38\n",
      "    MSE:      4751.42\n",
      "    Avg err:  43.0277\n",
      "    Total time for evaluation: 0.112370014191s\n",
      "Epoch 16:\n",
      "    101120: 0.463536173105  [3.961371 0.009815 0.673217 0.033377] [3.961371 2.453731 0.673217 0.834423] \n",
      "    Total time for epoch: 373.874530077s\n",
      "    ----------------\n",
      "    Evaluated the discriminator: 55.0% d_acc\n",
      "    Total time for evaluation: 0.258381128311s\n",
      "    ----------------\n",
      "    Code histogram:\n",
      "       Entropy: 2.53638320419\n",
      "       Zero prob: 0.428711\n",
      "       Mask entropy: 0.985285980145\n",
      "       Pct. in last bins: 0.0\n",
      "       Avg # nonzero elts: 73.125\n",
      "    ----------------\n",
      "    Evaluating autoencoder...\n",
      "    Max/min desired: 4899.0 -4013.0\n",
      "    Max/min recons:  4514.75 -3572.17\n",
      "    MSE:      4454.39\n",
      "    Avg err:  42.3956\n",
      "    Total time for evaluation: 0.113547086716s\n",
      "Epoch 17:\n",
      "    101120: 0.463722556829  [4.155376 0.010335 0.768983 0.032110] [4.155376 2.583631 0.768983 0.802762] \n",
      "    Total time for epoch: 373.568482876s\n",
      "    ----------------\n",
      "    Evaluated the discriminator: 54.75% d_acc\n",
      "    Total time for evaluation: 0.257883071899s\n",
      "    ----------------\n",
      "    Code histogram:\n",
      "       Entropy: 2.48022067823\n",
      "       Zero prob: 0.418555\n",
      "       Mask entropy: 0.980774680784\n",
      "       Pct. in last bins: 0.0\n",
      "       Avg # nonzero elts: 74.425\n",
      "    ----------------\n",
      "    Evaluating autoencoder...\n",
      "    Max/min desired: 4899.0 -4013.0\n",
      "    Max/min recons:  4289.84 -3395.25\n",
      "    MSE:      4629.39\n",
      "    Avg err:  41.4593\n",
      "    Total time for evaluation: 0.113365888596s\n",
      "Epoch 18:\n",
      "    101120: 0.446715146303  [4.621465 0.012132 0.737514 0.034042] [4.621465 3.032897 0.737514 0.851054] \n",
      "    Total time for epoch: 373.683024168s\n",
      "    ----------------\n",
      "    Evaluated the discriminator: 56.5% d_acc\n",
      "    Total time for evaluation: 0.25701713562s\n",
      "    ----------------\n",
      "    Code histogram:\n",
      "       Entropy: 2.6976927372\n",
      "       Zero prob: 0.386484\n",
      "       Mask entropy: 0.962493281412\n",
      "       Pct. in last bins: 0.0\n",
      "       Avg # nonzero elts: 78.53\n",
      "    ----------------\n",
      "    Evaluating autoencoder...\n",
      "    Max/min desired: 4899.0 -4013.0\n",
      "    Max/min recons:  4458.0 -3536.61\n",
      "    MSE:      4085.3\n",
      "    Avg err:  40.4191\n",
      "    Total time for evaluation: 0.113291025162s\n",
      "Epoch 19:\n",
      "    101120: 0.486621707678  [4.036946 0.010601 0.586258 0.032017] [4.036946 2.650254 0.586258 0.800434] \n",
      "    Total time for epoch: 374.170169115s\n",
      "    ----------------\n",
      "    Evaluated the discriminator: 53.25% d_acc\n",
      "    Total time for evaluation: 0.25630402565s\n",
      "    ----------------\n",
      "    Code histogram:\n",
      "       Entropy: 2.57277688015\n",
      "       Zero prob: 0.32793\n",
      "       Mask entropy: 0.912797133576\n",
      "       Pct. in last bins: 0.0\n",
      "       Avg # nonzero elts: 86.025\n",
      "    ----------------\n",
      "    Evaluating autoencoder...\n",
      "    Max/min desired: 4899.0 -4013.0\n",
      "    Max/min recons:  4565.23 -3651.49\n",
      "    MSE:      4479.06\n",
      "    Avg err:  42.27\n",
      "    Total time for evaluation: 0.114186048508s\n",
      "Epoch 20:\n",
      "    101120: 0.483104795218  [4.325810 0.011003 0.723027 0.034081] [4.325810 2.750762 0.723027 0.852020] \n",
      "    Total time for epoch: 373.593215942s\n",
      "    ----------------\n",
      "    Evaluated the discriminator: 54.5% d_acc\n",
      "    Total time for evaluation: 0.257334947586s\n",
      "    ----------------\n",
      "    Code histogram:\n",
      "       Entropy: 2.51867609653\n",
      "       Zero prob: 0.318945\n",
      "       Mask entropy: 0.903230841122\n",
      "       Pct. in last bins: 0.0\n",
      "       Avg # nonzero elts: 87.175\n",
      "    ----------------\n",
      "    Evaluating autoencoder...\n",
      "    Max/min desired: 4899.0 -4013.0\n",
      "    Max/min recons:  4453.84 -3537.29\n",
      "    MSE:      4581.47\n",
      "    Avg err:  43.9177\n",
      "    Total time for evaluation: 0.112967014313s\n",
      "Epoch 21:\n",
      "    101120: 0.466398686171  [3.605636 0.009942 0.343334 0.031068] [3.605636 2.485600 0.343334 0.776702] \n",
      "    Total time for epoch: 373.650485039s\n",
      "    ----------------\n",
      "    Evaluated the discriminator: 55.75% d_acc\n",
      "    Total time for evaluation: 0.25719499588s\n",
      "    ----------------\n",
      "    Code histogram:\n",
      "       Entropy: 2.54680518753\n",
      "       Zero prob: 0.333633\n",
      "       Mask entropy: 0.91859503641\n",
      "       Pct. in last bins: 0.0\n",
      "       Avg # nonzero elts: 85.295\n",
      "    ----------------\n",
      "    Evaluating autoencoder...\n",
      "    Max/min desired: 4899.0 -4013.0\n",
      "    Max/min recons:  4452.41 -3494.65\n",
      "    MSE:      4178.67\n",
      "    Avg err:  40.5137\n",
      "    Total time for evaluation: 0.113231897354s\n",
      "Epoch 22:\n",
      "    101120: 0.451637208462  [3.873539 0.009223 0.796035 0.030871] [3.873539 2.305732 0.796035 0.771772] \n",
      "    Total time for epoch: 374.617486s\n",
      "    ----------------\n",
      "    Evaluated the discriminator: 59.25% d_acc\n",
      "    Total time for evaluation: 0.259999036789s\n",
      "    ----------------\n",
      "    Code histogram:\n",
      "       Entropy: 2.57554478015\n",
      "       Zero prob: 0.403828\n",
      "       Mask entropy: 0.973145898411\n",
      "       Pct. in last bins: 0.0\n",
      "       Avg # nonzero elts: 76.31\n",
      "    ----------------\n",
      "    Evaluating autoencoder...\n",
      "    Max/min desired: 4899.0 -4013.0\n",
      "    Max/min recons:  4365.84 -3488.95\n",
      "    MSE:      4211.49\n",
      "    Avg err:  40.6328\n",
      "    Total time for evaluation: 0.113251924515s\n",
      "Epoch 23:\n",
      "    101120: 0.479654818773  [4.264905 0.010692 0.713042 0.035158] [4.264905 2.672902 0.713042 0.878962] \n",
      "    Total time for epoch: 374.52657795s\n",
      "    ----------------\n",
      "    Evaluated the discriminator: 52.0% d_acc\n",
      "    Total time for evaluation: 0.257463932037s\n",
      "    ----------------\n",
      "    Code histogram:\n",
      "       Entropy: 2.55399039513\n",
      "       Zero prob: 0.372422\n",
      "       Mask entropy: 0.952513526893\n",
      "       Pct. in last bins: 0.0\n",
      "       Avg # nonzero elts: 80.33\n",
      "    ----------------\n",
      "    Evaluating autoencoder...\n",
      "    Max/min desired: 4899.0 -4013.0\n",
      "    Max/min recons:  4466.12 -3529.6\n",
      "    MSE:      4197.55\n",
      "    Avg err:  41.0027\n",
      "    Total time for evaluation: 0.113259077072s\n",
      "Epoch 24:\n",
      "    101120: 0.487793505192  [4.250365 0.010548 0.738014 0.035018] [4.250365 2.636891 0.738014 0.875460] \n",
      "    Total time for epoch: 374.124492884s\n",
      "    ----------------\n",
      "    Evaluated the discriminator: 53.5% d_acc\n",
      "    Total time for evaluation: 0.257698059082s\n",
      "    ----------------\n",
      "    Code histogram:\n",
      "       Entropy: 2.59315364326\n",
      "       Zero prob: 0.367891\n",
      "       Mask entropy: 0.949038679381\n",
      "       Pct. in last bins: 0.0\n",
      "       Avg # nonzero elts: 80.91\n",
      "    ----------------\n",
      "    Evaluating autoencoder...\n",
      "    Max/min desired: 4899.0 -4013.0\n",
      "    Max/min recons:  4345.08 -3563.18\n",
      "    MSE:      4382.66\n",
      "    Avg err:  41.2584\n",
      "    Total time for evaluation: 0.113268852234s\n",
      "Epoch 25:\n",
      "    101120: 0.442209005356  [4.505188 0.011271 0.815475 0.034874] [4.505188 2.817851 0.815475 0.871862] \n",
      "    Total time for epoch: 374.820324898s\n",
      "    ----------------\n",
      "    Evaluated the discriminator: 56.0% d_acc\n",
      "    Total time for evaluation: 0.259447097778s\n",
      "    ----------------\n",
      "    Code histogram:\n",
      "       Entropy: 2.67365523587\n",
      "       Zero prob: 0.319883\n",
      "       Mask entropy: 0.90425397255\n",
      "       Pct. in last bins: 0.0\n",
      "       Avg # nonzero elts: 87.055\n",
      "    ----------------\n",
      "    Evaluating autoencoder...\n",
      "    Max/min desired: 4899.0 -4013.0\n",
      "    Max/min recons:  4593.89 -3621.89\n",
      "    MSE:      4111.67\n",
      "    Avg err:  40.3634\n",
      "    Total time for evaluation: 0.113736867905s\n"
     ]
    }
   ],
   "source": [
    "np.set_printoptions(formatter={'float_kind':'{:4f}'.format})\n",
    "\n",
    "BATCH_SIZE = 128\n",
    "NUM_BATCHES = ntrain / BATCH_SIZE\n",
    "NUM_EPOCHS = 25\n",
    "\n",
    "lead = \"    \"\n",
    "d_loss = 0.0\n",
    "a_losses = []\n",
    "d_acc = 0.0\n",
    "discrim_train_y = np.concatenate((np.ones(ntrain), np.zeros(ntrain)))\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    print \"Epoch \" + str(epoch + 1) + \":\"\n",
    "\n",
    "    # present batches randomly each epoch\n",
    "    lis = range(0, ntrain, BATCH_SIZE)\n",
    "    random.shuffle(lis)\n",
    "    \n",
    "    train_gen = True\n",
    "    train_discrim = True\n",
    "    \n",
    "    # keep track of start time and current batch #\n",
    "    i = 0\n",
    "    startTime = time.time()\n",
    "    for idx in lis:\n",
    "        batch = X_train[idx:idx+BATCH_SIZE, :,  :]\n",
    "        nbatch = batch.shape[0]\n",
    "        \n",
    "        a_losses = [\"autoencoder not training\"]\n",
    "        d_loss = \"discriminator not training\"\n",
    "        \n",
    "        # if both are disabled, we re-enable both\n",
    "        if (not train_gen and not train_discrim):\n",
    "            train_gen = True\n",
    "            train_discrim = True\n",
    "\n",
    "        \n",
    "        if (train_gen or n_discrim == 0):\n",
    "            # train autoencoder (\"generator\")\n",
    "            make_trainable(autoencoder, True)\n",
    "            make_trainable(discriminator, False)\n",
    "            \n",
    "            a_y = [batch] * n_recons + \\\n",
    "                  [np.ones(nbatch)] * n_discrim + \\\n",
    "                  [np.zeros((nbatch, bottleneck_size))] * n_code + \\\n",
    "                  [np.zeros(nbatch)] * n_other\n",
    "            a_losses = model.train_on_batch(batch, a_y)\n",
    "            \n",
    "            ad_loss = a_losses[-1]\n",
    "        else:\n",
    "            # re-enable generator training if disabled\n",
    "            train_gen = True\n",
    "        \n",
    "        \n",
    "        if (train_discrim and n_discrim > 0):\n",
    "            # train discriminator\n",
    "            make_trainable(autoencoder, False)\n",
    "            make_trainable(discriminator, True)\n",
    "            \n",
    "            generated = autoencoder.predict(batch)\n",
    "            discrim_batch_X = interleave([batch, generated])\n",
    "            discrim_batch_y = interleave([np.ones(nbatch), np.zeros(nbatch)])\n",
    "\n",
    "            d_loss = discriminator.train_on_batch(discrim_batch_X, discrim_batch_y)\n",
    "            \n",
    "            # turn loss into probability of predicting correctly\n",
    "            #d_loss = np.exp(-d_loss)\n",
    "            \n",
    "            #if (d_loss > 0.8):\n",
    "                # if discriminator is over 80% accurate, we don't train\n",
    "                # the discriminator next batch\n",
    "                #train_discrim = False\n",
    "            #elif (d_loss < 0.5):\n",
    "                # if discriminator is under 50% accurate, we don't train\n",
    "                # the generator next batch\n",
    "                #train_gen = False\n",
    "        else:\n",
    "            # re-enable discriminator training if disabled\n",
    "            train_discrim = True\n",
    "        \n",
    "        # print statistics every 10 batches so we know stuff is still going down\n",
    "        if (i % 10 == 0):\n",
    "            printStr = \"        \\r\" + lead + str(i * BATCH_SIZE) + \": \" + str(d_loss) + \" \"\n",
    "            print printStr,\n",
    "            \n",
    "            loss_arr = np.asarray(a_losses)\n",
    "            print loss_arr,\n",
    "            \n",
    "            if (len(loss_weights) > 1 and len(loss_arr) > 1):\n",
    "                for w in xrange(0, len(loss_weights)):\n",
    "                    loss_arr[w + 1] *= loss_weights[w]\n",
    "                print loss_arr,\n",
    "            \n",
    "        i += 1\n",
    "    print \"\"\n",
    "    \n",
    "    # print elapsed time for epoch\n",
    "    elapsed = time.time() - startTime\n",
    "    print lead + \"Total time for epoch: \" + str(elapsed) + \"s\"   \n",
    "    \n",
    "    # ---------------------------------------------------------\n",
    "    # evaluate discriminator on random samples every epoch\n",
    "    # ---------------------------------------------------------\n",
    "    startTime = time.time()\n",
    "    print lead + \"----------------\"\n",
    "    if (n_discrim > 0):\n",
    "        NUM = 200\n",
    "        rows = np.random.randint(X_train.shape[0], size = NUM)\n",
    "        generated = autoencoder.predict(X_train[rows, :], verbose = 0)\n",
    "        d_X = np.concatenate((X_train[rows, :], generated))\n",
    "        d_y = np.concatenate((np.ones(NUM), np.zeros(NUM)))\n",
    "        d_acc = test_discriminator(discriminator, autoencoder,\n",
    "                                   d_X, d_y, verbose = False)\n",
    "\n",
    "        print lead + \"Evaluated the discriminator: \" + str(d_acc) + \"% d_acc\"\n",
    "        elapsed = time.time() - startTime\n",
    "        print lead + \"Total time for evaluation: \" + str(elapsed) + \"s\"\n",
    "    else:\n",
    "        print lead + \"No discriminator\"\n",
    "    \n",
    "    \n",
    "    # ---------------------------------------------------------\n",
    "    # generate code histogram from said random samples\n",
    "    # ---------------------------------------------------------\n",
    "    NUM = 200\n",
    "    rows = np.random.randint(X_train.shape[0], size = NUM)\n",
    "    code = ac_enc.predict(X_train[rows, :], verbose = 0)\n",
    "    \n",
    "    print lead + \"----------------\"\n",
    "    print lead + \"Code histogram:\"\n",
    "    scalars = code.flatten()\n",
    "    \n",
    "    b = np.linspace(-1.0, 1.0, NBINS + 1)\n",
    "    hist = np.histogram(scalars, bins = b)\n",
    "    sample_hist_probs = hist[0].astype('float32')\n",
    "    sample_hist_bins = hist[1].astype('float32')\n",
    "    sample_hist_probs /= np.sum(sample_hist_probs)\n",
    "\n",
    "    entropy = 0\n",
    "    for i in sample_hist_probs:\n",
    "        if (i < 1e-4): continue\n",
    "        entropy += i * math.log(i, 2)\n",
    "    entropy = -entropy\n",
    "    \n",
    "    zero_prob = sample_hist_probs[NBINS / 2]\n",
    "    zero_prob = np.clip(zero_prob, 0.001, 0.999)\n",
    "    mask_entropy = -(zero_prob * math.log(zero_prob, 2) + (1.0 - zero_prob) * math.log(1.0 - zero_prob, 2))\n",
    "    \n",
    "    print \"       Entropy:\", entropy\n",
    "    print \"       Zero prob:\", sample_hist_probs[NBINS / 2]\n",
    "    print \"       Mask entropy:\", mask_entropy\n",
    "    print \"       Pct. in last bins:\", sample_hist_probs[0] + sample_hist_probs[-1]\n",
    "    \n",
    "    nnz = 0.0\n",
    "    for i in xrange(0, code.shape[0]):\n",
    "        r = np.round(code[i] * 1000.0) / 1000.0\n",
    "        nnz += np.count_nonzero(r)\n",
    "    nnz /= code.shape[0]\n",
    "    print \"       Avg # nonzero elts:\", nnz\n",
    "    \n",
    "    # ---------------------------------------------------------\n",
    "    # evaluate autoencoder on real data evey epoch\n",
    "    # ---------------------------------------------------------\n",
    "    startTime = time.time()\n",
    "    print lead + \"----------------\"\n",
    "    \n",
    "    print lead + \"Evaluating autoencoder...\"\n",
    "    metrics = autoencoderTest(\"./SA1.WAV\", \"SA1_res_reg_train_epoch\" + str(epoch+1), autoencoder, verbose = False)\n",
    "    \n",
    "    print lead + \"Max/min desired:\", metrics[0], metrics[1]\n",
    "    print lead + \"Max/min recons: \", metrics[2], metrics[3]\n",
    "    print lead + \"MSE:     \", metrics[4]\n",
    "    print lead + \"Avg err: \", metrics[5]\n",
    "    elapsed = time.time() - startTime\n",
    "    print lead + \"Total time for evaluation: \" + str(elapsed) + \"s\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.save('model_res_reg.h5')\n",
    "autoencoder.save('auto_res_reg.h5')\n",
    "\n",
    "discriminator.save('discrim_res_reg.h5')\n",
    "\n",
    "import h5py\n",
    "\n",
    "f = h5py.File('model_res_reg.h5', 'r+')\n",
    "del f['optimizer_weights']\n",
    "f.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"from keras.models import load_model\\n\\nobjs = {'PhaseShift1D' : PhaseShift1D}\\n\\nmodel = load_model('model_res_reg.h5', objs)\\nautoencoder = load_model('auto_res_reg.h5', objs)\\ndiscriminator = load_model('discrim_res_reg.h5', objs)\\n\""
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''from keras.models import load_model\n",
    "\n",
    "objs = {'PhaseShift1D' : PhaseShift1D}\n",
    "\n",
    "model = load_model('model_res_reg.h5', objs)\n",
    "autoencoder = load_model('auto_res_reg.h5', objs)\n",
    "discriminator = load_model('discrim_res_reg.h5', objs)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "enc = model.layers[1].layers\n",
    "dec = model.layers[2].layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "'''\n",
    "i = 0\n",
    "print \"-- Encoder --\"\n",
    "print \"\"\n",
    "for e in enc:\n",
    "    if type(e) is Convolution1D:\n",
    "        i += 1\n",
    "        print \"Conv layer\", i\n",
    "        w = e.weights[0].eval()\n",
    "        print \"    Avg weight norm:\", np.mean(np.abs(w))\n",
    "        print \"    Max weight norm:\", np.max(np.abs(w))\n",
    "        \n",
    "        if (len(e.weights) == 1): continue\n",
    "        b = e.weights[1].eval()\n",
    "        print \"    Avg bias norm:\", np.mean(np.abs(b))\n",
    "        print \"    Max bias norm:\", np.max(np.abs(b))\n",
    "print \"\"\n",
    "\n",
    "print \"-- Decoder --\"\n",
    "print \"\"\n",
    "for e in dec:\n",
    "    if type(e) is Convolution1D:\n",
    "        i += 1\n",
    "        print \"Conv layer\", i\n",
    "        w = e.weights[0].eval()\n",
    "        print \"    Avg weight norm:\", np.mean(np.abs(w))\n",
    "        print \"    Max weight norm:\", np.max(np.abs(w))\n",
    "        \n",
    "        if (len(e.weights) == 1): continue\n",
    "        b = e.weights[1].eval()\n",
    "        print \"    Avg bias norm:\", np.mean(np.abs(b))\n",
    "        print \"    Max bias norm:\", np.max(np.abs(b))\n",
    "\n",
    "#print [e.eval() for e in enc[-3].weights]\n",
    "'''\n",
    "0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Evaluated the discriminator: 56.125% d_acc\n"
     ]
    }
   ],
   "source": [
    "NUM = 400\n",
    "rows = np.random.randint(X_train.shape[0], size = NUM)\n",
    "generated = autoencoder.predict(X_train[rows, :], verbose = 0)\n",
    "d_X = np.concatenate((X_train[rows, :], generated))\n",
    "d_y = np.concatenate((np.ones(NUM), np.zeros(NUM)))\n",
    "d_acc = test_discriminator(discriminator, autoencoder,\n",
    "                           d_X, d_y, verbose = False)\n",
    "\n",
    "print lead + \"Evaluated the discriminator: \" + str(d_acc) + \"% d_acc\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(112, 512)\n",
      "112/112 [==============================] - 0s\n",
      "(112, 512, 1)\n",
      "(112, 512)\n",
      "Max/min desired: 4899.0 -4013.0\n",
      "Max/min recons:  4593.89 -3621.89\n",
      "./SA1.WAV  mse:  4111.67\n",
      "./SA1.WAV  avg err:  40.3634\n",
      "(93, 512)\n",
      "93/93 [==============================] - 0s\n",
      "(93, 512, 1)\n",
      "(93, 512)\n",
      "Max/min desired: 2961.0 -3057.0\n",
      "Max/min recons:  2793.24 -2744.23\n",
      "./SX383.WAV  mse:  2393.74\n",
      "./SX383.WAV  avg err:  28.6961\n",
      "(181, 512)\n",
      "181/181 [==============================] - 0s     \n",
      "(181, 512, 1)\n",
      "(181, 512)\n",
      "Max/min desired: 24636.0 -20122.0\n",
      "Max/min recons:  21259.9 -19079.8\n",
      "./fiveYears.wav  mse:  2.11132e+06\n",
      "./fiveYears.wav  avg err:  1053.12\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[24636.0, -20122.0, 21259.879, -19079.805, 2111324.5, 1053.1215]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "autoencoderTest(\"./SA1.WAV\", \"SA1_res_reg_\", autoencoder)\n",
    "autoencoderTest(\"./SX383.WAV\", \"SX383_res_reg_\", autoencoder)\n",
    "autoencoderTest(\"./fiveYears.wav\", \"fy_res_reg_\", autoencoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 9984/10000 [============================>.] - ETA: 0s"
     ]
    }
   ],
   "source": [
    "all_embed = ac_enc.predict(X_train[:10000], batch_size = BATCH_SIZE, verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "scalars = all_embed.flatten()\n",
    "log_scalars = np.log((scalars + 1.0) / 2.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.00780887\n",
      "0.00109983\n"
     ]
    }
   ],
   "source": [
    "print np.mean(scalars)\n",
    "print np.var(scalars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAhQAAAFkCAYAAAB4sKK5AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nDBSSJKmYgUKSJBX7/zNDDi/6dQIDAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f40dd484390>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "hist = np.histogram(scalars, bins = np.linspace(-1.0, 1.0, NBINS + 1))\n",
    "sample_hist_probs = hist[0].astype('float32')\n",
    "sample_hist_bins = hist[1].astype('float32')\n",
    "sample_hist_probs /= np.sum(sample_hist_probs)\n",
    "\n",
    "sample_hist_width = 1 * (sample_hist_bins[1] - sample_hist_bins[0])\n",
    "sample_hist_centers = (sample_hist_bins[:-1] + sample_hist_bins[1:]) / 2\n",
    "plt.bar(sample_hist_centers, sample_hist_probs, align='center', width=sample_hist_width)\n",
    "plt.show()\n",
    "\n",
    "if (NBINS > 9):\n",
    "    for i in xrange(1, 8):\n",
    "        p = 0\n",
    "        for j in xrange(0, i):\n",
    "            p += sample_hist_probs[j]\n",
    "            p += sample_hist_probs[-j - 1]\n",
    "        print p\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "112/112 [==============================] - 0s\n"
     ]
    }
   ],
   "source": [
    "[rate, data] = sciwav.read(\"./SA1.WAV\")\n",
    "data = data.astype(np.float32)\n",
    "processedWave, wparams = preprocessWaveform(data)\n",
    "windows = extractWindows(processedWave, STEP_SIZE, OVERLAP_SIZE)\n",
    "\n",
    "transformed, tparams = preprocessWindows(windows)\n",
    "\n",
    "transformed = np.reshape(transformed, (transformed.shape[0], WINDOW_SIZE, 1))\n",
    "embed = ac_enc.predict(transformed, batch_size = BATCH_SIZE, verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.280386\n",
      "94\n",
      "[-0.040000 0.000000 0.020000 0.060000 0.000000 0.040000 -0.020000 0.000000\n",
      " -0.040000 -0.020000 -0.060000 0.000000 0.000000 0.020000 0.040000\n",
      " -0.020000 -0.020000 -0.020000 -0.100000 -0.040000 -0.020000 0.000000\n",
      " 0.040000 0.000000 0.040000 0.000000 0.000000 -0.040000 -0.020000 -0.060000\n",
      " -0.020000 0.020000 0.020000 0.020000 0.000000 -0.020000 -0.020000\n",
      " -0.080000 -0.040000 -0.020000 0.000000 0.040000 0.000000 0.040000 0.020000\n",
      " 0.000000 -0.040000 -0.020000 -0.060000 -0.020000 0.000000 0.020000\n",
      " 0.020000 0.020000 -0.020000 -0.020000 -0.100000 -0.020000 -0.020000\n",
      " 0.000000 0.020000 0.020000 0.020000 0.020000 0.000000 -0.040000 -0.020000\n",
      " -0.040000 -0.020000 0.000000 0.000000 0.020000 0.000000 -0.020000\n",
      " -0.040000 -0.080000 -0.020000 -0.020000 0.020000 0.000000 0.040000\n",
      " 0.000000 0.020000 0.000000 -0.040000 -0.040000 -0.040000 -0.020000\n",
      " 0.020000 0.000000 0.020000 -0.020000 -0.020000 -0.040000 -0.080000\n",
      " -0.040000 -0.020000 0.020000 0.000000 0.040000 0.000000 0.020000 0.000000\n",
      " -0.020000 -0.060000 -0.020000 -0.040000 0.000000 0.000000 0.000000\n",
      " -0.020000 0.000000 -0.060000 -0.060000 -0.020000 -0.020000 0.020000\n",
      " 0.000000 0.040000 0.020000 0.020000 -0.020000 0.000000 -0.040000 -0.020000\n",
      " 0.000000 0.000000 0.020000]\n",
      "[-0.040000 0.000000 0.020000 0.060000 0.000000 0.040000 -0.020000 0.000000\n",
      " -0.040000 -0.020000 -0.060000 0.000000 0.000000 0.020000 0.040000\n",
      " -0.020000 -0.020000 -0.020000 -0.100000 -0.040000 -0.020000 0.000000\n",
      " 0.040000 0.000000 0.040000 0.000000 0.000000 -0.040000 -0.020000 -0.060000\n",
      " -0.020000 0.020000 0.020000 0.020000 0.000000 -0.020000 -0.020000\n",
      " -0.080000 -0.040000 -0.020000 0.000000 0.040000 0.000000 0.040000 0.020000\n",
      " 0.000000 -0.040000 -0.020000 -0.060000 -0.020000 0.000000 0.020000\n",
      " 0.020000 0.020000 -0.020000 -0.020000 -0.100000 -0.020000 -0.020000\n",
      " 0.000000 0.020000 0.020000 0.020000 0.020000 0.000000 -0.040000 -0.020000\n",
      " -0.040000 -0.020000 0.000000 0.000000 0.020000 0.000000 -0.020000\n",
      " -0.040000 -0.080000 -0.020000 -0.020000 0.020000 0.000000 0.040000\n",
      " 0.000000 0.020000 0.000000 -0.040000 -0.040000 -0.040000 -0.020000\n",
      " 0.020000 0.000000 0.020000 -0.020000 -0.020000 -0.040000 -0.080000\n",
      " -0.040000 -0.020000 0.020000 0.000000 0.040000 0.000000 0.020000 0.000000\n",
      " -0.020000 -0.060000 -0.020000 -0.040000 0.000000 0.000000 0.000000\n",
      " -0.020000 0.000000 -0.060000 -0.060000 -0.020000 -0.020000 0.020000\n",
      " 0.000000 0.040000 0.020000 0.020000 -0.020000 0.000000 -0.040000 -0.020000\n",
      " 0.000000 0.000000 0.020000]\n",
      "112/112 [==============================] - 0s\n",
      "(112, 512, 1)\n",
      "(112, 512)\n"
     ]
    }
   ],
   "source": [
    "idx = 56\n",
    "print np.sqrt(np.sum(np.sort(np.square(embed[idx]))[-1:])) / np.linalg.norm(embed[idx])\n",
    "print np.count_nonzero(embed[idx])\n",
    "print embed[idx]\n",
    "\n",
    "r = np.copy(embed)\n",
    "threshold = 0\n",
    "#r = np.sign(r)\n",
    "qnt_bins = NBINS\n",
    "#r[r < threshold] = -1.0\n",
    "#r[r >= threshold] = 1.0\n",
    "#r = np.round(r)\n",
    "#r[(r < 0) & (r > -0.93)] = -0.936508\n",
    "#r[(r > 0) & (r < 0.93)] = 0.936508\n",
    "r = (r + 1.0) / 2.0\n",
    "r = np.round(r * float(qnt_bins - 1))\n",
    "qnt = r[idx].astype('int')\n",
    "r /= float(qnt_bins - 1)\n",
    "r = (r * 2.0) - 1.0\n",
    "#r = np.sign(r)\n",
    "\n",
    "print r[idx]\n",
    "#print qnt\n",
    "\n",
    "\n",
    "\n",
    "autoencOutput = ac_dec.predict(r, batch_size = BATCH_SIZE, verbose = 1)\n",
    "print autoencOutput.shape\n",
    "autoencOutput = np.reshape(autoencOutput, (autoencOutput.shape[0], WINDOW_SIZE))\n",
    "\n",
    "print autoencOutput.shape\n",
    "recons = unpreprocessWindows(autoencOutput, tparams)\n",
    "\n",
    "wav = reconstructFromWindows(recons, OVERLAP_SIZE, OVERLAP_FUNC)\n",
    "wav = unpreprocessWaveform(wav, wparams)\n",
    "wav = np.clip(wav, -32767.0, 32767.0)\n",
    "\n",
    "sciwav.write(\"qnt_output_res_reg.wav\", rate, wav.astype(np.int16))\n",
    "\n",
    "idx = 45"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1.000000 -0.980198 -0.960396 -0.940594 -0.920792 -0.900990 -0.881188\n",
      " -0.861386 -0.841584 -0.821782 -0.801980 -0.782178 -0.762376 -0.742574\n",
      " -0.722772 -0.702970 -0.683168 -0.663366 -0.643564 -0.623762 -0.603960\n",
      " -0.584158 -0.564356 -0.544554 -0.524752 -0.504950 -0.485149 -0.465347\n",
      " -0.445545 -0.425743 -0.405941 -0.386139 -0.366337 -0.346535 -0.326733\n",
      " -0.306931 -0.287129 -0.267327 -0.247525 -0.227723 -0.207921 -0.188119\n",
      " -0.168317 -0.148515 -0.128713 -0.108911 -0.089109 -0.069307 -0.049505\n",
      " -0.029703 -0.009901 0.009901 0.029703 0.049505 0.069307 0.089109 0.108911\n",
      " 0.128713 0.148515 0.168317 0.188119 0.207921 0.227723 0.247525 0.267327\n",
      " 0.287129 0.306931 0.326733 0.346535 0.366337 0.386139 0.405941 0.425743\n",
      " 0.445545 0.465347 0.485149 0.504950 0.524752 0.544554 0.564356 0.584158\n",
      " 0.603960 0.623762 0.643564 0.663366 0.683168 0.702970 0.722772 0.742574\n",
      " 0.762376 0.782178 0.801980 0.821782 0.841584 0.861386 0.881188 0.900990\n",
      " 0.920792 0.940594 0.960396 0.980198 1.000000]\n",
      "[     0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      5     24     43\n",
      "    142    443    950   2180   4669  10366  22841  53733 126331 371771\n",
      " 423553 146130  61510  28444  13830   6765   3315   1662    812    346\n",
      "     93     33      7      1      1      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0]\n",
      "1280000\n",
      "[-1.000000 -0.980198 -0.960396 -0.940594 -0.920792 -0.900990 -0.881188\n",
      " -0.861386 -0.841584 -0.821782 -0.801980 -0.782178 -0.762376 -0.742574\n",
      " -0.722772 -0.702970 -0.683168 -0.663366 -0.643564 -0.623762 -0.603960\n",
      " -0.584158 -0.564356 -0.544554 -0.524752 -0.504950 -0.485149 -0.465347\n",
      " -0.445545 -0.425743 -0.405941 -0.386139 -0.366337 -0.346535 -0.326733\n",
      " -0.306931 -0.287129 -0.267327 -0.247525 -0.227723 -0.207921 -0.188119\n",
      " -0.168317 -0.148515 -0.128713 -0.108911 -0.089109 -0.069307 -0.049505\n",
      " -0.029703 -0.009901 0.009901 0.029703 0.049505 0.069307 0.089109 0.108911\n",
      " 0.128713 0.148515 0.168317 0.188119 0.207921 0.227723 0.247525 0.267327\n",
      " 0.287129 0.306931 0.326733 0.346535 0.366337 0.386139 0.405941 0.425743\n",
      " 0.445545 0.465347 0.485149 0.504950 0.524752 0.544554 0.564356 0.584158\n",
      " 0.603960 0.623762 0.643564 0.663366 0.683168 0.702970 0.722772 0.742574\n",
      " 0.762376 0.782178 0.801980 0.821782 0.841584 0.861386 0.881188 0.900990\n",
      " 0.920792 0.940594 0.960396 0.980198 1.000000]\n",
      "[0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000\n",
      " 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000\n",
      " 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000\n",
      " 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000\n",
      " 0.000000 0.000000 0.000000 0.000000 0.000000 0.000004 0.000019 0.000034\n",
      " 0.000111 0.000346 0.000742 0.001703 0.003648 0.008098 0.017845 0.041979\n",
      " 0.098696 0.290446 0.330901 0.114164 0.048055 0.022222 0.010805 0.005285\n",
      " 0.002590 0.001298 0.000634 0.000270 0.000073 0.000026 0.000005 0.000001\n",
      " 0.000001 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000\n",
      " 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000\n",
      " 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000\n",
      " 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000\n",
      " 0.000000 0.000000 0.000000 0.000000 0.000000]\n",
      "2.63115230518\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "b = np.linspace(-1.0, 1.0, NBINS + 1)\n",
    "print b\n",
    "\n",
    "h = np.histogram(scalars, bins = b)\n",
    "print h[0]\n",
    "print h[0].sum()\n",
    "print h[1]\n",
    "h = h[0].astype('float32')\n",
    "h = h / h.sum()\n",
    "print h\n",
    "\n",
    "entropy = 0\n",
    "for i in h:\n",
    "    if (i < 1e-4): continue\n",
    "    entropy += i * math.log(i, 2)\n",
    "entropy = -entropy\n",
    "print entropy\n",
    "\n",
    "if (NBINS > 4):\n",
    "    entropy = 0\n",
    "    for idx in [0, 1, 2, -1, -2, -3]:\n",
    "        i = h[idx]\n",
    "        if (i < 1e-4): continue\n",
    "        entropy += i * math.log(i, 2)\n",
    "    entropy = -entropy\n",
    "    print entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
